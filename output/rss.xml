<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Visions, Voices, Data</title><link>https://necromuralist.github.io/Visions-Voices-Data/</link><description>Adumbrations of data.</description><atom:link href="https://necromuralist.github.io/Visions-Voices-Data/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Fri, 26 Jul 2019 19:17:30 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Nested follower</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/nested-follower/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/nested-follower/#org3f5ef15"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/nested-follower/#org887d280"&gt;Create A Walker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/nested-follower/#org286e2fd"&gt;Specifications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/nested-follower/#orgf36a7c1"&gt;Middle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/nested-follower/#orga2503bf"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3f5ef15" class="outline-2"&gt;
&lt;h2 id="org3f5ef15"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3f5ef15"&gt;
&lt;p&gt;
This is assignment 1 from the Nature of Code course on Kadenze. I was originally going to make it a mouse-follower but I re-read the instructions and it seems like it's better to make it a random-walker. These are the requirements:
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org887d280" class="outline-3"&gt;
&lt;h3 id="org887d280"&gt;Create A Walker&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org887d280"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Create an object that moves around the screen&lt;/li&gt;
&lt;li&gt;Incorporate randomness or perlin noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org286e2fd" class="outline-3"&gt;
&lt;h3 id="org286e2fd"&gt;Specifications&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org286e2fd"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Needs to be visually different from the Nature of Code examples&lt;/li&gt;
&lt;li&gt;Use comments&lt;/li&gt;
&lt;li&gt;Only use &lt;code&gt;p5.js&lt;/code&gt; libraries&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf36a7c1" class="outline-2"&gt;
&lt;h2 id="orgf36a7c1"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf36a7c1"&gt;
&lt;script language="javascript" type="text/javascript" src="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/nested-follower/follower.js"&gt;&lt;/script&gt;
&lt;div id="nested-follower"&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/**
 * Random Walker
 *
 * This is an implementation of the Random Walker based on the example given in
 * "The Nature of Code"
 */

// This is the div where the canvas will be placed
let nested_parent_div_id = "nested-follower";

/**
 * The sketch creator
 * 
 * @param {P5} p
 */
let nested_follower_sketch = function(p) {
    /**
     * Setup the canvas
     *
     * - Attaches the canvas to the div
     * - Creates the walker objects
     */
    p.setup = function() {
	this.canvas = p.createCanvas($("#" + nested_parent_div_id).outerWidth(true), 800);
	p.parent = new NoiseWalker(p);
	p.followers = [new Follower(p, p.parent), new Follower(p, p.parent), new Follower(p, p.parent)];
    };

    /**
     * Refresh the objects by calling their update functions
     *
     * This also clears the background.
     */
    p.draw = function() {
	p.background(255);
	p.parent.update();
	p.followers.forEach(function(follower) {
	    follower.update();
	});
    };
};

/**
 * The main walker (with perlin noise)
 *
 * @param {P5} p
 */
function NoiseWalker(p) {
    this.position = p.createVector(p.width/2, p.height/2);
    this.velocity = p.createVector(0, 0);
    this.weight = p.round(p.random(5, 10));

    /**
     * Updates the walker's position
     */
    this.walk = function() {
	// calling sub on the vectors does an in-place update
	// using p5.Vector.sub creates a new vector
	// This is a static method so we use the module (p5) not the instance (p)
	acceleration = mouse.sub(this.position);

	// setMag always produces the same magnitude (but the orientation stays the same)
	acceleration.setMag(this.magnitude);
	console.log(acceleration);
	this.velocity = this.velocity.add(acceleration);
	this.position = this.position.add(this.velocity);

	// keep it within the window
	if (this.position.x &amp;lt; 0)
	    this.position.x = p.width;
	else if (this.position.x &amp;gt; p.width)
	    this.position.x = 0;
	if (this.position.y &amp;lt; 0)
	    this.position.y = p.height
	else if (this.position.y &amp;gt; p.height)
	    this.position.y = 0
  }

    this.display = function() {
	p.strokeWeight(this.weight);
      p.stroke(0);
      p.noFill();
      p.background(255, 255, 255, 10);
      p.circle(this.position.x, this.position.y, 48);
  }

    this.update = function() {
	this.walk();
	this.display()
    }
}

function Follower(p, parent) {
    this.parent = parent;
    this.position = p.createVector(p.random(p.width), p.random(p.height));
    this.velocity = p.createVector(0, 0);
    this.magnitude = p.random(0.05, 0.09);

    this.red = [63, 123, 191, 191, 191];
    this.green = [63, 63, 63, 63, 63];
    this.blue = [191, 191, 191, 127, 63];
    this.colors = this.red.length;
    this.color = p.round(p.random(this.colors));

    this.walk = function() {
	acceleration = this.parent.position.sub(this.position);

	// setMag always produces the same magnitude (but the orientation stays the same)
	acceleration.setMag(this.magnitude);
	this.velocity = this.velocity.add(acceleration);
	this.position = this.position.add(this.velocity)
    }

    this.display = function() {
	p.strokeWeight(p.random(5, 10));
	p.stroke(this.red[this.color], this.green[0], this.blue[0]);
	this.color = (this.color + 1) % this.colors;
	p.noFill();
	p.ellipse(this.position.x, this.position.y, p.random(10, 45), p.random(10, 45));
    }

    this.update = function() {
	this.walk();
	this.display();
    }
}

new p5(nested_follower_sketch, nested_parent_div_id);
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga2503bf" class="outline-2"&gt;
&lt;h2 id="orga2503bf"&gt;End&lt;/h2&gt;
&lt;/div&gt;</description><category>p5.js</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/nested-follower/</guid><pubDate>Tue, 23 Jul 2019 20:49:03 GMT</pubDate></item><item><title>A Mouse Follower</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-mouse-follower/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-mouse-follower/#org1b9f699"&gt;Beginning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-mouse-follower/#org779be40"&gt;Middle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-mouse-follower/#orgd863619"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1b9f699" class="outline-2"&gt;
&lt;h2 id="org1b9f699"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1b9f699"&gt;
&lt;p&gt;
Instead of a random walker this walker will be attracted (somewhat) to the mouse cursor.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org779be40" class="outline-2"&gt;
&lt;h2 id="org779be40"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org779be40"&gt;
&lt;script language="javascript" type="text/javascript" src="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-mouse-follower/follower.js"&gt;&lt;/script&gt;
&lt;div id="mouse-follower"&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;let parent_div_id = "mouse-follower";

let mouse_follower_sketch = function(p) {
    p.setup = function() {
	this.canvas = p.createCanvas($("#" + parent_div_id).outerWidth(true), 800);
	p.walker = new MouseWalker(p);
    }

    p.draw = function() {
	p.background(255);
	p.walker.walk();
	p.walker.display();
    }
};

function MouseWalker(p) {
    this.position = p.createVector(p.width/2, p.height/2);
    this.velocity = p.createVector(0, 0)

    this.walk = function() {
	mouse = p.createVector(p.mouseX, p.mouseY);
	// calling sub on the vectors does an in-place update
	// using p5.Vector.sub creates a new vector
	// This is a static method so we use the module (p5) not the instance (p)
	acceleration = mouse.sub(this.position);

	// setMag always produces the same magnitude (but the orientation stays the same)
	acceleration.setMag(0.1);
	this.velocity = this.velocity.add(acceleration)
	this.position = this.position.add(this.velocity)
  }

  this.display = function() {
      p.stroke(0);
      p.noFill();
      p.background(255, 255, 255, 25);
      p.ellipse(this.position.x, this.position.y, 48, 48);
  }
}

sketch_container = new p5(mouse_follower_sketch, parent_div_id);
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd863619" class="outline-2"&gt;
&lt;h2 id="orgd863619"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd863619"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Shiffman D. &lt;a href="https://natureofcode.com/"&gt;The nature of code&lt;/a&gt;: simulating natural systems with processing. Version 1.0, generated December 6, 2012. s.l.: Selbstverl.; 2012. 498 p.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>p5.js</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-mouse-follower/</guid><pubDate>Sun, 21 Jul 2019 23:03:37 GMT</pubDate></item><item><title>A Random Accelerator</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-accelerator/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-accelerator/#org345f6ab"&gt;Beginning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-accelerator/#org48ab9b2"&gt;Middle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-accelerator/#org1c3623a"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org345f6ab" class="outline-2"&gt;
&lt;h2 id="org345f6ab"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org345f6ab"&gt;
&lt;p&gt;
This is an extension of the random walker with acceleration added.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org48ab9b2" class="outline-2"&gt;
&lt;h2 id="org48ab9b2"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org48ab9b2"&gt;
&lt;script language="javascript" type="text/javascript" src="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-accelerator/walker.js"&gt;&lt;/script&gt;
&lt;div id="random-accelerator"&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;let random_accelerator_sketch = function(p) {
    p.setup = function() {
	let parent_div_id = "random-accelerator";
	this.canvas = p.createCanvas($("#" + parent_div_id).outerWidth(true), 800);
	this.canvas.parent(parent_div_id);
	p.walker = new Walker(p);
    }

    p.draw = function() {
	p.background(255);
	p.walker.walk();
	p.walker.display();
    }
};

function Walker(p) {
    this.position = p.createVector(p.width/2, p.height/2);
    this.velocity = p.createVector(0, 0)

    this.walk = function() {
	acceleration = p.createVector(p.random(-1, 1), p.random(-1, 1));
	acceleration = acceleration.mult(0.1)
	this.velocity = this.velocity.add(acceleration)
	this.position = this.position.add(this.velocity)
  }

  this.display = function() {
      p.stroke(0);
      p.noFill();
      p.background(255, 255, 255, 25);
      p.ellipse(this.position.x, this.position.y, 48, 48);
  }
}

sketch_container = new p5(random_accelerator_sketch, 'random-accelerator');
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1c3623a" class="outline-2"&gt;
&lt;h2 id="org1c3623a"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1c3623a"&gt;
&lt;p&gt;
This was a very rudimentary walker, the main point of it was that at this point we have the basic kinematic elements to make something following the rules of classical physics (more or less).
&lt;/p&gt;

&lt;ol class="org-ol"&gt;
&lt;li&gt;Shiffman D. &lt;a href="https://natureofcode.com/"&gt;The nature of code&lt;/a&gt;: simulating natural systems with processing. Version 1.0, generated December 6, 2012. s.l.: Selbstverl.; 2012. 498 p.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>p5.js</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-accelerator/</guid><pubDate>Sun, 21 Jul 2019 22:14:42 GMT</pubDate></item><item><title>A Random Walk(er)</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-walker/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-walker/#org0e33bfe"&gt;Beginning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-walker/#org782c2b1"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-walker/#org8821ffb"&gt;A Div to Locate the Sketch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-walker/#org7d5d788"&gt;The Javascript&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-walker/#orgc096d32"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0e33bfe" class="outline-2"&gt;
&lt;h2 id="org0e33bfe"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0e33bfe"&gt;
&lt;p&gt;
This is another post to see if I understand how to get &lt;a href="https://p5js.org/"&gt;p5.js&lt;/a&gt; working in nikola. It's been a while since I tried and I just want to see if I remember how. This uses the random walk example from Daniel Schiffman's book &lt;i&gt;the Nature of Code&lt;/i&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org782c2b1" class="outline-2"&gt;
&lt;h2 id="org782c2b1"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org782c2b1"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8821ffb" class="outline-3"&gt;
&lt;h3 id="org8821ffb"&gt;A Div to Locate the Sketch&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8821ffb"&gt;
&lt;p&gt;
The id of this div is set in the &lt;code&gt;p5.js&lt;/code&gt; &lt;code&gt;setup&lt;/code&gt; function as the parent of the sketch.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;script language="javascript" type="text/javascript" src="walker.js"&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;div id="random-walk-container"&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;script language="javascript" type="text/javascript" src="https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-walker/walker.js"&gt;&lt;/script&gt;
&lt;div id="random-walk-container"&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;b&gt;Note:&lt;/b&gt; Originally this wasn't working, because I had the line to include the javascript inside the &lt;code&gt;div&lt;/code&gt; to hold the canvas. Make sure that &lt;code&gt;div&lt;/code&gt; is always empty.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7d5d788" class="outline-3"&gt;
&lt;h3 id="org7d5d788"&gt;The Javascript&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7d5d788"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;let sketch = function(p) {
    p.setup = function() {
	let parent_div_id = "random-walk-container";
	this.canvas = p.createCanvas($("#" + parent_div_id).outerWidth(true), 300);
	this.canvas.parent();
	p.walker = new Walker(p);
    }

    p.draw = function() {
	p.background(255);
	p.walker.walk();
	p.walker.display();
    }
};

function Walker(p) {
  this.x = p.width/2;
  this.y = p.height/2;

  this.walk = function() {
    this.x = this.x + p.random(-1, 1) * 10;
    this.y = this.y + p.random(-1, 1) * 10;
  }

  this.display = function() {
    p.fill(0);
    p.ellipse(this.x, this.y, 48, 48);
  }
}

//let node = document.getElementById("random-walk")
//window.document.getElementsByTagName("body")[0].appendChild(node);
sketch_container = new p5(sketch, 'random-walk-container');
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc096d32" class="outline-2"&gt;
&lt;h2 id="orgc096d32"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc096d32"&gt;
&lt;p&gt;
As always, this was way harder than it should have been.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>javascript</category><category>p5.js</category><category>processing</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/nature-of-code/a-random-walker/</guid><pubDate>Sun, 21 Jul 2019 19:29:09 GMT</pubDate></item><item><title>The Origin of Bayes Theorem</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/notes/the-origin-of-bayes-theorem/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/notes/the-origin-of-bayes-theorem/#org3510a30"&gt;A Brief Sketch of The Timelines to Bayes' Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/notes/the-origin-of-bayes-theorem/#org9b573cc"&gt;The Equations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/notes/the-origin-of-bayes-theorem/#orgc8a5c0e"&gt;Bayes' Formulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/notes/the-origin-of-bayes-theorem/#org6d661d8"&gt;Laplace's First Version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/notes/the-origin-of-bayes-theorem/#org5ed47bd"&gt;Laplace's Final Version&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/notes/the-origin-of-bayes-theorem/#org457e64d"&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
I'm reading "The theory that would not die" and these are notes I took from them. The book didn't really give me a clear idea about what Price's argument was so I also read a &lt;a href="https://qz.com/1315731/the-most-important-formula-in-data-science-was-first-used-to-prove-the-existence-of-god/"&gt;Quartz&lt;/a&gt; article about that part of the story and, of course, Wikipedia came into it at some points.
&lt;/p&gt;
&lt;div id="outline-container-org3510a30" class="outline-2"&gt;
&lt;h2 id="org3510a30"&gt;A Brief Sketch of The Timelines to Bayes' Theorem&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3510a30"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;1718: &lt;a href="https://www.wikiwand.com/en/Abraham_de_Moivre"&gt;Abraham de Moivre&lt;/a&gt; publishes &lt;a href="https://www.wikiwand.com/en/The_Doctrine_of_Chances"&gt;The Doctrine of Chances&lt;/a&gt;, the first textbook on probability.&lt;/li&gt;
&lt;li&gt;1746-1749: Somewhere in this period &lt;a href="https://www.wikiwand.com/en/Thomas_Bayes"&gt;Thomas Bayes&lt;/a&gt; comes writes &lt;a href="https://www.wikiwand.com/en/An_Essay_towards_solving_a_Problem_in_the_Doctrine_of_Chances"&gt;An Essay towards solving a Problem in the Doctrine of the Chances&lt;/a&gt; which describes elements of Inverse Probability, in which the probability of a cause is calculated based on observed effects, stated as a thought experiment in which a person turned away from a table estimates the position of a ball based on being told whether subsequent balls randomly dropped on the same table are to the left or the right of it.&lt;/li&gt;
&lt;li&gt;1748: &lt;a href="https://www.wikiwand.com/en/David_Hume"&gt;David Hume&lt;/a&gt; publishes &lt;a href="https://www.wikiwand.com/en/Of_Miracles"&gt;Of Miracles&lt;/a&gt;, in which he argues that since miracles are, by nature, singular, they can never have as much evidence in their favor as against them.&lt;/li&gt;
&lt;li&gt;1749: &lt;a href="https://www.wikiwand.com/en/Pierre-Simon_Laplace"&gt;Pierre-Simon Laplace&lt;/a&gt; is born&lt;/li&gt;
&lt;li&gt;1764: &lt;a href="https://www.wikiwand.com/en/Richard_Price"&gt;Richard Price&lt;/a&gt; publishes &lt;a href="https://www.wikiwand.com/en/An_Essay_towards_solving_a_Problem_in_the_Doctrine_of_Chances"&gt;An Essay towards solving a Problem in the Doctrine of the Chances&lt;/a&gt; with his additions, believing that it could act as a refutation of Hume's argument&lt;/li&gt;
&lt;li&gt;1774: Laplace comes up with idea that the probability of a cause given the observed effect is the ratio of the probability of that effect given the cause to sum of the probabilities for all other causes given that effect.&lt;/li&gt;
&lt;li&gt;1781: Price tells &lt;a href="https://www.wikiwand.com/en/Marquis_de_Condorcet"&gt;the Marquis of Condorcet&lt;/a&gt; about Bayes' work and Laplace incorporates the use of the prior into his formulation&lt;/li&gt;
&lt;li&gt;1810: Laplace discovers &lt;a href="https://www.wikiwand.com/en/Central_limit_theorem"&gt;the Central Limit Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;1814: Laplace extends his version of Bayes' equation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9b573cc" class="outline-2"&gt;
&lt;h2 id="org9b573cc"&gt;The Equations&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9b573cc"&gt;
&lt;p&gt;
Since it's hard to write out the equations in bullet points I'm going to write some simple versions here.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc8a5c0e" class="outline-3"&gt;
&lt;h3 id="orgc8a5c0e"&gt;Bayes' Formulation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc8a5c0e"&gt;
&lt;p&gt;
"The theory that would not die" notes that Bayes' didn't write out an equation, but it can be written out something like this.
\[
P(\textit{cause}|\textit{effect}) = \frac{P(\textit{effect}|\textit{cause}) P(\textit{cause})}{P(\textit{effect})}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6d661d8" class="outline-3"&gt;
&lt;h3 id="org6d661d8"&gt;Laplace's First Version&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6d661d8"&gt;
&lt;p&gt;
Originally Laplace didn't have the prior's in his equation (I'll substitute &lt;i&gt;C&lt;/i&gt; for &lt;i&gt;cause&lt;/i&gt;, &lt;i&gt;E&lt;/i&gt; for &lt;i&gt;effect&lt;/i&gt; and &lt;i&gt;C'&lt;/i&gt; for not our theorized cause).
\[
P(C|E) = \frac{P(E|C)}{\sum P(E|C')}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5ed47bd" class="outline-3"&gt;
&lt;h3 id="org5ed47bd"&gt;Laplace's Final Version&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5ed47bd"&gt;
&lt;p&gt;
\[
P(C|E) = \frac{P(E|C)P_{\textit{prior}}(C)}{\sum P(E|C') P_{\textit{prior}} (C')}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org457e64d" class="outline-2"&gt;
&lt;h2 id="org457e64d"&gt;Sources&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org457e64d"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;McGrayne SB. The theory that would not die: how Bayes’ rule cracked the enigma code, hunted down Russian submarines, and emerged triumphant from two centuries of controversy. paperback ed. New Haven, Conn.: Yale University Press; 2011. 336 p.&lt;/li&gt;

&lt;li&gt;Kopf D. The most important formula in data science was first used to prove the existence of God [Internet]. Quartz. [cited 2019 May 12]. Available from: &lt;a href="https://qz.com/1315731/the-most-important-formula-in-data-science-was-first-used-to-prove-the-existence-of-god/"&gt;https://qz.com/1315731/the-most-important-formula-in-data-science-was-first-used-to-prove-the-existence-of-god/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>bayes theorem</category><category>history</category><category>notes</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/notes/the-origin-of-bayes-theorem/</guid><pubDate>Sun, 12 May 2019 21:02:18 GMT</pubDate></item><item><title>Looking at random graphs</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#org33dbd99"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#orgdff7cf6"&gt;Part 1 - Random Graph Identification&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#org643548a"&gt;Load the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#org0f77442"&gt;Graph Identification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#org3883dcc"&gt;Part 2 - Company Emails&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#org7e40ad3"&gt;Part 2A - Salary Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#org759157e"&gt;Part 2B - New Connections Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#org974d867"&gt;Separate the Target and Training Sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#org61bd2bb"&gt;Scaling the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/#orgb71a6eb"&gt;Feature Selection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org33dbd99" class="outline-2"&gt;
&lt;h2 id="org33dbd99"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org33dbd99"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# python standard library
import os
import pickle

# from pypi
import networkx
import numpy
import pandas

from sklearn.linear_model import LogisticRegressionCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import (
    ExtraTreesClassifier,
    RandomForestClassifier,
    )
from sklearn.feature_selection import (
    RFECV,
    SelectFromModel,
)
from sklearn.model_selection import (
    GridSearchCV,
    StratifiedKFold,
    train_test_split,
    )
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;% matplotlib inline
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdff7cf6" class="outline-2"&gt;
&lt;h2 id="orgdff7cf6"&gt;Part 1 - Random Graph Identification&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdff7cf6"&gt;
&lt;p&gt;
For the first part of this assignment you will analyze randomly generated graphs and determine which algorithm created them.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org643548a" class="outline-3"&gt;
&lt;h3 id="org643548a"&gt;Load the data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org643548a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;part_one_graphs = pickle.load(open('A4_graphs','rb'))
print(len(part_one_graphs))
print(type(part_one_graphs[0]))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
&lt;code&gt;part_one_graphs&lt;/code&gt; is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;Preferential Attachment (`'PA'`)&lt;/li&gt;
&lt;li&gt;Small World with low probability of rewiring (`'SW&lt;sub&gt;L&lt;/sub&gt;'`)&lt;/li&gt;
&lt;li&gt;Small World with high probability of rewiring (`'SW&lt;sub&gt;H&lt;/sub&gt;'`)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Analyze each of the 5 graphs and determine which of the three algorithms generated the graph.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;The `graph&lt;sub&gt;identification&lt;/sub&gt;` function should return a list of length 5 where each element in the list is either `'PA'`, `'SW&lt;sub&gt;L&lt;/sub&gt;'`, or `'SW&lt;sub&gt;H&lt;/sub&gt;'`.&lt;/b&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0f77442" class="outline-3"&gt;
&lt;h3 id="org0f77442"&gt;Graph Identification&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0f77442"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def graph_identification():
    """Identifies the type of graph each of the graphs is

    Returns:
     list: string identifiers for the type of graph
    """
    graph_types = []
    for graph in part_one_graphs:
	path = networkx.average_shortest_path_length(graph)
	coefficient = networkx.average_clustering(graph)
	if path &amp;gt; 6:
	    if coefficient &amp;lt; 0.5:
		graph_types.append("SW_L")
	    else:
		raise Exception("unexpected type")
	else:
	    if coefficient &amp;lt; 0.5:
		graph_types.append("PA")
	    else:
		graph_types.append("SW_H")
    return graph_types
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This was marked wrong by the grader.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3883dcc" class="outline-2"&gt;
&lt;h2 id="org3883dcc"&gt;Part 2 - Company Emails&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3883dcc"&gt;
&lt;p&gt;
For the second part of this assignment you will be working with a company's email network where each node corresponds to a person at the company, and each edge indicates that at least one email has been sent between two people.
&lt;/p&gt;

&lt;p&gt;
The network also contains the node attributes `Department` and `ManagementSalary`.
&lt;/p&gt;

&lt;p&gt;
`Department` indicates the department in the company which the person belongs to, and `ManagementSalary` indicates whether that person is receiving a managment position salary.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;email = networkx.read_gpickle('email_prediction.txt')
print(networkx.info(email))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7e40ad3" class="outline-3"&gt;
&lt;h3 id="org7e40ad3"&gt;Part 2A - Salary Prediction&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7e40ad3"&gt;
&lt;p&gt;
Using network `email`, identify the people in the network with missing values for the node attribute `ManagementSalary` and predict whether or not these individuals are receiving a managment position salary.
&lt;/p&gt;

&lt;p&gt;
To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn classifier on nodes that have `ManagementSalary` data, and predict a probability of the node receiving a managment salary for nodes where `ManagementSalary` is missing.
&lt;/p&gt;

&lt;p&gt;
Your predictions will need to be given as the probability that the corresponding employee is receiving a managment position salary.
&lt;/p&gt;

&lt;p&gt;
The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
&lt;/p&gt;

&lt;p&gt;
Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.75 or higher will receive full points.
&lt;/p&gt;

&lt;p&gt;
Using your trained classifier, return a series of length 252 with the data being the probability of receiving managment salary, and the index being the node id.
&lt;/p&gt;

&lt;pre class="example"&gt;
1       1.0
2       0.0
5       0.8
8       1.0
    ...
996     0.7
1000    0.5
1001    0.0
Length: 252, dtype: float64
&lt;/pre&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge5159b4" class="outline-4"&gt;
&lt;h4 id="orge5159b4"&gt;The Data Frame&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge5159b4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if not os.path.isfile("email_data.h5"):
    data = pandas.DataFrame(index=email.nodes())
    data["department"] = pandas.Series(networkx.get_node_attributes(email, "Department"))
    data["management"] = pandas.Series(networkx.get_node_attributes(email, "ManagementSalary"))
    data["clustering"] = pandas.Series(networkx.clustering(email))
    data["degree"] = pandas.Series(email.degree())
    data["degree_centrality"] = pandas.Series(networkx.degree_centrality(email))
    data["closeness_centrality"] = pandas.Series(networkx.closeness_centrality(email))
    data["betweenness_centrality"] = pandas.Series(networkx.betweenness_centrality(email))
    data["pagerank"] = pandas.Series(networkx.pagerank(email))
    _, authority = networkx.hits(email)
    data["authority"] = pandas.Series(authority)
    data.to_hdf("email_data.h5","df" )
else:
    data = pandas.read_hdf('email_data.h5', "df")
print(data.head())    
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(data.management.unique())
print(data.department.unique())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga644ae1" class="outline-4"&gt;
&lt;h4 id="orga644ae1"&gt;Department Dummy Variables&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga644ae1"&gt;
&lt;p&gt;
Even though I don't think it's going to prove useful, the &lt;code&gt;department&lt;/code&gt; feature is actually categorical, despite the use of integers so we'll have to use One-Hot-Encoding to add dummy variables for it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dummies_data = pandas.get_dummies(data, columns=["department"])
print(dummies_data.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb540acb" class="outline-4"&gt;
&lt;h4 id="orgb540acb"&gt;Separating the Training and Prediction Sets&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb540acb"&gt;
&lt;p&gt;
We're going to use the model to predict what the missing &lt;code&gt;management&lt;/code&gt; values are so I'm going to separate the missing and non-missing sets. 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_data = dummies_data[pandas.notnull(dummies_data.management)]
prediction_data = dummies_data[pandas.isnull(dummies_data.management)]
print(training_data.shape)
print(prediction_data.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The problem description tells us that the answer should have 252 entries so this is a safe assertion.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;assert len(prediction_data) == 252
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd47b865" class="outline-4"&gt;
&lt;h4 id="orgd47b865"&gt;Training and Target Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd47b865"&gt;
&lt;p&gt;
To train the model we'll need to separate out the &lt;code&gt;management&lt;/code&gt; column (and remove it entirely from the &lt;code&gt;prediction&lt;/code&gt; set).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;non_management = [column for column in training_data.columns if column != "management"]
y_train = training_data.management
x_train = training_data[non_management]
x_predict = prediction_data[non_management]
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org958c1f2" class="outline-4"&gt;
&lt;h4 id="org958c1f2"&gt;Scaling&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org958c1f2"&gt;
&lt;p&gt;
I don't think the Random Forest model that I'm going to use needs it, but I'm going to standardize the data.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_predict = pandas.DataFrame(scaler.transform(x_predict), index=x_predict.index)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3cc1309" class="outline-4"&gt;
&lt;h4 id="org3cc1309"&gt;Feature Selection&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3cc1309"&gt;
&lt;p&gt;
Since we now have so many features, I'm going to do some feature selection.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train.shape)
print(x_predict.shape)
trees = ExtraTreesClassifier(n_estimators=10)
eliminator = RFECV(estimator=trees, cv=StratifiedKFold(10), scoring="roc_auc")
eliminator.fit(x_train, y_train)
x_train_reduced = eliminator.transform(x_train)
x_predict_reduced = pandas.DataFrame(eliminator.transform(x_predict), index=x_predict.index)
print(x_train_reduced.shape)
print(x_predict_reduced.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
When I used the train-test-split training model it left 17 columns. I wonder if using the whole training set messes it up.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0408205" class="outline-4"&gt;
&lt;h4 id="org0408205"&gt;Logistic Regression&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0408205"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = LogisticRegressionCV(penalty="l1", scoring="roc_auc",
			     solver="liblinear", cv=StratifiedKFold(10))
model.fit(x_train_reduced, y_train)
print(model.scores_[1.0].mean())
print(model.scores_[1.0].std())
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
It seems to be doing much worse than when I used the train-test split.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3630724" class="outline-4"&gt;
&lt;h4 id="org3630724"&gt;Random Forests&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3630724"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameter_grid = dict(n_estimators=range(10, 100, 10))
search = GridSearchCV(RandomForestClassifier(), parameter_grid,
		      cv=StratifiedKFold(10), scoring="roc_auc")
search.fit(x_train_reduced, y_train)
print(search.best_score_)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class RandomForest(object):
    """builds the random forest

    Args:
     x_train(array): data to train on
     y_train(array): targets for training
     start (int): start value for number of estimators
     stop (int): upper value for range of estimators
     step (int): increment for range of estimators
     folds (int): K-folds for cross-validation    
    """
    def __init__(self, x_train, y_train,
		 start=10, stop=100, step=10, folds=10):
	self.x_train = x_train
	self.y_train = y_train
	self.start = start
	self.stop = stop
	self.step = step
	self.folds = folds
	self._parameters = None
	self._search = None
	self._model = None
	return

    @property
    def parameters(self):
	"""parameters for the grid-search"""
	if self._parameters is None:
	    self._parameters = dict(n_estimators=range(self.start,
						       self.stop,
						       self.step))
	return self._parameters

    @property
    def search(self):
	"""fitted grid search to find hyper-parameters"""
	if self._search is None:
	    self._search = GridSearchCV(RandomForestClassifier(),
					self.parameters,
					cv=StratifiedKFold(self.folds),
					scoring="roc_auc")
	    self._search.fit(self.x_train, self.y_train)
	return self._search

    @property
    def model(self):
	"""best model found by the grid search"""
	if self._model is None:
	    self._model = self.search.best_estimator_
	return self._model
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9d9e129" class="outline-4"&gt;
&lt;h4 id="org9d9e129"&gt;Data Loader&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9d9e129"&gt;
&lt;p&gt;
Since having all these org-babel things around makes things kind of hard I'm going to make a class to bundle everything together.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DataLoader(object):
    """loads and transforms the data
    Args:
     estimators (int): number of trees to use for feature elimination
    """
    def __init__(self, estimators=10):
	self.estimators = estimators
	self._data = None
	self._dummies_data = None
	self._training_data = None
	self._prediction_data = None
	self._non_management = None
	self._y_train = None
	self._x_train = None
	self._x_predict = None
	self._scaler = None
	self._x_train_scaled = None
	self._x_predict_scaled = None
	self._eliminator = None
	self._x_train_reduced = None
	self._x_predict_reduced = None
	return

    @property
    def data(self):
	"""The initial data"""
	if self._data is None:
	    if not os.path.isfile("email_data.h5"):
		data = pandas.DataFrame(index=email.nodes())
		data["department"] = pandas.Series(networkx.get_node_attributes(email, "Department"))
		data["management"] = pandas.Series(networkx.get_node_attributes(email, "ManagementSalary"))
		data["clustering"] = pandas.Series(networkx.clustering(email))
		data["degree"] = pandas.Series(email.degree())
		data["degree_centrality"] = pandas.Series(networkx.degree_centrality(email))
		data["closeness_centrality"] = pandas.Series(networkx.closeness_centrality(email))
		data["betweenness_centrality"] = pandas.Series(networkx.betweenness_centrality(email))
		data["pagerank"] = pandas.Series(networkx.pagerank(email))
		_, authority = networkx.hits(email)
		data["authority"] = pandas.Series(authority)
		data.to_hdf("email_data.h5","df" )
		self._data = data
	    else:
		self._data = pandas.read_hdf('email_data.h5', "df")
	return self._data

    @property
    def dummies_data(self):
	"""one-hot-encoded data"""
	if self._dummies_data is None:
	    self._dummies_data = pandas.get_dummies(self.data, columns=["department"])
	return self._dummies_data

    @property
    def training_data(self):
	"""data with management information"""
	if self._training_data is None:
	    self._training_data = self.dummies_data[pandas.notnull(
		self.dummies_data.management)]
	return self._training_data

    @property
    def prediction_data(self):
	"""data missing management information"""
	if self._prediction_data is None:
	    self._prediction_data = self.dummies_data[pandas.isnull(
		self.dummies_data.management)]
	    assert len(self._prediction_data) == 252
	return self._prediction_data

    @property
    def non_management(self):
	"""list of columns minus management"""
	if self._non_management is None:
	    self._non_management = [
		column for column in self.training_data.columns
		if column != "management"]
	return self._non_management

    @property
    def y_train(self):
	"""target-data for training"""
	if self._y_train is None:
	    self._y_train = self.training_data.management
	return self._y_train

    @property
    def x_train(self):
	"""data for training"""
	if self._x_train is None:
	    self._x_train = self.training_data[self.non_management]
	return self._x_train

    @property
    def x_predict(self):
	"""set to make predictions"""
	if self._x_predict is None:
	    self._x_predict = self.prediction_data[self.non_management]
	return self._x_predict

    @property
    def scaler(self):
	"""standard scaler"""
	if self._scaler is None:
	    self._scaler = StandardScaler()
	return self._scaler

    @property
    def x_train_scaled(self):
	"""training data scaled to 1 std, 0 mean"""
	if self._x_train_scaled is None:
	    self._x_train_scaled = self.scaler.fit_transform(self.x_train)
	return self._x_train_scaled

    @property
    def x_predict_scaled(self):
	"""prediction data with mean 0, std 1

	The answer requires the index so this is a dataframe
	instead of an array

	Returns:
	 pandas.DataFrame: scaled data with index preserved
	"""
	if self._x_predict_scaled is None:
	    self._x_predict_scaled = pandas.DataFrame(
		self.scaler.transform(self.x_predict),
		index=self.x_predict.index)
	return self._x_predict_scaled

    @property
    def eliminator(self):
	"""recursive feature eliminator"""
	if self._eliminator is None:
	    trees = ExtraTreesClassifier(n_estimators=10)
	    self._eliminator = RFECV(estimator=trees, cv=StratifiedKFold(10), 
				     scoring="roc_auc")
	    self._eliminator.fit(self.x_train_scaled, self.y_train)
	return self._eliminator

    @property
    def x_train_reduced(self):
	"""training data with features eliminated"""
	if self._x_train_reduced is None:
	    self._x_train_reduced = self.eliminator.transform(
		self.x_train_scaled)
	return self._x_train_reduced

    @property
    def x_predict_reduced(self):
	"""prediction data with features eliminated"""
	if self._x_predict_reduced is None:
	    self._x_predict_reduced = pandas.DataFrame(
		self.eliminator.transform(self.x_predict_scaled),
		index=self.x_predict_scaled.index)
	return self._x_predict_reduced
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org82af0de" class="outline-4"&gt;
&lt;h4 id="org82af0de"&gt;Submission&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org82af0de"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def salary_predictions():
    """Prediction that employee is management

    Calculates the probability that an employee is management

    Returns:
     pandas.Series: Node ID, probability of node
    """
    data = DataLoader()
    forest = RandomForest(data.x_train_reduced, data.y_train)
    # probabilites is an array with rows of 
    # [&amp;lt;probability not management&amp;gt;, &amp;lt;probability management&amp;gt;]
    # see forest.model.classes_ to see what each entry represents
    probabilities = forest.model.predict_proba(data.x_predict_reduced)
    return pandas.Series(probabilities[:, 1], index=data.x_predict_reduced.index)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;output = salary_predictions()
print(output.head())
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;assert all(output.index == DataLoader().prediction_data.index)
assert len(output) == 252
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org759157e" class="outline-3"&gt;
&lt;h3 id="org759157e"&gt;Part 2B - New Connections Prediction&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org759157e"&gt;
&lt;p&gt;
For the last part of this assignment, you will predict future connections between employees of the network. The future connections information has been loaded into the variable `future&lt;sub&gt;connections&lt;/sub&gt;`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;future_connections = pandas.read_csv('Future_Connections.csv', index_col=0, converters={0: eval})
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections.head(10))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections['Future Connection'].value_counts())
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;
Using network `G` and `future&lt;sub&gt;connections&lt;/sub&gt;`, identify the edges in `future&lt;sub&gt;connections&lt;/sub&gt;` with missing values and predict whether or not these edges will have a future connection.
&lt;/p&gt;

&lt;p&gt;
To accomplish this, you will need to create a matrix of features for the edges found in `future&lt;sub&gt;connections&lt;/sub&gt;` using networkx, train a sklearn classifier on those edges in `future&lt;sub&gt;connections&lt;/sub&gt;` that have `Future Connection` data, and predict a probability of the edge being a future connection for those edges in `future&lt;sub&gt;connections&lt;/sub&gt;` where `Future Connection` is missing.
&lt;/p&gt;

&lt;p&gt;
Your predictions will need to be given as the probability of the corresponding edge being a future connection.
&lt;/p&gt;

&lt;p&gt;
The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
&lt;/p&gt;

&lt;p&gt;
Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.75 or higher will receive full points.
&lt;/p&gt;

&lt;p&gt;
Using your trained classifier, return a series of length 122112 with the data being the probability of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.
&lt;/p&gt;

&lt;pre class="example"&gt;
(107, 348)    0.35
(542, 751)    0.40
(20, 426)     0.55
(50, 989)     0.35
          ...
(939, 940)    0.15
(555, 905)    0.35
(75, 101)     0.65
Length: 122112, dtype: float64
&lt;/pre&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb76cec6" class="outline-4"&gt;
&lt;h4 id="orgb76cec6"&gt;Add Network Features&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb76cec6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Futures(object):
    target = "Future Connection"
    data_file = "Future_Connections.csv"
    graph_file = "email_prediction.txt"
    networkx_data_index = 2
    folds = 10
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DataNames(object):
    resource_allocation = 'resource_allocation'
    jaccard = 'jaccard_coefficient'
    adamic = "adamic_adar"
    preferential = "preferential_attachment"
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def add_networkx_data(adder, name, graph=email, frame=future_connections):
    """Adds networkx data to the frame

    The networkx link-prediction functions return generators of triples:
     (first-node, second-node, value)

    This will use the index of the frame that's passed in as the source of 
    node-pairs for the networkx function (called `ebunch` in the networkx
    documentation) and the add only the value we want back to the frame

    Args:
     adder: networkx function to call to get the new data
     name: column-name to add to the frame
     graph: networkx graph to pass to the function
     frame (pandas.DataFrame): frame with node-pairs as index to add data to
    """
    frame[name] = [output[Futures.networkx_data_index]
		   for output in adder(graph, frame.index)]
    return frame
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf8f486f" class="outline-5"&gt;
&lt;h5 id="orgf8f486f"&gt;Adding A Resource Allocation Index&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-orgf8f486f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;add_networkx_data(networkx.resource_allocation_index,
		  DataNames.resource_allocation)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd96111a" class="outline-5"&gt;
&lt;h5 id="orgd96111a"&gt;Adding the Jaccard Coefficient&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-orgd96111a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;add_networkx_data(networkx.jaccard_coefficient, DataNames.jaccard)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org651bbdb" class="outline-5"&gt;
&lt;h5 id="org651bbdb"&gt;Adamic Adar&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-org651bbdb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;add_networkx_data(networkx.adamic_adar_index, DataNames.adamic)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbb70ec8" class="outline-5"&gt;
&lt;h5 id="orgbb70ec8"&gt;Preferential Attachment&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-orgbb70ec8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;add_networkx_data(networkx.preferential_attachment, DataNames.preferential)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgfcee2ed" class="outline-4"&gt;
&lt;h4 id="orgfcee2ed"&gt;Setup the Training and Testing Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfcee2ed"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3fb7468" class="outline-5"&gt;
&lt;h5 id="org3fb7468"&gt;Separating the Edges Without 'Future Connection' Values&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-org3fb7468"&gt;
&lt;p&gt;
We are going to train on the values in the data with predictions and then make predictions for those that don't.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;prediction_set = future_connections[future_connections[Futures.target].isnull()]
training_set = future_connections[future_connections[Futures.target].notnull()]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(prediction_set.shape)
print(training_set.shape)
assert len(prediction_set) + len(training_set) == len(future_connections)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org974d867" class="outline-3"&gt;
&lt;h3 id="org974d867"&gt;Separate the Target and Training Sets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org974d867"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;non_target = [column for column in future_connections.columns
	      if column != Futures.target]
x_train = training_set[non_target]
y_train = training_set[Futures.target]
x_predict = prediction_set[non_target]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;assert all(x_train.columns == x_predict.columns)
assert len(x_train) == len(x_test)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org61bd2bb" class="outline-3"&gt;
&lt;h3 id="org61bd2bb"&gt;Scaling the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org61bd2bb"&gt;
&lt;p&gt;
To enable the use of linear models I'm going to scale the data so the mean is 0 and the variance is 1.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_predict_scaled = scaler.transform(x_predict)

x_train_frame = pandas.DataFrame(x_train_scaled, columns=x_train.columns)
x_predict_frame = pandas.DataFrame(x_predict_scaled, columns=x_predict.columns)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(training.describe())
print(predictions.describe())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb71a6eb" class="outline-3"&gt;
&lt;h3 id="orgb71a6eb"&gt;Feature Selection&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb71a6eb"&gt;
&lt;p&gt;
To reduce the dimensionality I'm going to use model-based selection with Extra Trees.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;estimator = ExtraTreesClassifier()
estimator.fit(x_train_scaled, y_train)
selector = SelectFromModel(estimator, prefit=True)
x_train_trees_sfm = selector.transform(x_train_scaled)
x_predict_sfm = selector.transform(x_predict_scaled)
print(estimator.feature_importances_)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train_trees_sfm.shape)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org39fc4fa" class="outline-4"&gt;
&lt;h4 id="org39fc4fa"&gt;Missing Future Connections&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org39fc4fa"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = LogisticRegressionCV(n_jobs=-1, scoring='roc_auc', solver='liblinear',
			     cv=StratifiedKFold())
model.fit(x_train_trees_sfm, y_train)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for scores in model.scores_[1.0]:
    print(max(scores))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(model.classes_)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def new_connections_predictions():    
    probabilities = model.predict_proba(x_predict_sfm)
    return pandas.Series(probabilities[:, 1], index=prediction_set.index)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;outcome = new_connections_predictions()
assert len(outcome) == 122112, len(outcome)
print(outcome.head())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>networks</category><category>random graphs</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/looking-at-random-graphs/</guid><pubDate>Sat, 13 Apr 2019 18:59:44 GMT</pubDate></item><item><title>Selecting the E-Mail Model</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org3b21de6"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org70c949c"&gt;The Data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org64b87d3"&gt;Department&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org8322b55"&gt;Splitting the Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org2766c1d"&gt;Standardizing the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#orgb1d2229"&gt;Dummy Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org1dee0de"&gt;Feature Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org8655983"&gt;Fit and Display&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#orgf60b7de"&gt;Logistic Regression&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#orgde23cbb"&gt;L1 Penalty&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org8d37d18"&gt;Random Forests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org33ee064"&gt;K Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/#org6adf812"&gt;Support Vector Classifier (SVC)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3b21de6" class="outline-2"&gt;
&lt;h2 id="org3b21de6"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3b21de6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# pypi
from sklearn.ensemble import (
    ExtraTreesClassifier,
    RandomForestClassifier,
    )
from sklearn.feature_selection import (
    RFECV,
    SelectFromModel,
)
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import (
    GridSearchCV,
    StratifiedKFold,
    train_test_split,
    )

from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import matplotlib.pyplot as pyplot
import mglearn
import numpy
import pandas
import seaborn
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;% matplotlib inline
seaborn.set_style("whitegrid")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org70c949c" class="outline-2"&gt;
&lt;h2 id="org70c949c"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org70c949c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.read_hdf("email_data.h5", "df")
cleaned_data = data[pandas.notnull(data.management)]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(cleaned_data.head())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org64b87d3" class="outline-3"&gt;
&lt;h3 id="org64b87d3"&gt;Department&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org64b87d3"&gt;
&lt;p&gt;
Even though I don't think it's going to prove useful, the &lt;code&gt;department&lt;/code&gt; feature is actually categorical, despite the use of integers so we'll have to add dummy variables for it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cleaned_data = pandas.get_dummies(cleaned_data, columns=["department"])
print(cleaned_data.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8322b55" class="outline-3"&gt;
&lt;h3 id="org8322b55"&gt;Splitting the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8322b55"&gt;
&lt;p&gt;
For evaluation purposes I'll use the traditional train-test split.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x_data = cleaned_data.loc[:, cleaned_data.columns != "management"]

y_data = cleaned_data.management

print(x_data.head())
print(y_data.head())
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(y_data.value_counts())
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;seaborn.countplot(x='management', data=cleaned_data)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
It looks like the management data is unbalanced, so I'll do a stratified split.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, stratify=y_data)
print(x_train.shape)
print(y_test.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;seaborn.countplot(y_train)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Looks close enough for government work.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2766c1d" class="outline-2"&gt;
&lt;h2 id="org2766c1d"&gt;Standardizing the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org2766c1d"&gt;
&lt;p&gt;
The linear models expect the data to be standardized, so to make the comparisons fair I'll standardize the data first. First, a look at the data before scaling.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train.describe())
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now I'll scale it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scaler = StandardScaler()
scaler.fit(x_train)
x_train = pandas.DataFrame(scaler.transform(x_train), columns=x_train.columns)
x_test = scaler.transform(x_test)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now the means should be near 0 (very small) and the standard deviations should be around 1.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train.describe())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb1d2229" class="outline-2"&gt;
&lt;h2 id="orgb1d2229"&gt;Dummy Classifier&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb1d2229"&gt;
&lt;p&gt;
As a baseline I'll use a &lt;a href="http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators"&gt;Dummy Classifier&lt;/a&gt; which uses a simple rule rather than the input data to make predictions.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameter_grid = dict(strategy=["stratified", 'most_frequent', 'prior', 'uniform'])
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now we'll do a grid search.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;grid_search = GridSearchCV(DummyClassifier(), parameter_grid,
			   cv=StratifiedKFold(10), scoring="roc_auc")
grid_search.fit(x_train, y_train)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;BASELINE = grid_search.score(x_test, y_test)
print(grid_search.best_params_)
print(BASELINE)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
It looks like it chose the &lt;b&gt;stratified&lt;/b&gt; strategy, which should predict that the instances are all non-managers. Our baseline AUC score is 0.5 (0.47 now?).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;results = pandas.DataFrame(grid_search.cv_results_)
print(results.head(1))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure = pyplot.figure()
axe = figure.gca()
strategies = parameter_grid["strategy"]
x = pyplot.xticks(list(range(len(strategies))), strategies)
axe.plot(range(len(strategies)), results.mean_test_score)
axe.set_title("Dummy Classifier Strategy Vs AUC")
axe.set_xlabel("strategy")
axe.set_ylabel("AUC Score")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
So it looks like all the strategies except &lt;b&gt;stratified&lt;/b&gt; did the same - and even the stratified did basically the same if you round it off.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1dee0de" class="outline-2"&gt;
&lt;h2 id="org1dee0de"&gt;Feature Selection&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1dee0de"&gt;
&lt;p&gt;
I'm going to need to do some feature reduction, but figuring out what is important and what isn't is something I'm going to have to leave to the machine. I'm going to assume that the features thrown out by logistic regression with l1 penalization are unimportant. 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;logistic_model = LogisticRegressionCV(penalty='l1',
				      solver='liblinear', scoring="roc_auc")
logistic_model.fit(x_train, y_train)
model = SelectFromModel(logistic_model, prefit=True)

x_train_positive = model.transform(x_train)
x_test_positive = model.transform(x_test)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(logistic_model.score(x_test, y_test))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Logistic Regression with &lt;code&gt;L1&lt;/code&gt; penalty seems to do reasonably well even without feature selection.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;logistic_model.fit(x_train_positive, y_train)
print(logistic_model.score(x_test_positive, y_test))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
It looks like feature selection didn't really help here.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train.shape)
print(x_train_positive.shape)
print(model.ranking_)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
As a double-check I'll use a tree-based, recursive feature-elimination version.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;trees = ExtraTreesClassifier(n_estimators=10)
eliminator = RFECV(estimator=trees, cv=StratifiedKFold(10), scoring="roc_auc")
eliminator.fit(x_train, y_train)
x_train_trees = eliminator.transform(x_train)
x_test_trees = eliminator.transform(x_test)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train_trees.shape)
print(eliminator.ranking_)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This eliminated many more columns than the Logistic Regression version did.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;warning&lt;/b&gt; this seem to change every time you run it - the randomness changes it. Only the elimination of the first column seems to do as well as not running it at all.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8655983" class="outline-2"&gt;
&lt;h2 id="org8655983"&gt;Fit and Display&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8655983"&gt;
&lt;p&gt;
This is a convenience function so I can fit and display the scores for the models.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def fit_and_display(model, identifier):
    """Fit and display the scores

    Args:
     model: The instantiated model to fit
     identifier (str): something to output at the beginning
    """
    print(identifier)
    print("=" * len(identifier))
    model.fit(x_train, y_train)
    print("\nX-train")
    print("Score: {:.2f}".format(model.score(x_test, y_test)))
    print("\nX-Train Positive")
    model.fit(x_train_positive, y_train)
    print("Score: {:.2f}".format(model.score(x_test_positive, y_test)))
    print("\nX-Train Trees")
    model.fit(x_train_trees, y_train)
    print("Score: {:.2f}".format(model.score(x_test_trees, y_test)))
    print("\nBest Training Score: {}".format(search.best_score_))
    return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf60b7de" class="outline-2"&gt;
&lt;h2 id="orgf60b7de"&gt;Logistic Regression&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf60b7de"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgde23cbb" class="outline-3"&gt;
&lt;h3 id="orgde23cbb"&gt;L1 Penalty&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgde23cbb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = LogisticRegressionCV(penalty="l1", scoring="roc_auc", solver="liblinear")
fit_and_display(model, "Logistic Regression L1")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
I've already run the Logistic Regression using a 'l1' but I'll try it again with 'l2' to see if it improved.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = LogisticRegressionCV(scoring="roc_auc", solver="liblinear")
fit_and_display(model, "LogisticRegression")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
L1 seems to do better than L1 overall, although it doesn't do as well with the recursively data form some reason.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8d37d18" class="outline-2"&gt;
&lt;h2 id="org8d37d18"&gt;Random Forests&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8d37d18"&gt;
&lt;p&gt;
I'll try a &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"&gt;Random Forest&lt;/a&gt; classifier next.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameter_grid = dict(n_estimators=range(10, 100, 10))
search = GridSearchCV(RandomForestClassifier(), parameter_grid,
		      cv=StratifiedKFold(10), scoring="roc_auc")
fit_and_display(search, "Random Forest")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This seems to have done much better than the logistic regression did. My logistic-regression feature reduction doesn't seem to help.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class RandomForest(object):
    """trains a random forest on the x-test-trees set

    Args:
     start (int): first n-estimators value to use
     stop (int): last n-estimators value (minus step)
     step (int): amount to increment estimators
     folds (int): Cross-validation-folds to usen

    Returns:
     GridSearchCV: grid-search with the best estimator
    """

    def __init__(self, start, stop, step, folds=10):
	self.start = start
	self.stop = stop
	self.step = step
	self.folds = folds
	self._search = None
	self._parameter_grid = None
	return

    @property
    def parameter_grid(self):
	"""dict of the number of estimators to use"""
	if self._parameter_grid is None:
	    self._parameter_grid = dict(n_estimators=list(range(self.start,
								self.stop,
								self.step)))
	return self._parameter_grid

    @property
    def search(self):
	"""grid-search cv object"""
	if self._search is None:
	    self._search = GridSearchCV(RandomForestClassifier(),
					self.parameter_grid,
					cv=StratifiedKFold(self.folds),
					scoring="roc_auc")
	return self._search    

    def fit(self):
	"""fits the model to the tree-based reduced-feature data"""
	self.search.fit(x_train_trees, y_train)
	print(self.search.score(x_test_trees, y_test))
	print(self.search.best_estimator_.feature_importances_)
	print(self.search.best_params_)
	return

    def plot(self):
	"""Plots estimators vs AUC scores"""
	figure = pyplot.figure()
	axe = figure.gca()
	axe.plot(self.parameter_grid["n_estimators"],
		 self.search.cv_results_["mean_test_score"])
	axe.set_title("Estimator Count vs AUC")
	axe.set_xlabel("Number of estimators (trees)")
	axe.set_ylabel("Mean AUC Score")
	return
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;search = RandomForest(10, 100, 10)
search.fit()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Not a lot of variance in the importance of the features.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;search.plot()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Would things get better with more trees?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;search = RandomForest(150, 250, 10)
search.fit()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;search.plot()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
In this case the test-score was better, although the training scores don't look much better. I guess it's the randomness coming into play again. I'll try a long run instead.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;search = RandomForest(10, 500, 10)
search.fit()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;search.plot()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The test-score for the best estimator is actually a little worse than it was for the previous case, although it's qute a small difference.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org33ee064" class="outline-2"&gt;
&lt;h2 id="org33ee064"&gt;K Nearest Neighbors&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org33ee064"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameters = dict(n_neighbors=range(10, 20),
		  weights=["uniform", "distance"],
		  p=[1, 2],
		  leaf_size=range(10, 50, 10))

search = GridSearchCV(KNeighborsClassifier(), parameters, scoring="roc_auc")
search.fit(x_train_trees, y_train)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(search.score(x_test_trees, y_test))
print(search.best_params_)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This doesn't seem to do so well, although I'm not as experienced at using it so I might be using bad parameters.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6adf812" class="outline-2"&gt;
&lt;h2 id="org6adf812"&gt;Support Vector Classifier (SVC)&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6adf812"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameters = dict(C=numpy.arange(.1, 1, 0.1), gamma=range(1, 10, 1),
		  kernel=["linear", 'rbf', 'sigmoid'])
search = GridSearchCV(SVC(class_weight='balanced'), parameters, scoring='roc_auc')
fit_and_display(search, "SVC")
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(search.score(x_test_trees, y_test))
print(search.best_params_)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now that the data is scaled, the svc does much better, alhough still not as well as the random forest.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>model selection</category><category>networks</category><category>sklearn</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/selecting-the-e-mail-model/</guid><pubDate>Sat, 13 Apr 2019 18:57:42 GMT</pubDate></item><item><title>Future E-Mail</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org1e2ee87"&gt;Tangle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org16e4ad0"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#orgdbbb4c7"&gt;Constants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org0ed5ded"&gt;The Email-Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#orga604a9b"&gt;The Data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org7776d75"&gt;The Given Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#orgea7793c"&gt;Adding networkx features&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org5b46c3c"&gt;Add Networkx Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org316a568"&gt;Adding A Resource Allocation Index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org050df12"&gt;Adding the Jaccard Coefficient&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org6e6f821"&gt;Adamic Adar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#orgcb047c1"&gt;Preferential Attachment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org63f57bc"&gt;Community-Based Link Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org41a3ff8"&gt;Saving the Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org6dde7ba"&gt;Setup the Training and Testing Data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#orgf8f95de"&gt;Separating the Edges Without 'Future Connection' Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org41fa513"&gt;Separate the Target and Training Sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org9be0bf5"&gt;Setting Up the Testing and Training Sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org7f976ac"&gt;Scaling the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org0ce286f"&gt;Feature Selection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org3c88f4f"&gt;Fitting the Models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#orga16bdfc"&gt;Persistent Storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org9f4b4d9"&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org71794d0"&gt;Fit Grid Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#orgc4da334"&gt;Random Forests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/#org12fd641"&gt;Extra Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
This will select a model to predict whether an edge in the email-network that currently doesn't have an edge will have one in the future.
&lt;/p&gt;
&lt;div id="outline-container-org1e2ee87" class="outline-2"&gt;
&lt;h2 id="org1e2ee87"&gt;Tangle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1e2ee87"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;futures&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;files&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;networkx&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;connections&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;jaccard&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;coefficient&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;adamic&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;adar&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;preferential&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;attachment&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;connections&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;future&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;connections&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;scaled&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;unpickle&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;rfs&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;trees&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;rfs&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sfm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;trees&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;fsm&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;identifiers&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="ow"&gt;and&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sets&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="ow"&gt;and&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;all&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;logistic&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;searches&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;forests&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;extra&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;trees&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org16e4ad0" class="outline-2"&gt;
&lt;h2 id="org16e4ad0"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org16e4ad0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# python standard library
import os
import pickle

# pypi
import networkx
import pandas
import seaborn

from numba import jit

from sklearn.ensemble import (
    ExtraTreesClassifier,
    RandomForestClassifier,
    )
from sklearn.feature_selection import (
    RFECV,
    SelectFromModel,
    )
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import (
    train_test_split,
    GridSearchCV,
    StratifiedKFold,
    )
from sklearn.preprocessing import StandardScaler
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;% matplotlib inline
seaborn.set_style("whitegrid")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdbbb4c7" class="outline-2"&gt;
&lt;h2 id="orgdbbb4c7"&gt;Constants&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdbbb4c7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Futures(object):
    target = "Future Connection"
    data_file = "Future_Connections.csv"
    graph_file = "email_prediction.txt"
    networkx_data_index = 2
    folds = 10
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DataNames(object):
    resource_allocation = 'resource_allocation'
    jaccard = 'jaccard_coefficient'
    adamic = "adamic_adar"
    preferential = "preferential_attachment"
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Files(object):
    """File-names for data persistence"""
    future_training_data = 'future_training_data.csv'
    future_selection_outcomes = 'future_selection_outcomes.pkl'
    future_model_selection = "future_model_cvs.pkl"
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Training(object):
    """data-pickles"""
    x_train_lr_rfs = "x_train_lr_rfs.pkl"
    x_test_lr_rfs = "x_test_lr_rfs.pkl"
    x_train_trees_rfs = "x_train_trees_rfs.pkl"
    x_test_trees_rfs = "x_test_trees_rfs.pkl"
    x_train_lr_sfm = "x_train_lr_sfm.pkl"
    x_test_lr_sfm = "x_test_lr_sfm.pkl"
    x_train_trees_sfm = "x_train_trees_sfm.pkl"
    x_test_trees_sfm = "x_test_trees_sfm.pkl"
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0ed5ded" class="outline-2"&gt;
&lt;h2 id="org0ed5ded"&gt;The Email-Graph&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0ed5ded"&gt;
&lt;p&gt;
To get the features for the models we'll need to use the email-graph.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;email = networkx.read_gpickle(Futures.graph_file)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga604a9b" class="outline-2"&gt;
&lt;h2 id="orga604a9b"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga604a9b"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7776d75" class="outline-3"&gt;
&lt;h3 id="org7776d75"&gt;The Given Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7776d75"&gt;
&lt;p&gt;
We're given a csv file with the training and prediction data in it ('Future&lt;sub&gt;Connections.csv&lt;/sub&gt;').
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;head Future_Connections.csv
&lt;span class="nb"&gt;echo&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Org-mode converted it to a table, but it's actually a CSV. The first line of data looks like this.
&lt;/p&gt;

&lt;pre class="example"&gt;
"(6, 840)",0.0
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;future_connections_pre_loaded = os.path.isfile(Files.future_training_data)
if future_connections_pre_loaded:
    future_connections = pandas.read_csv(Files.future_training_data,
					 index_col=0)
else:
    future_connections = pandas.read_csv(Futures.data_file,
					 index_col=0,
					 converters={0: eval})
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
So, we're loading the node-pairs (edges) as the index of the data-frame and explicitly telling pandas that the Future Connection values should be converted , which I don't think is necessary, but this came with the problem statement so I'll just leave it in in case there's some side-effect I'm not aware of.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections[Futures.target].value_counts())
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This is a fairly big (and lopsided) data-set.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;seaborn.countplot(x=Futures.target, data=future_connections)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgea7793c" class="outline-2"&gt;
&lt;h2 id="orgea7793c"&gt;Adding networkx features&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgea7793c"&gt;
&lt;p&gt;
To create features to train the model and make predictions, I'm going to use the networkx &lt;a href="https://networkx.github.io/documentation/networkx-1.10/reference/algorithms.link_prediction.html"&gt;link prediction&lt;/a&gt; algorithms.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5b46c3c" class="outline-3"&gt;
&lt;h3 id="org5b46c3c"&gt;Add Networkx Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5b46c3c"&gt;
&lt;p&gt;
This is a function to get networkx data and add it to the data-frame. It won't work for the community-based algorithms.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def add_networkx_data(adder, name, graph=email, frame=future_connections):
    """Adds networkx data to the frame

    The networkx link-prediction functions return generators of triples:
     (first-node, second-node, value)

    This will use the index of the frame that's passed in as the source of 
    node-pairs for the networkx function (called `ebunch` in the networkx
    documentation) and the add only the value we want back to the frame

    Args:
     adder: networkx function to call to get the new data
     name: column-name to add to the frame
     graph: networkx graph to pass to the function
     frame (pandas.DataFrame): frame with node-pairs as index to add data to
    """
    frame[name] = [output[Futures.networkx_data_index]
		   for output in adder(graph, frame.index)]
    return frame
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org316a568" class="outline-3"&gt;
&lt;h3 id="org316a568"&gt;Adding A Resource Allocation Index&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org316a568"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if not future_connections_pre_loaded:
    add_networkx_data(networkx.resource_allocation_index,
		      DataNames.resource_allocation)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org050df12" class="outline-3"&gt;
&lt;h3 id="org050df12"&gt;Adding the Jaccard Coefficient&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org050df12"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if not future_connections_pre_loaded:
    add_networkx_data(networkx.jaccard_coefficient, DataNames.jaccard)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6e6f821" class="outline-3"&gt;
&lt;h3 id="org6e6f821"&gt;Adamic Adar&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6e6f821"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if not future_connections_pre_loaded:
    add_networkx_data(networkx.adamic_adar_index, DataNames.adamic)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcb047c1" class="outline-3"&gt;
&lt;h3 id="orgcb047c1"&gt;Preferential Attachment&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcb047c1"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if not future_connections_pre_loaded:
    add_networkx_data(networkx.preferential_attachment, DataNames.preferential)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(future_connections.head(1))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org63f57bc" class="outline-3"&gt;
&lt;h3 id="org63f57bc"&gt;Community-Based Link Prediction&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org63f57bc"&gt;
&lt;p&gt;
This requires identifying 'communities' first, so I'll defer it for now.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;#add_networkx_data(networkx.cn_soundarajan_hopcroft, DataNames.common_neighbors)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
These three all require communities for them to work (so I'm skipping them):
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;cn&lt;sub&gt;soundarajan&lt;/sub&gt;&lt;sub&gt;hopcroft&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;ra&lt;sub&gt;index&lt;/sub&gt;&lt;sub&gt;soundarajan&lt;/sub&gt;&lt;sub&gt;hopcroft&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;within&lt;sub&gt;inter&lt;/sub&gt;&lt;sub&gt;cluster&lt;/sub&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org41a3ff8" class="outline-3"&gt;
&lt;h3 id="org41a3ff8"&gt;Saving the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org41a3ff8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;future_connections.to_csv(Files.future_training_data)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6dde7ba" class="outline-2"&gt;
&lt;h2 id="org6dde7ba"&gt;Setup the Training and Testing Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6dde7ba"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf8f95de" class="outline-3"&gt;
&lt;h3 id="orgf8f95de"&gt;Separating the Edges Without 'Future Connection' Values&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf8f95de"&gt;
&lt;p&gt;
We are going to train on the values in the data with predictions and then make predictions for those that don't. For model selection we don't need the set missing predictions, but I'll separate it out anyway to be complete.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;prediction_set = future_connections[future_connections[Futures.target].isnull()]
training_set = future_connections[future_connections[Futures.target].notnull()]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(prediction_set.shape)
print(training_set.shape)
assert len(prediction_set) + len(training_set) == len(future_connections)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org41fa513" class="outline-3"&gt;
&lt;h3 id="org41fa513"&gt;Separate the Target and Training Sets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org41fa513"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;non_target = [column for column in future_connections.columns
	      if column != Futures.target]
training = training_set[non_target]
testing = training_set[Futures.target]
predictions = prediction_set[non_target]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;assert all(training.columns == predictions.columns)
assert len(training) == len(testing)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9be0bf5" class="outline-3"&gt;
&lt;h3 id="org9be0bf5"&gt;Setting Up the Testing and Training Sets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9be0bf5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x_train, x_test, y_train, y_test = train_test_split(training, testing, stratify=testing)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train.shape)
print(x_test.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;seaborn.countplot(y_train)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;seaborn.countplot(y_test)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7f976ac" class="outline-3"&gt;
&lt;h3 id="org7f976ac"&gt;Scaling the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7f976ac"&gt;
&lt;p&gt;
To enable the use of linear models I'm going to scale the data so the mean is 0 and the variance is 1.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

x_train = pandas.DataFrame(x_train, columns=training.columns)
x_test = pandas.DataFrame(x_test, columns=training.columns)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train.describe())
print(x_test.describe())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0ce286f" class="outline-3"&gt;
&lt;h3 id="org0ce286f"&gt;Feature Selection&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0ce286f"&gt;
&lt;p&gt;
To reduce the dimensionality I'm going to use recursive feature selection and model-based selection.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def pickle_it(thing, name):
    """saves the thing as a pickle"""
    with open(name, "wb") as writer:
	pickle.dump(thing, writer)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def unpickle_it(name):
    """loads the object from the file-name

    Args:
     name (str): name of binary pickle file

    Returns:
     obj: unpickled object
    """
    with open(name, 'rb') as reader:
	thing = pickle.load(reader)
    return thing
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd64a1ca" class="outline-4"&gt;
&lt;h4 id="orgd64a1ca"&gt;RFECV with Logistic Regression&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd64a1ca"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if os.path.isfile(Training.x_train_lr_rfs):
    x_train_lr_rfs = unpickle_it(Training.x_train_lr_rfs)
    x_test_lr_rfs = unpickle_it(Training.x_test_lr_rfs)
else:
    estimator = LogisticRegressionCV(n_jobs=-1)
    selector = RFECV(estimator, scoring='roc_auc',
		     n_jobs=-1,
		     cv=StratifiedKFold(Futures.folds))
    x_train_lr_rfs = selector.fit_transform(x_train, y_train)
    x_test_lr_rfs = selector.transform(x_test)
    pickle_it(x_train_lr_rfs, Training.x_train_lr_rfs)
    pickle_it(x_test_lr_rfs, Training.x_test_lr_rfs)
    print(selector.ranking_)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
It looks like it only discarded preferential attachment.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org39ea1b4" class="outline-4"&gt;
&lt;h4 id="org39ea1b4"&gt;RFECV with Extra Trees&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org39ea1b4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if os.path.isfile(Training.x_train_trees_rfs):
    x_train_trees_rfs = unpickle_it(Training.x_train_trees_rfs)
    x_test_trees_rfs = unpickle_it(Training.x_test_trees_rfs)
else:
    estimator = ExtraTreesClassifier()
    selector = RFECV(estimator, scoring='roc_auc', n_jobs=-1, cv=StratifiedKFold(Futures.folds))
    x_train_trees_rfs = selector.fit_transform(x_train, y_train)
    x_test_trees_rfs = selector.transform(x_test)
    pickle_it(x_train_trees_rfs, Training.x_train_trees_rfs)
    pickle_it(x_test_trees_rfs, Training.x_test_trees_rfs)
    print(selector.ranking_)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Strangely, the Extra Trees Classifier didn't remove any columns…
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3330d0a" class="outline-4"&gt;
&lt;h4 id="org3330d0a"&gt;Select Model Logistic Regression&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3330d0a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if os.path.isfile(Training.x_train_lr_sfm):
    x_train_lr_sfm = unpickle_it(Training.x_train_lr_sfm)
    x_test_lr_sfm = unpickle_it(Training.x_test_lr_sfm)
else:
    estimator = LogisticRegressionCV(
	n_jobs=-1, scoring='roc_auc',
	cv=StratifiedKFold(Futures.folds)).fit(x_train,
					       y_train)
    selector = SelectFromModel(estimator, prefit=True)
    x_train_lr_sfm = selector.transform(x_train)
    x_test_lr_sfm = selector.transform(x_test)
    pickle_it(x_train_lr_sfm, Training.x_train_lr_sfm)
    pickle_it(x_test_lr_sfm, Training.x_test_lr_sfm)
    print(estimator.coef_)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train_lr_sfm.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This was more aggressive, cutting out half the features. It looks like it kept &lt;b&gt;Jaccard Coefficient&lt;/b&gt; and &lt;b&gt;Adamic Adar&lt;/b&gt; and got rid of &lt;b&gt;Resource Allocation&lt;/b&gt; and &lt;b&gt;Preferential Attachment&lt;/b&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf4d974b" class="outline-4"&gt;
&lt;h4 id="orgf4d974b"&gt;Select Model Extra Trees&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf4d974b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if os.path.isfile(Training.x_train_trees_sfm):
    x_train_trees_sfm = unpickle_it(Training.x_train_trees_sfm)
    x_test_trees_sfm = unpickle_it(Training.x_test_trees_sfm)
else:
    estimator = ExtraTreesClassifier()
    estimator.fit(x_train, y_train)
    selector = SelectFromModel(estimator, prefit=True)
    x_train_trees_sfm = selector.transform(x_train)
    x_test_trees_sfm = selector.transform(x_test)
    pickle_it(x_train_trees_sfm, Training.x_train_trees_sfm)
    pickle_it(x_test_trees_sfm, Training.x_test_trees_sfm)
    print(estimator.feature_importances_)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(x_train_trees_sfm.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This is sometimes more aggressive, keeping only the &lt;b&gt;Adamic Adar&lt;/b&gt; feature… But maybe that's all you need, we'll see. Then again, other times it isn't as aggressive, only trimming two columns, and this tiem it only trimmed one…
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3c88f4f" class="outline-2"&gt;
&lt;h2 id="org3c88f4f"&gt;Fitting the Models&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3c88f4f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga16bdfc" class="outline-3"&gt;
&lt;h3 id="orga16bdfc"&gt;Persistent Storage&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga16bdfc"&gt;
&lt;p&gt;
The outcomes will be stored in a dictionary called &lt;code&gt;scores&lt;/code&gt; with descriptions of the best model and feature-selection mapped to their testing-score.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if os.path.isfile(Files.future_model_selection):
    with open(Files.future_model_selection, 'rb') as pkl:
	scores = pickle.load(pkl)
else:
    scores = {}
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def fit_and_print(estimator, x_train, x_test):
    """fits the estimator to the data

    Args:
     estimator: model to fit
     x_train: scaled data to fit model to
     x_test: data to test the model with

    Returns:
     tuple: model fit to the data, test score
    """
    model = estimator.fit(x_train, y_train)
    test_score = model.score(x_test, y_test)
    print("Mean Cross-Validation Score: {:.2f}".format(model.scores_[1].mean()))
    print("Testing Score: {:.2f}".format(test_score))
    return model, test_score
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data_sets = {("extra trees", 'select from model') : (x_train_trees_sfm, x_test_trees_sfm),
	     ("extra trees", 'recursive feature selection') : (x_train_trees_rfs, x_test_trees_rfs),
	     ('logistic regression', "recursive feature selection") : (x_train_lr_rfs, x_test_lr_rfs),
	     ('logistic regression', "select from model") : (x_train_lr_sfm, x_test_lr_sfm)}
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def key_by_value(source, search_value):
    """Find the key in a dict that matches a value

    Args:
     source (dict): dictionary with value to search for
     search_value: value to search for

    Returns:
     object: key in source that matched value
    """
    for key, value in source.items():
	if value == search_value:
	    return key
    return
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def fit_and_print_all(model, model_name):
    """Fits the model against all data instances

    Args:
     model: model to fit to the data sets
     model_name: identifier for the outcomes
    """
    for data_set, x in data_sets.items():
	selector, method = data_set
	train, test = x
	key = ','.join([model_name, selector, method])
	print("Training Shape: {}".format(train.shape))
	if key not in scores:
	    print(key)
	    fitted, score = fit_and_print(model, train, test)
	    scores[key] = score
	else:
	    score = scores[key]
	    print("{}: {:.3f}".format(key, score))
	print()

    best_score = max(scores.values())
    best_key = key_by_value(scores, best_score)
    print("Best Model So Far: {}, Score={:.2f}".format(
	best_key,
	best_score))
    with open(Files.future_model_selection, 'wb') as writer:
	pickle.dump(scores, writer)
    return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9f4b4d9" class="outline-3"&gt;
&lt;h3 id="org9f4b4d9"&gt;Logistic Regression&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9f4b4d9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;logistic_model = LogisticRegressionCV(n_jobs=-1, scoring="roc_auc",
				      solver='liblinear',
				      cv=StratifiedKFold(Futures.folds))
fit_and_print_all(logistic_model, "Logistic Regression")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org71794d0" class="outline-3"&gt;
&lt;h3 id="org71794d0"&gt;Fit Grid Search&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org71794d0"&gt;
&lt;p&gt;
Since the Logistic Regression had its own cross-validation I didn't use a grid search, but for the forests I'll use one to figure out the best number of estimators. I'll have to look into what the other parameters do to figure out whether they're going to be useful.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def fit_grid_search(estimator, parameters, x_train, x_test):
    """Fits the estimator using grid search

    Args:
     estimator: Model to fit
     parameters (dict): hyper-parameters for the grid search
     x_train (array): the training data input
     x_test (array): data to evaluate the best model with

    Returns: 
     tuple: Best Model, best model score
    """
    search = GridSearchCV(estimator, parameters, n_jobs=-1, scoring='roc_auc',
			  cv=StratifiedKFold(Futures.folds))
    search.fit(x_train, y_train)
    best_model = search.best_estimator_
    test_score = best_model.score(x_test, y_test)
    print("Mean of Mean Cross-Validation Scores: {:.2f}".format(
	search.cv_results_["mean_train_score"].mean()))
    print("Mean of Cross-Validation Score STDs: {:.2f}".format(
	search.cv_results_["std_train_score"].mean()))
    print("Testing Score: {:.2f}".format(test_score))
    return best_model, test_score
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def fit_grid_searches(estimator, parameters, name, data_sets=data_sets):
    """Fits the estimator against all the data-sets

    Args:
     estimator: instance of model to test
     parameters: dict of grid-search parameters
     name: identifier for the model
    """
    for data_set, x in data_sets.items():
	selector, method = data_set
	train, test = x
	key = ",".join([name, selector, method])
	if key not in scores:
	    print(key)
	    fitted, score = fit_grid_search(estimator, parameters, train, test)
	    scores[key] = score
	else:
	    score = scores[key]
	    print("{}: {:.2f}".format(key, score))
	print()
    best = max(scores.values())
    best_key = key_by_value(scores, best)
    print("Best Model So Far: {}, Score={:.2f}".format(best_key, best))
    with open(Files.future_model_selection, 'wb') as writer:
	pickle.dump(scores, writer)
    return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc4da334" class="outline-3"&gt;
&lt;h3 id="orgc4da334"&gt;Random Forests&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc4da334"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameters = dict(n_estimators = list(range(10, 200, 10)))
forest = RandomForestClassifier()
fit_grid_searches(forest, parameters, "Random Forest")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org12fd641" class="outline-3"&gt;
&lt;h3 id="org12fd641"&gt;Extra Trees&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org12fd641"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scores = {k:v for k,v in scores.items() if not k.startswith('Extra Trees,extra trees')}
parameters = dict(n_estimators = list(range(10, 200, 10)))
trees = ExtraTreesClassifier()
fit_grid_searches(trees, parameters, "Extra Trees")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>networks</category><category>prediction</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/future-e-mail/</guid><pubDate>Sat, 13 Apr 2019 18:52:40 GMT</pubDate></item><item><title>Friends and Politics</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#orga245960"&gt;Part 1 - Friendships&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#org9c9f86a"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#orgd82df7f"&gt;Friendships data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#orgf5dfb4a"&gt;Degree, Closeness, and Normalized Betweenness Centrality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#org4dcf6ed"&gt;Most Connected Friend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#orgd2a7953"&gt;Fewest Hops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#orge0b792f"&gt;Most Important Connection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#orgea68c32"&gt;Part 2 - Political Blogs&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#org5e610fd"&gt;Scaled Page Rank of &lt;i&gt;realclearpolitics.com&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#orgace4645"&gt;Top Five Blogs by Page Rank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#org8ef4c97"&gt;HITS Score for Real Clear Politics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#orgd37ed95"&gt;Top 5 Blogs by Hub Score&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/#org120bfe2"&gt;Top Five Blogs By Authority&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;a href="https://en.wikipedia.org/wiki/Centrality"&gt;Node Centrality&lt;/a&gt; is a measure of the importance of a node to a network. This will explore measures of centrality using two networks, a friendship network, and a blog network.
&lt;/p&gt;

&lt;div id="outline-container-orga245960" class="outline-2"&gt;
&lt;h2 id="orga245960"&gt;Part 1 - Friendships&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga245960"&gt;
&lt;p&gt;
This will look at a network of friendships at a university department. Each node corresponds to a person (identified by an integer node label), and an edge indicates friendship. 
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9c9f86a" class="outline-3"&gt;
&lt;h3 id="org9c9f86a"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9c9f86a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import networkx
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd82df7f" class="outline-3"&gt;
&lt;h3 id="orgd82df7f"&gt;Friendships data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd82df7f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;friendships = networkx.read_gml('friendships.gml')
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(len(friendships))
print(networkx.is_connected(friendships))
print(networkx.is_directed(friendships))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
There are 1,133 people in the friendship network, which is a connected, undirected graph.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf5dfb4a" class="outline-3"&gt;
&lt;h3 id="orgf5dfb4a"&gt;Degree, Closeness, and Normalized Betweenness Centrality&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf5dfb4a"&gt;
&lt;p&gt;
Find the degree centrality, closeness centrality, and normalized betweenness centrality (excluding endpoints) of node 100.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Centrality#Degree_centrality"&gt;Degree Centrality&lt;/a&gt; scores the nodes based on the number of links they have to other nose. The assumption is that a node with more connections should be more important.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Closeness_centrality"&gt;Closeness Centrality&lt;/a&gt; uses the lengths of shortest paths to decide importance. The less distance there is between a node and the other nodes the more important it is.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Betweenness_centrality"&gt;Betweenness Centrality&lt;/a&gt; counts the number of shortest paths between pairs of nodes that pass through a node.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DEGREE_CENTRALITY = networkx.degree_centrality(friendships)
CLOSENESS_CENTRALITY = networkx.closeness_centrality(friendships)
BETWEENNESS_CENTRALITY = networkx.betweenness_centrality(friendships)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def node_centrality(node=100):
    """gets measures of centrality for node

    Args:
     node (int): the number (key) for the node

    Returns:
     tuple: 
      - float: degree centrality
      - float: closeness centrality
      - float: normalized betweeness centrality
    """
    return (DEGREE_CENTRALITY[node],
	    CLOSENESS_CENTRALITY[node], BETWEENNESS_CENTRALITY[node])
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("Node 101:")
degree, closeness, betweenness = node_centrality()
print("Degree Centrality: {0:.4f}".format(degree))
print("Closeness Centrality: {0:.2f}".format(closeness))
print("Normalized Betweenness Centrality: {0:.6f}".format(betweenness))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def largest_node(centrality):
    """gets the node with the best (largest) score

    Args:
     centrality (dict): one of the centrality score dicts

    Returns:
     int: name of the node with the best score
    """
    return list(
	reversed(sorted((value, node)
			for (node, value) in centrality.items())))[0][1]
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4dcf6ed" class="outline-3"&gt;
&lt;h3 id="org4dcf6ed"&gt;Most Connected Friend&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4dcf6ed"&gt;
&lt;p&gt;
We want to contact one person in our friendship network and have him or her contact all his or her immediate friends. To have the greatest impact, this person should have the most links in the network. Which node is this?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def most_connected_friend():
    """returns the node with the best degree centrality"""
    return largest_node(DEGREE_CENTRALITY)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;MOST_CONNECTED = most_connected_friend()
print("Most Connected Friend: {}".format(MOST_CONNECTED))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;connected = networkx.Graph()
friends = friendships[MOST_CONNECTED]
for friend in friends:
    connected.add_edge(MOST_CONNECTED, friend)
positions = networkx.spring_layout(connected)
networkx.draw(connected, positions, with_labels=False, node_color='b', node_size=50)
networkx.draw(connected, positions, nodelist=[MOST_CONNECTED], node_color="r")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Node 105 does appear to be well connected.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd2a7953" class="outline-3"&gt;
&lt;h3 id="orgd2a7953"&gt;Fewest Hops&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd2a7953"&gt;
&lt;p&gt;
We want to reach everyone in the network by having one person passing messages to his friends who can then pass it on and so forth (a six-degrees of separation type scenario) but we want the fewest number of transfers. &lt;i&gt;Which friend is closest to all the people in the friendship network?&lt;/i&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def closest_friend():
    """the node with the best closeness centrality

    Returns:
     int: Identifier for the node closest to all the other nodes
    """
    return largest_node(CLOSENESS_CENTRALITY)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;CLOSEST_FRIEND = closest_friend()
print("Closest Friend: {}".format(CLOSEST_FRIEND))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;positions = networkx.spring_layout(friendships)
networkx.draw(friendships, positions, node_size=1, alpha=0.25, node_color='b')
networkx.draw_networkx_nodes(friendships, positions, nodelist=[CLOSEST_FRIEND],
			     node_color='r', node_size=50)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;
Interesting to look at, if not the most informative.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge0b792f" class="outline-3"&gt;
&lt;h3 id="orge0b792f"&gt;Most Important Connection&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge0b792f"&gt;
&lt;p&gt;
Although the graph is connected, if you took out one persion from the network, which one would cause the most disruption (which person is in the path of the most shortest paths)?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def betweenness_centrality():
    """the node with the highest betweenness centrality

    Returns:
     int: ID of the person who sits on the most shortest paths
    """
    return largest_node(BETWEENNESS_CENTRALITY)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;MOST_BETWEEN = betweenness_centrality()
print("Most Between Friend: {}".format(MOST_BETWEEN))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgea68c32" class="outline-2"&gt;
&lt;h2 id="orgea68c32"&gt;Part 2 - Political Blogs&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgea68c32"&gt;
&lt;p&gt;
Now we're going to use &lt;a href="https://en.wikipedia.org/wiki/PageRank"&gt;PageRank&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/HITS_algorithm"&gt;Hyperlink-Induced Topic Search (HITS)&lt;/a&gt;  to look at a directed network of political blogs, where nodes correspond to a blog and edges correspond to links between blogs.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;blogs = networkx.read_gml('blogs.gml')
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(len(blogs))
print(networkx.is_directed(blogs))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;networkx.draw(blogs, alpha=0.5, node_size=1, node_color='r')
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5e610fd" class="outline-3"&gt;
&lt;h3 id="org5e610fd"&gt;Scaled Page Rank of &lt;i&gt;realclearpolitics.com&lt;/i&gt;&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5e610fd"&gt;
&lt;p&gt;
&lt;i&gt;PageRank&lt;/i&gt; scores web-pages by the number of important nodes that link directly to them. It is possible for the algorithm to get stuck if there are no edges leading out from a directed subgraph, producing erroneous page-ranks so the &lt;i&gt;Scaled Page Rank&lt;/i&gt; uses a random-restart do decide when to occasionally jump to a new node, an idea similar to the way Stochastic Gradient Descent avoids being stuck in local minima. The &lt;a href="https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html"&gt;Networkx pagerank&lt;/a&gt; uses a default of 0.85, which I will use, so it will do a random-restart about 15% of the time.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;PAGE_RANK = networkx.pagerank(blogs)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def real_clear_politics_page_rank():
    """Page Rank of realclearpolitics.com

    Returns:
     float: The PageRank for the realclearpolitics blog.
    """
    return PAGE_RANK['realclearpolitics.com']
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("Real Clear Politics Page Rank: {0:.4f}".format(real_clear_politics_page_rank()))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgace4645" class="outline-3"&gt;
&lt;h3 id="orgace4645"&gt;Top Five Blogs by Page Rank&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgace4645"&gt;
&lt;p&gt;
This time the PageRank scores will be used to find what it thinks are the most important blogs.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def top_five(ranks, count=5):
    """gets the top-five blogs by rank

    Args:
     count (int): number to return

    Returns:
     list [str]: names of the top blogs - most to least important
    """
    top = list(reversed(sorted((rank, node)
			       for node, rank in ranks.items())))[:count]
    return [node for rank, node in top]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def top_five_page_rank():
    """Top 5 nodes by page rank

    Returns:
     list [str]: top-five blogs by page-rank
    """
    return top_five(PAGE_RANK)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("Top Five Blogs by PageRank")

for blog in top_five_page_rank():
    print("  - {}".format(blog))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8ef4c97" class="outline-3"&gt;
&lt;h3 id="org8ef4c97"&gt;HITS Score for Real Clear Politics&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8ef4c97"&gt;
&lt;p&gt;
This uses the HITS algorithm to find the authority and hub scores for &lt;i&gt;realclearpolitics.com&lt;/i&gt;. This algorithm tries to identify &lt;code&gt;hubs&lt;/code&gt;, collections of links that directed users to important pages, and &lt;code&gt;authoratative&lt;/code&gt; pages, pages that are deemed important because of their relevant content (as identified by the fact that they are linked to by &lt;code&gt;hubs&lt;/code&gt;).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;HUBS, AUTHORITIES = networkx.hits(blogs)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def real_clear_politics_hits():
    """HITS score for realclearpolitics.com

    Returns:
     tuple (float, float): hub score, authority score
    """
    return HUBS['realclearpolitics.com'], AUTHORITIES['realclearpolitics.com']
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hub, authority = real_clear_politics_hits()
print("Real Clear Politics")
print("Hub: {0:.5f}\nAuthority: {0:.5f}".format(hub, authority))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd37ed95" class="outline-3"&gt;
&lt;h3 id="orgd37ed95"&gt;Top 5 Blogs by Hub Score&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd37ed95"&gt;
&lt;p&gt;
This will find the top five blogs based on their hub scores (meaning they are the ones who link to the most authoratative sites).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def top_five_hubs():
    """Top five blogs by hub scores

    Returns:
     list (str): Names of top-five hub blogs
    """
    return top_five(HUBS)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;top_five_hub_blogs = top_five_hubs()
print('Top Five Hub Blogs')
for blog in top_five_hub_blogs:
    print(" - {}".format(blog))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org120bfe2" class="outline-3"&gt;
&lt;h3 id="org120bfe2"&gt;Top Five Blogs By Authority&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org120bfe2"&gt;
&lt;p&gt;
This will find the top five political blogs based on how many of the hub-blogs link to them.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def top_five_authorities():
    """the top 5 blogs by authorities score

    Returns:
     list (str): names of the most authoratative blogs
    """
    return top_five(AUTHORITIES)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("Top Five Authoratative Blogs")
authoratative_blogs = top_five_authorities()
for blog in authoratative_blogs:
    print(" - {}".format(blog))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>centrality</category><category>networks</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/friends-and-politics/</guid><pubDate>Sat, 13 Apr 2019 18:40:48 GMT</pubDate></item><item><title>Company E-Mail</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/#orgea9a174"&gt;Tangle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/#org65be22a"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/#org4d601f7"&gt;1 - Load the Directed Multigraph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/#org18ba0de"&gt;Number of employees and emails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/#orgaf3d6a0"&gt;Information Routes&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/#orgcdffb7c"&gt;Part 1. Assume that information in this company can only be exchanged through email.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/#org0a6f264"&gt;Part 2. Now assume that a communication channel established by an email allows information to be exchanged both ways.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/#org6c723d4"&gt;Largest Weakly Connected Component&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
This will go through the process of importing and analyzing an internal email communication network between employees of a mid-sized manufacturing company. 
Each node represents an employee and each directed edge between two nodes represents an individual email. The left node represents the sender and the right node represents the recipient.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;+------+      +--------+
|Sender| --&amp;gt;  |Receiver|
+------+      +--------+
&lt;/pre&gt;&lt;/div&gt;

&lt;div id="outline-container-orgea9a174" class="outline-2"&gt;
&lt;h2 id="orgea9a174"&gt;Tangle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgea9a174"&gt;
&lt;p&gt;
This is for troubleshooting.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;imports&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;

&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;answer&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org65be22a" class="outline-2"&gt;
&lt;h2 id="org65be22a"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org65be22a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import networkx
import pandas
import seaborn
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;% matplotlib inline
seaborn.set_style("whitegrid")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This line must be commented out when submitting to the autograder
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;email_network = pandas.read_table('email_network.txt', dtype={"#Sender": str, "Recipient": str})
print(email_network.head())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4d601f7" class="outline-2"&gt;
&lt;h2 id="org4d601f7"&gt;1 - Load the Directed Multigraph&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4d601f7"&gt;
&lt;p&gt;
Using networkx, load up the directed multigraph from `email&lt;sub&gt;network.txt&lt;/sub&gt;`. Make sure the node names are strings.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;This function should return a directed multigraph networkx graph.&lt;/b&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def answer_one():
    """Loads the email-network graph

    Returns:
     networkx.MultiDiGraph: the graph of the email network
    """
    # there's a bug in networkx loading MultiDiGraphs from pandas data-frames
    # so this is a work-around
    graph = networkx.MultiDiGraph()
    tuples = [(sender, recipient, {"time": time})
	      for (sender, recipient, time) in email_network.values]
    graph.add_edges_from(tuples)
    return graph
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;one = answer_one()
networkx.draw_networkx(one)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org18ba0de" class="outline-2"&gt;
&lt;h2 id="org18ba0de"&gt;Number of employees and emails&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org18ba0de"&gt;
&lt;p&gt;
How many employees and emails are represented in the graph from Question 1?
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;This function should return a tuple (#employees, #emails).&lt;/b&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def answer_two():
    """Counts the number of employees and emails

    Returns:
     tuple: count of employees, count of emails
    """
    one = answer_one()
    return (one.order(), one.size())
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(answer_two())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgaf3d6a0" class="outline-2"&gt;
&lt;h2 id="orgaf3d6a0"&gt;Information Routes&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgaf3d6a0"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcdffb7c" class="outline-3"&gt;
&lt;h3 id="orgcdffb7c"&gt;Part 1. Assume that information in this company can only be exchanged through email.&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcdffb7c"&gt;
&lt;p&gt;
When an employee sends an email to another employee, a communication channel has been created, allowing the sender to provide information to the receiver, but not vice versa. 
&lt;/p&gt;

&lt;p&gt;
Based on the emails sent in the data, is it possible for information to go from every employee to every other employee?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0a6f264" class="outline-3"&gt;
&lt;h3 id="org0a6f264"&gt;Part 2. Now assume that a communication channel established by an email allows information to be exchanged both ways.&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0a6f264"&gt;
&lt;p&gt;
Based on the emails sent in the data, is it possible for information to go from every employee to every other employee?
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;This function should return a tuple of bools (part1, part2).&lt;/b&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def answer_three():
    """decides connectivity based on emails

    First: Assume communication is not necessarily allowed both ways - 
    based on data, can every employee contact each other?

    Second: Assume any contact means there's two way communication. 
    Can every employee be contacted?

    Returns:
     tuple: (every employee contacted every other employee, every employee contacted once)
    """
    emails = answer_one()
    nodes = emails.nodes()
    other_nodes = len(nodes) - 1
    fully_connected = all((len(emails.neighbors(node)) == other_nodes for node in nodes))
    undirected = emails.to_undirected()
    all_connected = True
    for left_node in nodes:
	for right_node in nodes:
	    if left_node != right_node and not undirected.has_edge(left_node, right_node):
		all_connected = False
		break
	if not all_connected:
	    break
    return fully_connected, all_connected
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(answer_three())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6c723d4" class="outline-2"&gt;
&lt;h2 id="org6c723d4"&gt;Largest Weakly Connected Component&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6c723d4"&gt;
&lt;p&gt;
How many nodes are in the largest (in terms of nodes) weakly connected component?
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;This function should return an int.&lt;/b&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def answer_four():
    """Count of nodes in the largest weakly connected component"""
    one = answer_one()
    return len(max(networkx.weakly_connected_component_subgraphs(one), key=len).nodes())
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
According to &lt;a href="https://en.wikipedia.org/wiki/Connectivity_(graph_theory)#Definitions_of_components.2C_cuts_and_connectivity"&gt;Wikipedia&lt;/a&gt;, a directed graph is weakly connected if replacing every directed edge with an undirected one creates a connected graph, so if the undirected graph in the next section is a connected graph, then the entire email graph is weakly connected.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(answer_four())
undirected = one.to_undirected()
print(networkx.is_connected(undirected))
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;
def answer&lt;sub&gt;five&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;


&lt;p&gt;
def answer&lt;sub&gt;six&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;


&lt;p&gt;
def answer&lt;sub&gt;seven&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;


&lt;p&gt;
def answer&lt;sub&gt;eight&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;


&lt;p&gt;
def answer&lt;sub&gt;nine&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;


&lt;p&gt;
def answer&lt;sub&gt;ten&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;


&lt;p&gt;
def answer&lt;sub&gt;eleven&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;


&lt;p&gt;
def answer&lt;sub&gt;twelve&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;


&lt;p&gt;
def answer&lt;sub&gt;thirteen&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;


&lt;p&gt;
def answer&lt;sub&gt;fourteen&lt;/sub&gt;():
&lt;/p&gt;

&lt;p&gt;
return # Your Answer Here
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>networks</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/networks/company-e-mail/</guid><pubDate>Sat, 13 Apr 2019 18:34:37 GMT</pubDate></item></channel></rss>