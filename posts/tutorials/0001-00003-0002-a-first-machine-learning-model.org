#+BEGIN_COMMENT
.. title: A First Machine Learning Model
.. slug: a-first-machine-learning-model
.. date: 2020-02-17 19:52:52 UTC-08:00
.. tags: kaggle,tutorial,machine learning
.. category: Tutorial
.. link: 
.. description: A first machine learning model.
.. type: text
.. status: 
.. updated: 

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 
#+PROPERTY: header-args :session /run/user/1000/jupyter/kernel-e3d39daa-b285-4dc9-a05e-b527dd6bf716.json
* Beginning
  This part 2 of a re-do of the kaggle [[https://www.kaggle.com/learn/intro-to-machine-learning][Intro to Machine Learning]] tutorial - Your First Machine Learning Model.
** Imports
*** Python
#+begin_src python :results none
from argparse import Namespace
from datetime import datetime
from functools import partial
from pathlib import Path
#+end_src
*** PyPi
#+begin_src python :results none
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge
from sklearn.tree import DecisionTreeRegressor

import hvplot.pandas
import pandas
#+end_src
*** Others
#+begin_src python :results none
from graeae import EmbedHoloviews, EnvironmentLoader, Timer
#+end_src
** Set Up
*** Plottting
#+begin_src python :results none
SLUG = "a-first-machine-learning-model"
OUTPUT_PATH = Path("../../files/posts/tutorials/")/SLUG
Embed = partial(EmbedHoloviews, folder_path=OUTPUT_PATH)
Plot = Namespace(
    height=800,
    width=1000,
)
#+end_src
*** The Timer
#+begin_src python :results none
TIMER = Timer()
#+end_src
*** Environment
#+begin_src python :results none
ENVIRONMENT = EnvironmentLoader()
#+end_src
*** The Data
#+begin_src python :results none
data = pandas.read_csv(
    Path(ENVIRONMENT["HOUSE-PRICES-IOWA"]).expanduser()/"train.csv")
#+end_src
* Middle
** Step 1: Specify Prediction Target
Select the target variable, which corresponds to the sales price. Save this to a new variable called `y`. You'll need to print a list of the columns to find the name of the column you need.

#+begin_src python :results output raw :exports both
for column in sorted(data.columns):
    print(f"- {column} ({data[column].dtype})")
#+end_src

#+RESULTS:
#+begin_example
- 1stFlrSF (int64)
- 2ndFlrSF (int64)
- 3SsnPorch (int64)
- Alley (object)
- BedroomAbvGr (int64)
- BldgType (object)
- BsmtCond (object)
- BsmtExposure (object)
- BsmtFinSF1 (int64)
- BsmtFinSF2 (int64)
- BsmtFinType1 (object)
- BsmtFinType2 (object)
- BsmtFullBath (int64)
- BsmtHalfBath (int64)
- BsmtQual (object)
- BsmtUnfSF (int64)
- CentralAir (object)
- Condition1 (object)
- Condition2 (object)
- Electrical (object)
- EnclosedPorch (int64)
- ExterCond (object)
- ExterQual (object)
- Exterior1st (object)
- Exterior2nd (object)
- Fence (object)
- FireplaceQu (object)
- Fireplaces (int64)
- Foundation (object)
- FullBath (int64)
- Functional (object)
- GarageArea (int64)
- GarageCars (int64)
- GarageCond (object)
- GarageFinish (object)
- GarageQual (object)
- GarageType (object)
- GarageYrBlt (float64)
- GrLivArea (int64)
- HalfBath (int64)
- Heating (object)
- HeatingQC (object)
- HouseStyle (object)
- Id (int64)
- KitchenAbvGr (int64)
- KitchenQual (object)
- LandContour (object)
- LandSlope (object)
- LotArea (int64)
- LotConfig (object)
- LotFrontage (float64)
- LotShape (object)
- LowQualFinSF (int64)
- MSSubClass (int64)
- MSZoning (object)
- MasVnrArea (float64)
- MasVnrType (object)
- MiscFeature (object)
- MiscVal (int64)
- MoSold (int64)
- Neighborhood (object)
- OpenPorchSF (int64)
- OverallCond (int64)
- OverallQual (int64)
- PavedDrive (object)
- PoolArea (int64)
- PoolQC (object)
- RoofMatl (object)
- RoofStyle (object)
- SaleCondition (object)
- SalePrice (int64)
- SaleType (object)
- ScreenPorch (int64)
- Street (object)
- TotRmsAbvGrd (int64)
- TotalBsmtSF (int64)
- Utilities (object)
- WoodDeckSF (int64)
- YearBuilt (int64)
- YearRemodAdd (int64)
- YrSold (int64)
#+end_example

That is a huge number of features.

Our target is /SalePrice/.

#+begin_src python :results none
Y = data.SalePrice
#+end_src
** Step 2: Create X
#+begin_quote
 Now you will create a DataFrame called `X` holding the predictive features.
 
 Since you want only some columns from the original data, you'll first create a list with the names of the columns you want in `X`.
 
 You'll use just the following columns in the list (you can copy and paste the whole list to save some typing, though you'll still need to add quotes):
     * LotArea
     * YearBuilt
     * 1stFlrSF
     * 2ndFlrSF
     * FullBath
     * BedroomAbvGr
     * TotRmsAbvGrd
#+end_quote

#+begin_src python :results none
FEATURES = [
    "1stFlrSF",
    "2ndFlrSF",
    "BedroomAbvGr",
    "FullBath",
    "LotArea",
    "TotRmsAbvGrd",
    "YearBuilt",
]
X = data[FEATURES]
#+end_src

Split up the data into training and validation sets.

#+begin_src python :results none
x_train, x_validate, y_train, y_validate = train_test_split(X, Y, random_state=1)
#+end_src
** Step 3: Specify and Fit Model
*** A Linear Regression Model
    As a baseline, I'll fit a simple [[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html][Linear Regression]] (ordinary-least-squares) model.
#+begin_src python :results output :exports both
regression = LinearRegression()
scores = cross_val_score(regression, x_train, y_train, cv=5)
print(f"{scores.mean():0.2f} (+/- {2 * scores.std():0.2f})")
regression = regression.fit(x_train, y_train)
print(f"Training R^2: {regression.score(x_train, y_train): 0.2f}")
print(f"Validation R^2: {regression.score(x_validate, y_validate):0.2f}")
#+end_src

#+RESULTS:
: 0.66 (+/- 0.17)
: Training R^2:  0.68
: Validation R^2: 0.77

*** Decision Tree

#+begin_quote
Create a =DecisionTreeRegressor= and save it as =iowa_model=. Ensure you've done the relevant import from sklearn to run this command.

Then fit the model you just created using the data in =X= and =y= that you saved above.
#+end_quote
#+begin_src python :results output :exports both
tree = DecisionTreeRegressor()
scores = cross_val_score(tree, x_train, y_train, cv=5)
print(f"{scores.mean():0.2f} (+/- {2 * scores.std():0.2f})")

tree = tree.fit(x_train, y_train)
print(f"Training R^2: {tree.score(x_train, y_train): 0.2f}")
print(f"Validation R^2: {tree.score(x_validate, y_validate):0.2f}")
#+end_src

#+RESULTS:
: 0.54 (+/- 0.30)
: Training R^2:  1.00
: Validation R^2: 0.74

So our linear regression actually does better than the tree does. It looks like the tree might be overfitting on the training data.
** Make Predictions
#+begin_quote
Make predictions with the model's =predict= command using =X= as the data. Save the results to a variable called =predictions=.
#+end_quote
*** The whole data set
#+begin_src python :results none
predictions_tree = tree.predict(X)
predictions_line = regression.predict(X)

x_y_tree = pandas.DataFrame(dict(predicted=predictions_tree, actual=Y))
x_y_line = pandas.DataFrame(dict(predicted=predictions_line, actual=Y))
ideal = pandas.DataFrame(dict(x=Y, y=Y))

tree_plot = x_y_tree.hvplot.scatter(x="actual", y="predicted", label="Decision Tree")
line_plot = x_y_line.hvplot.scatter(x="actual", y="predicted", label="Linear Regression")
ideal_plot = ideal.hvplot(x="x", y="y")
plot = (tree_plot * line_plot * ideal_plot).opts(title="Actual Vs Predictions",
                                    width=Plot.width,
                                    height=Plot.height)
source = Embed(plot=plot, file_name="actual_vs_predicted")()
#+end_src

#+begin_src python :results output html :exports both
print(source)
#+end_src

#+RESULTS:
#+begin_export html
: <object type="text/html" data="actual_vs_predicted.html" style="width:100%" height=800>
:   <p>Figure Missing</p>
: </object>
#+end_export

Despite the \(r^2\) value the decision tree looks like it did better than the linear model.
*** The Validation Set
** Think About Your Results
#+begin_src python :results none
predictions_tree = tree.predict(x_validate)
predictions_line = regression.predict(x_validate)

x_y_tree = pandas.DataFrame(dict(predicted=predictions_tree, actual=y_validate))
x_y_line = pandas.DataFrame(dict(predicted=predictions_line, actual=y_validate))
ideal = pandas.DataFrame(dict(x=y_validate, y=y_validate))
tree_plot = x_y_tree.hvplot.scatter(x="actual", y="predicted", label="Decision Tree")
line_plot = x_y_line.hvplot.scatter(x="actual", y="predicted", label="Linear Regression")
ideal_plot = ideal.hvplot(x="x", y="y")
plot = (tree_plot * line_plot * ideal_plot).opts(title="Actual Vs Predictions (Validation Set)",
                                    width=Plot.width,
                                    height=Plot.height)
source = Embed(plot=plot, file_name="actual_vs_predicted_validation")()
#+end_src

#+begin_src python :results output html :exports both
print(source)
#+end_src

#+RESULTS:
#+begin_export html
: <object type="text/html" data="actual_vs_predicted_validation.html" style="width:100%" height=800>
:   <p>Figure Missing</p>
: </object>
#+end_export
It's hard to see which model really does better here - based on the \(r^2\) value the linear model did better, but the plot makes it look like it kind of goes off as the values get bigger.
** Step 4: Calculate the Mean Absolute Error in Validation Data

val_mae = ____

# uncomment following line to see the validation_mae
#print(val_mae)

* End
** Sources
   - De Cock D. Ames, Iowa: Alternative to the Boston housing data as an end of semester regression project. Journal of Statistics Education. 2011 Nov 1;19(3). [[http://jse.amstat.org/v19n3/decock.pdf][Link to PDF]]
