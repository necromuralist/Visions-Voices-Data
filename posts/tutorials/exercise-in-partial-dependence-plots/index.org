#+BEGIN_COMMENT
.. title: Exercise In Partial Dependence Plots
.. slug: exercise-in-partial-dependence-plots
.. date: 2020-02-09 13:26:37 UTC-08:00
.. tags: interpret,machine learning,visualization,tutorial
.. category: Machine Learning
.. link: 
.. description: An exercise in Partial Dependence Plots.
.. type: text
.. status: 
.. updated: 

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 
* Beginning
  This is my re-working of the Kaggle [[https://www.kaggle.com/learn/machine-learning-explainability][Machine Learning Explainability]] exercise in Partial Dependece Plots. It uses data from the [[https://www.kaggle.com/c/new-york-city-taxi-fare-prediction][Taxi Fare Prediction]] competition.
** Imports
*** Python
#+begin_src jupyter-python :session pdp :results none
from pathlib import Path
#+end_src
*** PyPi
#+begin_src jupyter-python :session pdp :results none
from matplotlib import pyplot
from matplotlib.pyplot import rcParams
from pdpbox import pdp
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, RandomizedSearchCV

import pandas
#+end_src
*** Others
#+begin_src jupyter-python :session pdp :results none
from graeae import EnvironmentLoader, Timer
#+end_src
** Set Up
*** The Environment
#+begin_src jupyter-python :session pdp :results none
ENVIRONMENT = EnvironmentLoader()
#+end_src
*** The Timer
#+begin_src jupyter-python :session pdp :results none
TIMER = Timer()
#+end_src
*** Plotting
#+begin_src jupyter-python :session pdp :results none
SLUG = "exercise-in-partial-dependence-plots"
OUTPUT_PATH = Path("../../files/posts/tutorials/")/SLUG

rcParams["figure.figsize"] = (6, 4)
#+end_src
*** The Data
#+begin_src jupyter-python :session pdp :results none
ROWS = 5 * 10**4
data = pandas.read_csv(Path(ENVIRONMENT["NY-TAXI"]).expanduser()/"train.csv",
                       nrows=ROWS)
data = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +
                  'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +
                  'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +
                  'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +
                  'fare_amount > 0'
                  )
y = data.fare_amount
base_features = ['pickup_longitude',
                 'pickup_latitude',
                 'dropoff_longitude',
                 'dropoff_latitude']

X = data[base_features]

x_train, x_validate, y_train, y_validate = train_test_split(X, y, random_state=1)
#+end_src
* Middle
** The First Model
#+begin_src jupyter-python :session pdp :results output :exports both
first_model = RandomForestRegressor(n_estimators=180,
                                    max_depth=50, random_state=1).fit(x_train, y_train)
print(f"Training R^2: {first_model.score(x_train, y_train):0.2f}")
print(f"Validation R^2: {first_model.score(x_validate, y_validate):0.2f}")
#+end_src

#+RESULTS:
: Training R^2: 0.92
: Validation R^2: 0.43

** Question 1

#+begin_src jupyter-python :session pdp :results output :exports both
FEATURE = 'pickup_longitude'
pdp_dist = pdp.pdp_isolate(model=first_model,
                           dataset=x_validate,
                           model_features=base_features,
                           feature=FEATURE)
pdp.pdp_plot(pdp_dist, FEATURE)
output = f"{FEATURE}_pdp_plot.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
#+end_src

#+RESULTS:
:RESULTS:
: [[file:pickup_longitude_pdp_plot.png]]
[[file:./.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png]]
:END:
 [[file:pickup_longitude_pdp_plot.png]][[file:./.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png]]

#+begin_quote
Why does the partial dependence plot have this U-shape?
#+end_quote

At the extremes you have the locations that can potentially travel the furthest, creating the biggest fairs, but as you move to the center you reduce the amount you can possibly travel - although the change isn't symmetric so this isn't the only explanation, otherwise if it were then you would expect the two ends to have the highest values and the nadir to be at the halfway point (-73.95).

#+begin_quote
Does your explanation suggest what shape to expect in the partial dependence plots for the other features?

Create all other partial plots.
#+end_quote

#+begin_src jupyter-python :session pdp :results output :exports both

for FEATURE in base_features:
    pdp_dist = pdp.pdp_isolate(model=first_model,
                               dataset=x_validate,
                               model_features=base_features,
                               feature=FEATURE)
    pdp.pdp_plot(pdp_dist, FEATURE)
    output = f"{FEATURE}_pdp_plot.png"
    pyplot.savefig(OUTPUT_PATH/output)
    print(f"[[file:{output}]]")
#+end_src

#+RESULTS:
:RESULTS:
: [[file:pickup_longitude_pdp_plot.png]]
: [[file:pickup_latitude_pdp_plot.png]]
: [[file:dropoff_longitude_pdp_plot.png]]
: [[file:dropoff_latitude_pdp_plot.png]]
[[file:./.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png]]
[[file:./.ob-jupyter/f2c9b641a0b8f044729d15efd208f672e3ecb7c1.png]]
[[file:./.ob-jupyter/6334156ec3931ff43161785fde6f1f27a460bccd.png]]
[[file:./.ob-jupyter/4417e1dcc2d023ce83c886e006ce5de690bfe97b.png]]
:END:
[[file:pickup_latitude_pdp_plot.png]]
[[file:dropoff_longitude_pdp_plot.png]]
[[file:dropoff_latitude_pdp_plot.png]][[file:./.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png]]

** Question 2

Now you will run a 2D partial dependence plot.  As a reminder, here is the code from the tutorial.  

 Create a 2D plot for the features =pickup_longitude= and =dropoff_longitude=.

#+begin_src jupyter-python :session pdp :results output :exports both
FEATURES = "pickup_longitude dropoff_longitude".split()
interaction  =  pdp.pdp_interact(model=first_model,
                                 dataset=x_validate,
                                 model_features=base_features,
                                 features=FEATURES)
pdp.pdp_interact_plot(pdp_interact_out=interaction,
                      feature_names=FEATURES, plot_type='contour')
output = "longitude_interaction.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
#+end_src

#+RESULTS:
:RESULTS:
: [[file:longitude_interaction.png]]
[[file:./.ob-jupyter/744ca56de862b725f3b5132f1c80c9bf41837026.png]]
:END:

[[file:longitude_interaction.png]]


Our plot shows that the fares are highest at the top-left and bottom-right corners, as you might expect, since this would be the furthest distance from pickup to dropoff.
** Question 3
#+begin_quote
Consider a ride starting at longitude -73.92 and ending at longitude -74. Using the graph from the last question, estimate how much money the rider would have saved if they'd started the ride at longitude -73.98 instead?
#+end_quote

I don't exactly agree with the interpretation given by the kaggle notebook. Looking at the plot, -73.92 to -74 appears to cost 27, while a -73.92 to -74 would cost 9 - but the notebook says that -73.92 to -74 costs 24. So I would say there would be a saving of 18 while the given answer is 15. To reconcile the difference (kind of) we might say that -73.92 to -74 costs 12, not 9 - it's not really easy to tell by the plot, in which case I would also say the savings is 15.
** Question 4
#+begin_quote
In the PDP's you've seen so far, location features have primarily served as a proxy to capture distance traveled. In the permutation importance lessons, you added the features `abs_lon_change` and `abs_lat_change` as a more direct measure of distance.

Create these features again here.

After you run it, identify the most important difference between this partial dependence plot and the one you got without absolute value features. The code to generate the PDP without absolute value features is at the top of this code cell.
#+end_quote

#+begin_src jupyter-python :session pdp :results output :exports both
FEATURE = 'pickup_longitude'
pdp_dist_original = pdp.pdp_isolate(model=first_model,
                                    dataset=x_validate,
                                    model_features=base_features,
                                    feature=FEATURE)
pdp.pdp_plot(pdp_dist_original, FEATURE)
output = "pre_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
#+end_src

#+RESULTS:
[[file:pre_distance.png]][[file:./.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png]]

#+begin_src jupyter-python :session pdp :results output :exports both
data['abs_lon_change'] = abs(data.pickup_longitude - data.dropoff_longitude)
data['abs_lat_change'] = abs(data.pickup_latitude - data.dropoff_longitude)

features_2  = ['pickup_longitude',
               'pickup_latitude',
               'dropoff_longitude',
               'dropoff_latitude',
               'abs_lat_change',
               'abs_lon_change']

X = data[features_2]
new_x_train, new_x_validate, new_y_train, new_y_validate = train_test_split(X, y, random_state=1)

estimators = list(range(50, 200, 10))
max_depth = list(range(10, 100, 10)) + [None]

grid = dict(n_estimators=estimators,
            max_depth=max_depth)

model = RandomForestRegressor()
search = RandomizedSearchCV(estimator=model,
                            param_distributions=grid,
                            n_iter=40,
                            n_jobs=-1,
                            random_state=1)
with TIMER:
    search.fit(new_x_train, new_y_train)
second_model = search.best_estimator_
print(f"CV Training R^2: {search.best_score_:0.2f}")
print(f"Training R^2: {first_model.score(x_train, y_train): 0.2f}")
print(f"Validation R^2: {first_model.score(x_validate, y_validate):0.2f}")
print(search.best_params_)
#+end_src

#+RESULTS:
: 2020-02-09 17:36:14,915 graeae.timers.timer start: Started: 2020-02-09 17:36:14.915123
: 2020-02-09 17:43:16,053 graeae.timers.timer end: Ended: 2020-02-09 17:43:16.053011
: 2020-02-09 17:43:16,053 graeae.timers.timer end: Elapsed: 0:07:01.137888
: CV Training R^2: 0.46
: Training R^2:  0.92
: Validation R^2: 0.43
: {'n_estimators': 190, 'max_depth': 30}

#+begin_src jupyter-python :session pdp :results output :exports both
FEATURE = 'pickup_longitude'
pdp_dist = pdp.pdp_isolate(model=second_model,
                           dataset=new_x_validate,
                           model_features=features_2,
                           feature=FEATURE)

pdp.pdp_plot(pdp_dist, FEATURE)
output = "pickup_longitude_with_distance_added.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
#+end_src

#+RESULTS:
[[file:pickup_longitude_with_distance_added.png]][[file:./.ob-jupyter/98c17657a972a412bc6d33619b6e23ee5818b884.png]]

** Question 5
#+begin_quote
Consider a scenario where you have only 2 predictive features, which we will call `feat_A` and `feat_B`. Both features have minimum values of -1 and maximum values of 1.  The partial dependence plot for `feat_A` increases steeply over its whole range, whereas the partial dependence plot for feature B increases at a slower rate (less steeply) over its whole range.
Does this guarantee that `feat_A` will have a higher permutation importance than `feat_B`.  Why or why not?
#+end_quote

It doesn't - the partial dependence plot shows how the predictions change based on the inputs, but it isn't the same thing as the feature importance - it might be the case that a few inputs create a large difference but most points don't, in which case the feature importance won't be very large.
