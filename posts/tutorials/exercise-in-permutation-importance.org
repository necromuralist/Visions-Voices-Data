#+BEGIN_COMMENT
.. title: Exercise in Permutation Importance
.. slug: exercise-in-permutation-importance
.. date: 2020-02-06 10:45:53 UTC-08:00
.. tags: tutorial,feature selection,permutation importance
.. category: Permutation Importance
.. link: 
.. description: An exercise in Permutation Importance
.. type: text
.. status: 
.. updated: 

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 
* Beginning
  This is my re-do of the [[https://www.kaggle.com/learn/machine-learning-explainability][Machine Learning Explainability]] Permutation Importance exercise on kaggle. It uses the data from the [[https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data][New York City Taxi Fare Prediction]] dataset on kaggle.
** Imports
*** From Python
#+BEGIN_SRC ipython :session permutation :results none
from argparse import Namespace
from functools import partial
from pathlib import Path
#+end_src
*** From PyPi
#+BEGIN_SRC ipython :session permutation :results none
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from tabulate import tabulate

import hvplot.pandas
import pandas
#+END_SRC
*** Others
#+BEGIN_SRC ipython :session permutation :results none
from graeae import EmbedHoloviews, EnvironmentLoader
#+end_src
** Set Up
*** Plotting
#+BEGIN_SRC ipython :session permutation :results none
SLUG = "exercise-in-permutation-importance"
PATH = Path("../../files/posts/tutorials/")/SLUG
Plot = Namespace(
    width=1000,
    height=800,
    )
Embed = partial(EmbedHoloviews, folder_path=PATH)
#+end_src
*** The Environment
#+BEGIN_SRC ipython :session permutation :results none
ENVIRONMENT = EnvironmentLoader()
#+end_src
*** Table Printer
#+BEGIN_SRC ipython :session permutation :results none
TABLE = partial(tabulate, tablefmt="orgtbl", headers="keys")
#+end_src
* Middle
*** The Dataset
    Since this is about permutation importance we're just going to load a subset and use previously discovered cleaning methods.

#+BEGIN_SRC ipython :session permutation :results output raw :exports both
PATH = Path(ENVIRONMENT["NY-TAXI"]).expanduser()
assert PATH.is_dir()
data = pandas.read_csv(PATH/"train.csv", nrows=50000)
print(TABLE(data.iloc[0].reset_index().rename(columns={"index": "Column", 0: "Value"}), showindex=False))
#+end_src

#+RESULTS:
| Column            |                       Value |
|-------------------+-----------------------------|
| key               | 2009-06-15 17:26:21.0000001 |
| fare_amount       |                         4.5 |
| pickup_datetime   |     2009-06-15 17:26:21 UTC |
| pickup_longitude  |                  -73.844311 |
| pickup_latitude   |                   40.721319 |
| dropoff_longitude |                   -73.84161 |
| dropoff_latitude  |          40.712278000000005 |
| passenger_count   |                           1 |

**** Trim Outliers
#+BEGIN_SRC ipython :session permutation :results output raw :exports both
print(TABLE(data.describe()))
#+END_SRC

#+RESULTS:
|       | fare_amount | pickup_longitude | pickup_latitude | dropoff_longitude | dropoff_latitude | passenger_count |
|-------+-------------+------------------+-----------------+-------------------+------------------+-----------------|
| count |       50000 |            50000 |           50000 |             50000 |            50000 |           50000 |
| mean  |     11.3642 |         -72.5098 |         39.9338 |          -72.5046 |          39.9263 |         1.66784 |
| std   |     9.68556 |          10.3939 |         6.22486 |           10.4076 |          6.01474 |         1.28919 |
| min   |          -5 |         -75.4238 |        -74.0069 |          -84.6542 |         -74.0064 |               0 |
| 25%   |           6 |         -73.9921 |         40.7349 |          -73.9912 |          40.7344 |               1 |
| 50%   |         8.5 |         -73.9818 |         40.7527 |          -73.9801 |          40.7534 |               1 |
| 75%   |        12.5 |         -73.9671 |         40.7674 |          -73.9636 |          40.7682 |               2 |
| max   |         200 |          40.7835 |         401.083 |            40.851 |          43.4152 |               6 |

#+BEGIN_SRC ipython :session permutation :results output raw :exports both
to_plot = data[[column for column in data.columns if "latitude" in column or "longitude" in column]]
plot = to_plot.hvplot.box().opts(title="Column Box-Plots", width=Plot.width, height=Plot.height)
Embed(plot=plot, file_name="column_box_plots")()
#+END_SRC

#+RESULTS:
#+begin_export html
<object type="text/html" data="column_box_plots.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

So you can see that there are negaive fares, which seems wrong, and a big spread for some of the values.

     This uses the pandas [[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html][query]] method which let's you write slightly more readable code for boolean slicing.

#+BEGIN_SRC ipython :session permutation :results output :exports both
data = data.query("pickup_latitude > 40.7 and pickup_latitude < 40.8 and " +
                  "dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and " +
                  "pickup_longitude > -74 and pickup_longitude < -73.9 and " +
                  "dropoff_longitude > -74 and dropoff_longitude < -73.9 and " +
                  "fare_amount > 0"
                  )
#+END_SRC


* End
* Raw
#+BEGIN_EXAMPLE
data = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +
                  'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +
                  'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +
                  'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +
                  'fare_amount > 0'
                  )

y = data.fare_amount

base_features = ['pickup_longitude',
                 'pickup_latitude',
                 'dropoff_longitude',
                 'dropoff_latitude',
                 'passenger_count']

X = data[base_features]


train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
first_model = RandomForestRegressor(n_estimators=50, random_state=1).fit(train_X, train_y)

# Environment Set-Up for feedback system.
from learntools.core import binder
binder.bind(globals())
from learntools.ml_explainability.ex2 import *
print("Setup Complete")

# show data
print("Data sample:")
data.head()


# The following two cells may also be useful to understand the values in the training data:

# In[ ]:


train_X.describe()


# In[ ]:


train_y.describe()


# ## Question 1
# 
# The first model uses the following features
# - pickup_longitude
# - pickup_latitude
# - dropoff_longitude
# - dropoff_latitude
# - passenger_count
# 
# Before running any code... which variables seem potentially useful for predicting taxi fares? Do you think permutation importance will necessarily identify these features as important?
# 
# Once you've thought about it, run `q_1.solution()` below to see how you might think about this before running the code.

# In[ ]:


# Check your answer (Run this code cell to receive credit!)
q_1.solution()


# ## Question 2
# 
# Create a `PermutationImportance` object called `perm` to show the importances from `first_model`.  Fit it with the appropriate data and show the weights.
# 
# For your convenience, the code from the tutorial has been copied into a comment in this code cell.

# In[ ]:


import eli5
from eli5.sklearn import PermutationImportance

# Make a small change to the code below to use in this problem. 
# perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)

# Check your answer
q_2.check()

# uncomment the following line to visualize your results
# eli5.show_weights(perm, feature_names = val_X.columns.tolist())


# Uncomment the lines below for a hint or to see the solution.

# In[ ]:


# q_2.hint()
# q_2.solution()


# ## Question 3
# Before seeing these results, we might have expected each of the 4 directional features to be equally important.
# 
# But, on average, the latitude features matter more than the longititude features. Can you come up with any hypotheses for this?
# 
# After you've thought about it, check here for some possible explanations:

# In[ ]:


# Check your answer (Run this code cell to receive credit!)
q_3.solution()


# ## Question 4
# 
# Without detailed knowledge of New York City, it's difficult to rule out most hypotheses about why latitude features matter more than longitude.
# 
# A good next step is to disentangle the effect of being in certain parts of the city from the effect of total distance traveled.  
# 
# The code below creates new features for longitudinal and latitudinal distance. It then builds a model that adds these new features to those you already had.
# 
# Fill in two lines of code to calculate and show the importance weights with this new set of features. As usual, you can uncomment lines below to check your code, see a hint or get the solution.

# In[ ]:


# create new features
data['abs_lon_change'] = abs(data.dropoff_longitude - data.pickup_longitude)
data['abs_lat_change'] = abs(data.dropoff_latitude - data.pickup_latitude)

features_2  = ['pickup_longitude',
               'pickup_latitude',
               'dropoff_longitude',
               'dropoff_latitude',
               'abs_lat_change',
               'abs_lon_change']

X = data[features_2]
new_train_X, new_val_X, new_train_y, new_val_y = train_test_split(X, y, random_state=1)
second_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(new_train_X, new_train_y)

# Create a PermutationImportance object on second_model and fit it to new_val_X and new_val_y
# Use a random_state of 1 for reproducible results that match the expected solution.
perm2 = ____

# show the weights for the permutation importance you just calculated
____

# Check your answer
q_4.check()


# How would you interpret these importance scores? Distance traveled seems far more important than any location effects. 
# 
# But the location still affects model predictions, and dropoff location now matters slightly more than pickup location. Do you have any hypotheses for why this might be? The techniques in the next lessons will help you` dive into this more.

# In[ ]:


# Check your answer (Run this code cell to receive credit!)
q_4.solution()


# ## Question 5
# 
# A colleague observes that the values for `abs_lon_change` and `abs_lat_change` are pretty small (all values are between -0.1 and 0.1), whereas other variables have larger values.  Do you think this could explain why those coordinates had larger permutation importance values in this case?  
# 
# Consider an alternative where you created and used a feature that was 100X as large for these features, and used that larger feature for training and importance calculations. Would this change the outputted permutaiton importance values?
# 
# Why or why not?
# 
# After you have thought about your answer, either try this experiment or look up the answer in the cell below.

# In[ ]:


# Check your answer (Run this code cell to receive credit!)
q_5.solution()


# ## Question 6
# 
# You've seen that the feature importance for latitudinal distance is greater than the importance of longitudinal distance. From this, can we conclude whether travelling a fixed latitudinal distance tends to be more expensive than traveling the same longitudinal distance?
# 
# Why or why not? Check your answer below.

# In[ ]:


# Check your answer (Run this code cell to receive credit!)
q_6.solution()


# ## Keep Going
# 
# Permutation importance is useful useful for debugging, understanding your model, and communicating a high-level overview from your model.  
# 
# Next, learn about **[partial dependence plots](https://www.kaggle.com/dansbecker/partial-plots)** to see **how** each feature affects predictions.
# 

# ---
# **[Machine Learning Explainability Home Page](https://www.kaggle.com/learn/machine-learning-explainability)**
# 
# 
# 
# 
# 
# *Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*

#+END_EXAMPLE
