#+BEGIN_COMMENT
.. title: Exercise in Permutation Importance
.. slug: exercise-in-permutation-importance
.. date: 2020-02-06 10:45:53 UTC-08:00
.. tags: tutorial,feature selection,permutation importance
.. category: Permutation Importance
.. link: 
.. description: An exercise in Permutation Importance
.. type: text
.. status: 
.. updated: 

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 
* Beginning
  This is my re-do of the [[https://www.kaggle.com/learn/machine-learning-explainability][Machine Learning Explainability]] Permutation Importance exercise on kaggle. It uses the data from the [[https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data][New York City Taxi Fare Prediction]] dataset on kaggle.
** Imports
*** From Python
#+BEGIN_SRC ipython :session permutation :results none
from argparse import Namespace
from functools import partial
from pathlib import Path
#+end_src
*** From PyPi
#+BEGIN_SRC ipython :session permutation :results none
from eli5.sklearn import PermutationImportance

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from tabulate import tabulate

import eli5
import hvplot.pandas
import pandas
#+END_SRC
*** Others
#+BEGIN_SRC ipython :session permutation :results none
from graeae import EmbedHoloviews, EnvironmentLoader
#+end_src
** Set Up
*** Plotting
#+BEGIN_SRC ipython :session permutation :results none
SLUG = "exercise-in-permutation-importance"
PATH = Path("../../files/posts/tutorials/")/SLUG
Plot = Namespace(
    width=1000,
    height=800,
    )
Embed = partial(EmbedHoloviews, folder_path=PATH)
#+end_src
*** The Environment
#+BEGIN_SRC ipython :session permutation :results none
ENVIRONMENT = EnvironmentLoader()
#+end_src
*** Table Printer
#+BEGIN_SRC ipython :session permutation :results none
TABLE = partial(tabulate, tablefmt="orgtbl", headers="keys", showindex=False)
#+end_src
* Middle
*** The Dataset
    Since this is about permutation importance we're just going to load a subset and use previously discovered cleaning methods.

#+BEGIN_SRC ipython :session permutation :results output raw :exports both
PATH = Path(ENVIRONMENT["NY-TAXI"]).expanduser()
assert PATH.is_dir()
data = pandas.read_csv(PATH/"train.csv", nrows=50000)
print(TABLE(data.iloc[0].reset_index().rename(columns={"index": "Column", 0: "Value"})))
#+end_src

#+RESULTS:
| Column            |                       Value |
|-------------------+-----------------------------|
| key               | 2009-06-15 17:26:21.0000001 |
| fare_amount       |                         4.5 |
| pickup_datetime   |     2009-06-15 17:26:21 UTC |
| pickup_longitude  |                  -73.844311 |
| pickup_latitude   |                   40.721319 |
| dropoff_longitude |                   -73.84161 |
| dropoff_latitude  |          40.712278000000005 |
| passenger_count   |                           1 |

**** Trim Outliers
#+BEGIN_SRC ipython :session permutation :results output raw :exports both
print(TABLE(data.describe(), showindex=True))
#+END_SRC

#+RESULTS:
|       | fare_amount | pickup_longitude | pickup_latitude | dropoff_longitude | dropoff_latitude | passenger_count |
|-------+-------------+------------------+-----------------+-------------------+------------------+-----------------|
| count |       50000 |            50000 |           50000 |             50000 |            50000 |           50000 |
| mean  |     11.3642 |         -72.5098 |         39.9338 |          -72.5046 |          39.9263 |         1.66784 |
| std   |     9.68556 |          10.3939 |         6.22486 |           10.4076 |          6.01474 |         1.28919 |
| min   |          -5 |         -75.4238 |        -74.0069 |          -84.6542 |         -74.0064 |               0 |
| 25%   |           6 |         -73.9921 |         40.7349 |          -73.9912 |          40.7344 |               1 |
| 50%   |         8.5 |         -73.9818 |         40.7527 |          -73.9801 |          40.7534 |               1 |
| 75%   |        12.5 |         -73.9671 |         40.7674 |          -73.9636 |          40.7682 |               2 |
| max   |         200 |          40.7835 |         401.083 |            40.851 |          43.4152 |               6 |

#+BEGIN_SRC ipython :session permutation :results output raw :exports both
to_plot = data[[column for column in data.columns if "latitude" in column or "longitude" in column]]
plot = to_plot.hvplot.box().opts(title="Column Box-Plots", width=Plot.width, height=Plot.height)
Embed(plot=plot, file_name="column_box_plots")()
#+END_SRC

#+RESULTS:
#+begin_export html
<object type="text/html" data="column_box_plots.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

So you can see that there are negative fares, which seems wrong, and some outliers.

This uses the pandas [[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html][query]] method which let's you write slightly more readable code for boolean slicing.

#+BEGIN_SRC ipython :session permutation :results output :exports both
print(f"{len(data)}")
data = data.query("pickup_latitude > 40.7 and pickup_latitude < 40.8 and " +
                  "dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and " +
                  "pickup_longitude > -74 and pickup_longitude < -73.9 and " +
                  "dropoff_longitude > -74 and dropoff_longitude < -73.9 and " +
                  "fare_amount > 0"
                  )
print(f"{len(data)}"
#+END_SRC

*** Set Up the Training and Test Sets
#+BEGIN_SRC ipython :session permutation :results output :exports both
y = data.fare_amount
base_features = ['pickup_longitude',
                 'pickup_latitude',
                 'dropoff_longitude',
                 'dropoff_latitude',
                 'passenger_count']

X = data[base_features]
x_train, x_validate, y_train, y_validate = train_test_split(X, y, random_state=1)

print(f"{len(x_train):,}")
print(f"{len(x_validate):,}")
#+END_SRC

#+RESULTS:
: 23,466
: 7,823
** Build and Train the Model
#+BEGIN_SRC ipython :session permutation :results none
first_model = RandomForestRegressor(n_estimators=50, random_state=1).fit(x_train, y_train)
#+end_src

** Questions
*** Question 1
#+begin_quote
 The first model uses the following features:
 - pickup_longitude
 - pickup_latitude
 - dropoff_longitude
 - dropoff_latitude
 - passenger_count

Before running any code... which variables seem potentially useful for predicting taxi fares? Do you think permutation importance will necessarily identify these features as important?
#+end_quote

I think that pickup and dropoff latitude might be important, since this would reflect where in the city the person was and wanted to go. Passenger count might make a difference as well, but I don't know if there's a greater charge for more people. Longitude might also be useful, but my guess would be that the North-South location is more indicative of the type of place you are in (uptown or downtown) and thus how far you have to travel (I have a vague notion that New York City is longer vertically than horizontally, but I don't know if this is true). This would be even more important if the fares change by location, but I don't know if that's the case.

#+BEGIN_SRC ipython :session permutation :results none
permutor = PermutationImportance(first_model, random_state=1).fit(x_validate, y_validate)
#+end_src

#+BEGIN_SRC ipython :session permutation :results output raw :exports both
ipython_html = eli5.show_weights(permutor, feature_names=x_validate.columns.tolist())
table = pandas.read_html(ipython_html.data)[0]
print(TABLE(table))
#+END_SRC

#+RESULTS:
| Weight            | Feature           |
|-------------------+-------------------|
| 0.8387  ± 0.0168  | dropoff_latitude  |
| 0.8326  ± 0.0212  | pickup_latitude   |
| 0.5947  ± 0.0432  | pickup_longitude  |
| 0.5326  ± 0.0275  | dropoff_longitude |
| -0.0022  ± 0.0014 | passenger_count   |

So it looks like latitude and longitude are important, with latitude a little more important than longitude and passenger count isn't important.
** A New Model
*** Question 4
#+begin_quote
Without detailed knowledge of New York City, it's difficult to rule out most hypotheses about why latitude features matter more than longitude.

A good next step is to disentangle the effect of being in certain parts of the city from the effect of total distance traveled.  

The code below creates new features for longitudinal and latitudinal distance. It then builds a model that adds these new features to those you already had.
#+end_quote

*** Feature Engineering
    We're going to estimate the distance traveled by using the differences in latitude and longitude from the pickup to the dropoff. This should give us a taxicab-distance estimate.

#+BEGIN_SRC ipython :session permutation :results none
data['absolute_change_longitude'] = abs(data.dropoff_longitude - data.pickup_longitude)
data['absolute_change_latitude'] = abs(data.dropoff_latitude - data.pickup_latitude)

features_2  = ['pickup_longitude',
               'pickup_latitude',
               'dropoff_longitude',
               'dropoff_latitude',
               'absolute_change_latitude',
               'absolute_change_longitude']

X = data[features_2]
new_x_train, new_x_validate, new_y_train, new_y_validate = train_test_split(X, y, random_state=1)
second_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(new_x_train, new_y_train)
#+end_src

*** The Permutation Importance
#+BEGIN_SRC ipython :session permutation :results output raw :exports both
permutor = PermutationImportance(second_model, random_state=1).fit(new_x_validate, new_y_validate)
ipython_html = eli5.show_weights(permutor, feature_names=new_x_validate.columns.tolist())
table = pandas.read_html(ipython_html.data)[0]
print(TABLE(table))
#+END_SRC

#+RESULTS:
| Weight           | Feature                   |
|------------------+---------------------------|
| 0.5786  ± 0.0294 | absolute_change_latitude  |
| 0.4469  ± 0.0509 | absolute_change_longitude |
| 0.0860  ± 0.0334 | pickup_latitude           |
| 0.0735  ± 0.0114 | dropoff_latitude          |
| 0.0735  ± 0.0101 | dropoff_longitude         |
| 0.0609  ± 0.0067 | pickup_longitude          |

The distance traveled seems to be the most important feature for the fair, even more than the actual locations, probably because taxis charge by distance.

*** Question 5

#+BEGIN_SRC ipython :session permutation :results output raw :exports both
print(TABLE(new_x_train.sample(random_state=1).iloc[0].reset_index()))
#+END_SRC

#+RESULTS:
| index                     |    31975 |
|---------------------------+----------|
| pickup_longitude          | -73.9706 |
| pickup_latitude           |  40.7613 |
| dropoff_longitude         | -73.9806 |
| dropoff_latitude          |  40.7483 |
| absolute_change_latitude  |  0.01302 |
| absolute_change_longitude | 0.010067 |

#+BEGIN_SRC ipython :session permutation :results output :exports both
print(new_x_validate.describe())
#+END_SRC

#+RESULTS:
#+begin_example
       pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \
count       7823.000000      7823.000000        7823.000000       7823.000000   
mean         -73.976957        40.756877         -73.975293         40.757591   
std            0.014663         0.018064           0.015877          0.018669   
min          -73.999977        40.700400         -73.999992         40.700293   
25%          -73.988180        40.745044         -73.987078         40.746345   
50%          -73.979933        40.757881         -73.978427         40.758602   
75%          -73.968008        40.769486         -73.966296         40.770561   
max          -73.900123        40.799865         -73.901790         40.799984   

       absolute_change_latitude  absolute_change_longitude  
count               7823.000000                7823.000000  
mean                   0.015091                   0.013029  
std                    0.012508                   0.011554  
min                    0.000000                   0.000000  
25%                    0.006089                   0.004968  
50%                    0.011745                   0.010110  
75%                    0.020781                   0.017798  
max                    0.084413                   0.087337  
#+end_example

#+begin_quote
 A colleague observes that the values for =absolute_change_longitude= and =absolute_change_latitude= are pretty small (all values are between -0.1 and 0.1), whereas other variables have larger values.  Do you think this could explain why those coordinates had larger permutation importance values in this case?  

Consider an alternative where you created and used a feature that was 100X as large for these features, and used that larger feature for training and importance calculations. Would this change the outputted permutation importance values?

Why or why not?
#+end_quote

#+BEGIN_SRC ipython :session permutation :results output :exports both
for column in ("pickup_longitude pickup_latitude dropoff_longitude "
               "dropoff_latitude absolute_change_latitude "
               "absolute_change_longitude").split():
    print(f"{column}: {new_x_validate[column].max() - new_x_validate[column].min():0.3f}")
#+END_SRC

#+RESULTS:
: pickup_longitude: 0.100
: pickup_latitude: 0.099
: dropoff_longitude: 0.098
: dropoff_latitude: 0.100
: absolute_change_latitude: 0.084
: absolute_change_longitude: 0.087

Intuitively I would think that the difference in the scales would make a difference.

#+begin_src ipython :session permutation :results none
data["bigger_pickup_longitude"] = data.pickup_longitude * 100
data["bigger_absolute_change_longitude"] = data.absolute_change_longitude * 100
features_3  = ['pickup_longitude',
               'pickup_latitude',
               'dropoff_longitude',
               'dropoff_latitude',
               'absolute_change_latitude',
               'absolute_change_longitude',
               'bigger_pickup_longitude',
               'bigger_absolute_change_longitude'
               ]

X = data[features_3]
big_x_train, big_x_validate, big_y_train, big_y_validate = train_test_split(X, y, random_state=1)
big_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(big_x_train, big_y_train)
#+end_src

#+BEGIN_SRC ipython :session permutation :results output raw :exports both
permutor = PermutationImportance(big_model, random_state=1).fit(big_x_validate, big_y_validate)
ipython_html = eli5.show_weights(permutor, feature_names=big_x_validate.columns.tolist())
table = pandas.read_html(ipython_html.data)[0]
print(TABLE(table))

#+END_SRC

#+RESULTS:
| Weight           | Feature                          |
|------------------+----------------------------------|
| 0.5939  ± 0.0578 | absolute_change_latitude         |
| 0.2875  ± 0.0181 | absolute_change_longitude        |
| 0.2084  ± 0.0166 | bigger_absolute_change_longitude |
| 0.0754  ± 0.0281 | pickup_latitude                  |
| 0.0717  ± 0.0075 | dropoff_latitude                 |
| 0.0648  ± 0.0119 | dropoff_longitude                |
| 0.0460  ± 0.0125 | bigger_pickup_longitude          |
| 0.0345  ± 0.0036 | pickup_longitude                 |

Making the pickup longitude made it more important than the original, but it didn't change its ranking relative to the other features so I wouldn't say that the scale had an effect.

*** Question 6
#+begin_quote
You've seen that the feature importance for latitudinal distance is greater than the importance of longitudinal distance. From this, can we conclude whether travelling a fixed latitudinal distance tends to be more expensive than traveling the same longitudinal distance?
#+end_quote

No, the feature importance indicates that it is useful in predicting fares, but it doesn't automatically mean that the fares will increase with the change in latitude. It might be the case that the change in longitude affects the cost of a change in latitude as well, so a fixed latitude distance might change depending on the longitude or latitude + longitude combination.
* End
  The suggested next tutorial is about [[https://www.kaggle.com/dansbecker/partial-plots][Partial Dependence Plots]].

