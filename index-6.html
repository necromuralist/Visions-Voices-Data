<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Adumbrations of data." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Visions, Voices, Data (old posts, page 6) | Visions, Voices, Data</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Visions-Voices-Data/index-6.html" rel="canonical">
<link href="." rel="prev" type="text/html">
<link href="index-5.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><!--
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/p5.min.js"
</script>
-->
<script async src="javascript/p5.min.js" type="text/javascript"></script>
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/Visions-Voices-Data/"><span id="blog-title">Visions, Voices, Data</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item dropdown"><a aria-expanded="false" aria-haspopup="true" class="nav-link dropdown-toggle" data-toggle="dropdown" href="#">Pages</a>
<div class="dropdown-menu"><a class="dropdown-item" href="https://necromuralist.github.io/">Cloistered Monkey</a> <a class="dropdown-item" href="/pages/giss/giss-yearly-anomalies-by-climate-zone">GISS Anomalies</a></div>
</li>
</ul>
<!-- DuckDuckGo custom search -->
<form action="https://duckduckgo.com/" class="navbar-form pull-left" id="search" method="get" name="search"><input name="sites" type="hidden" value="https://necromuralist.github.io/Visions-Voices-Data/"> <input name="k8" type="hidden" value="#444444"> <input name="k9" type="hidden" value="#D51920"> <input name="kt" type="hidden" value="h"> <input class="span2" maxlength="255" name="q" placeholder="Searchâ€¦" type="text"> <input style="visibility: hidden;display:none" type="submit" value="DuckDuckGo Search"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/tutorials/exercise-in-partial-dependence-plots/">Exercise In Partial Dependence Plots</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/tutorials/exercise-in-partial-dependence-plots/" rel="bookmark"><time class="published dt-published" datetime="2020-02-09T13:26:37-08:00" itemprop="datePublished" title="2020-02-09 13:26">2020-02-09 13:26</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org116c1e6">Beginning</a>
<ul>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org6c9c970">Imports</a>
<ul>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org1671a2c">Python</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org5132b35">PyPi</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#orgec8ed0d">Others</a></li>
</ul>
</li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#orgdae510a">Set Up</a>
<ul>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#orgbcfa12b">The Environment</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#orgbfcd106">The Timer</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org01fb5ec">Plotting</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org0feb87b">The Data</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org48a820f">Middle</a>
<ul>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#orgd59439b">The First Model</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org5b9a1c3">Question 1</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#orgf53f3c3">Question 2</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org5ec7614">Question 3</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#orgdc69d42">Question 4</a></li>
<li><a href="/posts/tutorials/exercise-in-partial-dependence-plots/#org77524af">Question 5</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org116c1e6">
<h2 id="org116c1e6">Beginning</h2>
<div class="outline-text-2" id="text-org116c1e6">
<p>This is my re-working of the Kaggle <a href="https://www.kaggle.com/learn/machine-learning-explainability">Machine Learning Explainability</a> exercise in Partial Dependece Plots. It uses data from the <a href="https://www.kaggle.com/c/new-york-city-taxi-fare-prediction">Taxi Fare Prediction</a> competition.</p>
</div>
<div class="outline-3" id="outline-container-org6c9c970">
<h3 id="org6c9c970">Imports</h3>
<div class="outline-text-3" id="text-org6c9c970"></div>
<div class="outline-4" id="outline-container-org1671a2c">
<h4 id="org1671a2c">Python</h4>
<div class="outline-text-4" id="text-org1671a2c">
<div class="highlight">
<pre><span></span>from pathlib import Path
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5132b35">
<h4 id="org5132b35">PyPi</h4>
<div class="outline-text-4" id="text-org5132b35">
<div class="highlight">
<pre><span></span>from matplotlib import pyplot
from matplotlib.pyplot import rcParams
from pdpbox import pdp
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, RandomizedSearchCV

import pandas
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgec8ed0d">
<h4 id="orgec8ed0d">Others</h4>
<div class="outline-text-4" id="text-orgec8ed0d">
<div class="highlight">
<pre><span></span>from graeae import EnvironmentLoader, Timer
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgdae510a">
<h3 id="orgdae510a">Set Up</h3>
<div class="outline-text-3" id="text-orgdae510a"></div>
<div class="outline-4" id="outline-container-orgbcfa12b">
<h4 id="orgbcfa12b">The Environment</h4>
<div class="outline-text-4" id="text-orgbcfa12b">
<div class="highlight">
<pre><span></span>ENVIRONMENT = EnvironmentLoader()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgbfcd106">
<h4 id="orgbfcd106">The Timer</h4>
<div class="outline-text-4" id="text-orgbfcd106">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org01fb5ec">
<h4 id="org01fb5ec">Plotting</h4>
<div class="outline-text-4" id="text-org01fb5ec">
<div class="highlight">
<pre><span></span>SLUG = "exercise-in-partial-dependence-plots"
OUTPUT_PATH = Path("../../files/posts/tutorials/")/SLUG

rcParams["figure.figsize"] = (6, 4)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org0feb87b">
<h4 id="org0feb87b">The Data</h4>
<div class="outline-text-4" id="text-org0feb87b">
<div class="highlight">
<pre><span></span>ROWS = 5 * 10**4
data = pandas.read_csv(Path(ENVIRONMENT["NY-TAXI"]).expanduser()/"train.csv",
                       nrows=ROWS)
data = data.query('pickup_latitude &gt; 40.7 and pickup_latitude &lt; 40.8 and ' +
                  'dropoff_latitude &gt; 40.7 and dropoff_latitude &lt; 40.8 and ' +
                  'pickup_longitude &gt; -74 and pickup_longitude &lt; -73.9 and ' +
                  'dropoff_longitude &gt; -74 and dropoff_longitude &lt; -73.9 and ' +
                  'fare_amount &gt; 0'
                  )
y = data.fare_amount
base_features = ['pickup_longitude',
                 'pickup_latitude',
                 'dropoff_longitude',
                 'dropoff_latitude']

X = data[base_features]

x_train, x_validate, y_train, y_validate = train_test_split(X, y, random_state=1)
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org48a820f">
<h2 id="org48a820f">Middle</h2>
<div class="outline-text-2" id="text-org48a820f"></div>
<div class="outline-3" id="outline-container-orgd59439b">
<h3 id="orgd59439b">The First Model</h3>
<div class="outline-text-3" id="text-orgd59439b">
<div class="highlight">
<pre><span></span>first_model = RandomForestRegressor(n_estimators=180,
                                    max_depth=50, random_state=1).fit(x_train, y_train)
print(f"Training R^2: {first_model.score(x_train, y_train):0.2f}")
print(f"Validation R^2: {first_model.score(x_validate, y_validate):0.2f}")
</pre></div>
<pre class="example">
Training R^2: 0.92
Validation R^2: 0.43
</pre></div>
</div>
<div class="outline-3" id="outline-container-org5b9a1c3">
<h3 id="org5b9a1c3">Question 1</h3>
<div class="outline-text-3" id="text-org5b9a1c3">
<div class="highlight">
<pre><span></span>FEATURE = 'pickup_longitude'
pdp_dist = pdp.pdp_isolate(model=first_model,
                           dataset=x_validate,
                           model_features=base_features,
                           feature=FEATURE)
pdp.pdp_plot(pdp_dist, FEATURE)
output = f"{FEATURE}_pdp_plot.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
</pre></div>
<pre class="example">
[[file:pickup_longitude_pdp_plot.png]]
</pre>
<div class="figure">
<p><img alt="eb446225f180346680b332c924342792de7e5135.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png"></p>
</div>
<p><img alt="pickup_longitude_pdp_plot.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/pickup_longitude_pdp_plot.png"><img alt="eb446225f180346680b332c924342792de7e5135.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png"></p>
<blockquote>
<p>Why does the partial dependence plot have this U-shape?</p>
</blockquote>
<p>At the extremes you have the locations that can potentially travel the furthest, creating the biggest fairs, but as you move to the center you reduce the amount you can possibly travel - although the change isn't symmetric so this isn't the only explanation, otherwise if it were then you would expect the two ends to have the highest values and the nadir to be at the halfway point (-73.95).</p>
<blockquote>
<p>Does your explanation suggest what shape to expect in the partial dependence plots for the other features?</p>
<p>Create all other partial plots.</p>
</blockquote>
<div class="highlight">
<pre><span></span>for FEATURE in base_features:
    pdp_dist = pdp.pdp_isolate(model=first_model,
                               dataset=x_validate,
                               model_features=base_features,
                               feature=FEATURE)
    pdp.pdp_plot(pdp_dist, FEATURE)
    output = f"{FEATURE}_pdp_plot.png"
    pyplot.savefig(OUTPUT_PATH/output)
    print(f"[[file:{output}]]")
</pre></div>
<pre class="example">
[[file:pickup_longitude_pdp_plot.png]]
[[file:pickup_latitude_pdp_plot.png]]
[[file:dropoff_longitude_pdp_plot.png]]
[[file:dropoff_latitude_pdp_plot.png]]
</pre>
<p><img alt="eb446225f180346680b332c924342792de7e5135.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png"> <img alt="f2c9b641a0b8f044729d15efd208f672e3ecb7c1.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/f2c9b641a0b8f044729d15efd208f672e3ecb7c1.png"> <img alt="6334156ec3931ff43161785fde6f1f27a460bccd.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/6334156ec3931ff43161785fde6f1f27a460bccd.png"> <img alt="4417e1dcc2d023ce83c886e006ce5de690bfe97b.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/4417e1dcc2d023ce83c886e006ce5de690bfe97b.png"></p>
<p><img alt="pickup_latitude_pdp_plot.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/pickup_latitude_pdp_plot.png"> <img alt="dropoff_longitude_pdp_plot.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/dropoff_longitude_pdp_plot.png"> <img alt="dropoff_latitude_pdp_plot.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/dropoff_latitude_pdp_plot.png"><img alt="eb446225f180346680b332c924342792de7e5135.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png"></p>
</div>
</div>
<div class="outline-3" id="outline-container-orgf53f3c3">
<h3 id="orgf53f3c3">Question 2</h3>
<div class="outline-text-3" id="text-orgf53f3c3">
<p>Now you will run a 2D partial dependence plot. As a reminder, here is the code from the tutorial.</p>
<p>Create a 2D plot for the features <code>pickup_longitude</code> and <code>dropoff_longitude</code>.</p>
<div class="highlight">
<pre><span></span>FEATURES = "pickup_longitude dropoff_longitude".split()
interaction  =  pdp.pdp_interact(model=first_model,
                                 dataset=x_validate,
                                 model_features=base_features,
                                 features=FEATURES)
pdp.pdp_interact_plot(pdp_interact_out=interaction,
                      feature_names=FEATURES, plot_type='contour')
output = "longitude_interaction.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
</pre></div>
<pre class="example">
[[file:longitude_interaction.png]]
</pre>
<div class="figure">
<p><img alt="744ca56de862b725f3b5132f1c80c9bf41837026.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/744ca56de862b725f3b5132f1c80c9bf41837026.png"></p>
</div>
<div class="figure">
<p><img alt="longitude_interaction.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/longitude_interaction.png"></p>
</div>
<p>Our plot shows that the fares are highest at the top-left and bottom-right corners, as you might expect, since this would be the furthest distance from pickup to dropoff.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org5ec7614">
<h3 id="org5ec7614">Question 3</h3>
<div class="outline-text-3" id="text-org5ec7614">
<blockquote>
<p>Consider a ride starting at longitude -73.92 and ending at longitude -74. Using the graph from the last question, estimate how much money the rider would have saved if they'd started the ride at longitude -73.98 instead?</p>
</blockquote>
<p>I don't exactly agree with the interpretation given by the kaggle notebook. Looking at the plot, -73.92 to -74 appears to cost 27, while a -73.92 to -74 would cost 9 - but the notebook says that -73.92 to -74 costs 24. So I would say there would be a saving of 18 while the given answer is 15. To reconcile the difference (kind of) we might say that -73.92 to -74 costs 12, not 9 - it's not really easy to tell by the plot, in which case I would also say the savings is 15.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgdc69d42">
<h3 id="orgdc69d42">Question 4</h3>
<div class="outline-text-3" id="text-orgdc69d42">
<blockquote>
<p>In the PDP's you've seen so far, location features have primarily served as a proxy to capture distance traveled. In the permutation importance lessons, you added the features `abs_lon_change` and `abs_lat_change` as a more direct measure of distance.</p>
<p>Create these features again here.</p>
<p>After you run it, identify the most important difference between this partial dependence plot and the one you got without absolute value features. The code to generate the PDP without absolute value features is at the top of this code cell.</p>
</blockquote>
<div class="highlight">
<pre><span></span>FEATURE = 'pickup_longitude'
pdp_dist_original = pdp.pdp_isolate(model=first_model,
                                    dataset=x_validate,
                                    model_features=base_features,
                                    feature=FEATURE)
pdp.pdp_plot(pdp_dist_original, FEATURE)
output = "pre_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
</pre></div>
<p><img alt="pre_distance.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/pre_distance.png"><img alt="eb446225f180346680b332c924342792de7e5135.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png"></p>
<div class="highlight">
<pre><span></span>data['abs_lon_change'] = abs(data.pickup_longitude - data.dropoff_longitude)
data['abs_lat_change'] = abs(data.pickup_latitude - data.dropoff_longitude)

features_2  = ['pickup_longitude',
               'pickup_latitude',
               'dropoff_longitude',
               'dropoff_latitude',
               'abs_lat_change',
               'abs_lon_change']

X = data[features_2]
new_x_train, new_x_validate, new_y_train, new_y_validate = train_test_split(X, y, random_state=1)

estimators = list(range(50, 200, 10))
max_depth = list(range(10, 100, 10)) + [None]

grid = dict(n_estimators=estimators,
            max_depth=max_depth)

model = RandomForestRegressor()
search = RandomizedSearchCV(estimator=model,
                            param_distributions=grid,
                            n_iter=40,
                            n_jobs=-1,
                            random_state=1)
with TIMER:
    search.fit(new_x_train, new_y_train)
second_model = search.best_estimator_
print(f"CV Training R^2: {search.best_score_:0.2f}")
print(f"Training R^2: {first_model.score(x_train, y_train): 0.2f}")
print(f"Validation R^2: {first_model.score(x_validate, y_validate):0.2f}")
print(search.best_params_)
</pre></div>
<pre class="example">
2020-02-09 17:36:14,915 graeae.timers.timer start: Started: 2020-02-09 17:36:14.915123
2020-02-09 17:43:16,053 graeae.timers.timer end: Ended: 2020-02-09 17:43:16.053011
2020-02-09 17:43:16,053 graeae.timers.timer end: Elapsed: 0:07:01.137888
CV Training R^2: 0.46
Training R^2:  0.92
Validation R^2: 0.43
{'n_estimators': 190, 'max_depth': 30}
</pre>
<div class="highlight">
<pre><span></span>FEATURE = 'pickup_longitude'
pdp_dist = pdp.pdp_isolate(model=second_model,
                           dataset=new_x_validate,
                           model_features=features_2,
                           feature=FEATURE)

pdp.pdp_plot(pdp_dist, FEATURE)
output = "pickup_longitude_with_distance_added.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
</pre></div>
<p><img alt="pickup_longitude_with_distance_added.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/pickup_longitude_with_distance_added.png"><img alt="98c17657a972a412bc6d33619b6e23ee5818b884.png" src="/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/98c17657a972a412bc6d33619b6e23ee5818b884.png"></p>
</div>
</div>
<div class="outline-3" id="outline-container-org77524af">
<h3 id="org77524af">Question 5</h3>
<div class="outline-text-3" id="text-org77524af">
<blockquote>
<p>Consider a scenario where you have only 2 predictive features, which we will call `feat_A` and `feat_B`. Both features have minimum values of -1 and maximum values of 1. The partial dependence plot for `feat_A` increases steeply over its whole range, whereas the partial dependence plot for feature B increases at a slower rate (less steeply) over its whole range. Does this guarantee that `feat_A` will have a higher permutation importance than `feat_B`. Why or why not?</p>
</blockquote>
<p>It doesn't - the partial dependence plot shows how the predictions change based on the inputs, but it isn't the same thing as the feature importance - it might be the case that a few inputs create a large difference but most points don't, in which case the feature importance won't be very large.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/tutorials/partial-dependence-plots/">Partial Dependence Plots</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/tutorials/partial-dependence-plots/" rel="bookmark"><time class="published dt-published" datetime="2020-02-08T12:48:50-08:00" itemprop="datePublished" title="2020-02-08 12:48">2020-02-08 12:48</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/tutorials/partial-dependence-plots/#orgc9ff046">Beginning</a>
<ul>
<li><a href="/posts/tutorials/partial-dependence-plots/#orgbf55cdd">What is this about?</a></li>
<li><a href="/posts/tutorials/partial-dependence-plots/#org39ad250">How does it work?</a></li>
<li><a href="/posts/tutorials/partial-dependence-plots/#org45cbc22">Imports</a>
<ul>
<li><a href="/posts/tutorials/partial-dependence-plots/#org1243b13">Python</a></li>
<li><a href="/posts/tutorials/partial-dependence-plots/#orgb037c53">PyPi</a></li>
</ul>
</li>
<li><a href="/posts/tutorials/partial-dependence-plots/#org01fd838">Set Up</a>
<ul>
<li><a href="/posts/tutorials/partial-dependence-plots/#org248cd26">Plotting</a></li>
<li><a href="/posts/tutorials/partial-dependence-plots/#orge42515e">The Data</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/tutorials/partial-dependence-plots/#orgd88870e">Middle</a>
<ul>
<li><a href="/posts/tutorials/partial-dependence-plots/#orgb2101df">A Decision Tree Model</a>
<ul>
<li><a href="/posts/tutorials/partial-dependence-plots/#org5eb50fd">Visualizing the Decision Tree</a></li>
<li><a href="/posts/tutorials/partial-dependence-plots/#org18d972e">Partial Dependency Plot</a></li>
<li><a href="/posts/tutorials/partial-dependence-plots/#org47522f7">Distance Covered</a></li>
</ul>
</li>
<li><a href="/posts/tutorials/partial-dependence-plots/#orga562fef">A Random Forest Model</a></li>
<li><a href="/posts/tutorials/partial-dependence-plots/#org15aefcd">2D Partial Dependence Plots</a>
<ul>
<li><a href="/posts/tutorials/partial-dependence-plots/#orgaf48c0f">Forest From the Trees</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/tutorials/partial-dependence-plots/#org10d9dd0">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgc9ff046">
<h2 id="orgc9ff046">Beginning</h2>
<div class="outline-text-2" id="text-orgc9ff046"></div>
<div class="outline-3" id="outline-container-orgbf55cdd">
<h3 id="orgbf55cdd">What is this about?</h3>
<div class="outline-text-3" id="text-orgbf55cdd">
<p>These are my notes/re-write of the <a href="https://www.kaggle.com/dansbecker/partial-plots">Partial Dependence Plots</a> tutorial on kaggle. Partial Dependence Plots are a complement to Permutation Importance in that Permutation Importance can tell you which features are contributing to the model but don't tell you how features change with different inputs. Given that they have the word "Plot" in their name you can guess that this is a visualization method and since humans have a hard time visualizing things with more than three dimensions, using them requires selecting a subset of features (or maybe just one feature) to visualize at a time, so the fact that Permutation Importance ranks features by their contribution might come in handy in deciding which features to use.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org39ad250">
<h3 id="org39ad250">How does it work?</h3>
<div class="outline-text-3" id="text-org39ad250">
<p>In a simplistic view we might say that it takes input data and then repeatedly alters the values in our feature of choice, freezing the other values so that we can see how varying the feature changes the prediction. You can then repeat this for multiple rows of data and take the average prediction for each value of our feature.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org45cbc22">
<h3 id="org45cbc22">Imports</h3>
<div class="outline-text-3" id="text-org45cbc22"></div>
<div class="outline-4" id="outline-container-org1243b13">
<h4 id="org1243b13">Python</h4>
<div class="outline-text-4" id="text-org1243b13">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">os</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb037c53">
<h4 id="orgb037c53">PyPi</h4>
<div class="outline-text-4" id="text-orgb037c53">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span>
<span class="kn">from</span> <span class="nn">pdpbox</span> <span class="kn">import</span> <span class="n">pdp</span><span class="p">,</span> <span class="n">get_dataset</span><span class="p">,</span> <span class="n">info_plots</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>

<span class="kn">import</span> <span class="nn">graphviz</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org01fd838">
<h3 id="org01fd838">Set Up</h3>
<div class="outline-text-3" id="text-org01fd838"></div>
<div class="outline-4" id="outline-container-org248cd26">
<h4 id="org248cd26">Plotting</h4>
<div class="outline-text-4" id="text-org248cd26">
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"partial-dependence-plots"</span>
<span class="n">OUTPUT_PATH</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"../../files/posts/tutorials/"</span><span class="p">)</span><span class="o">/</span><span class="n">SLUG</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge42515e">
<h4 id="orge42515e">The Data</h4>
<div class="outline-text-4" id="text-orge42515e">
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/.env"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">load_dotenv</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"FIFA-2018"</span><span class="p">))</span><span class="o">.</span><span class="n">expanduser</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgd88870e">
<h2 id="orgd88870e">Middle</h2>
<div class="outline-text-2" id="text-orgd88870e"></div>
<div class="outline-3" id="outline-container-orgb2101df">
<h3 id="orgb2101df">A Decision Tree Model</h3>
<div class="outline-text-3" id="text-orgb2101df">
<p>The data is the same set from the Permutation Importance exercise (team data to predict whether one of them would become the <i>Budweiser Man of the Match</i>). Out initial model will be a shallow decision tree so that we can compare its structure to the plots.</p>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"Man of the Match"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"Yes"</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">numpy</span><span class="o">.</span><span class="n">int64</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_validate</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_validate</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">tree_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org5eb50fd">
<h4 id="org5eb50fd">Visualizing the Decision Tree</h4>
<div class="outline-text-4" id="text-org5eb50fd">
<div class="highlight">
<pre><span></span><span class="n">tree_graph</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">tree_graph</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="s2">"decision_tree.dot"</span>
<span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="o">/</span><span class="n">output</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s2">"png"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"[[file:{output}.png]]"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="decision_tree.dot.png" src="/posts/tutorials/partial-dependence-plots/decision_tree.dot.png"></p>
</div>
</div>
</div>
<div class="outline-4" id="outline-container-org18d972e">
<h4 id="org18d972e">Partial Dependency Plot</h4>
<div class="outline-text-4" id="text-org18d972e">
<p>To create the plot we can use the <a href="https://pdpbox.readthedocs.io/en/latest/">PDPBox library</a>.</p>
</div>
<ul class="org-ul">
<li><a id="orgc7ef633"></a>Create the Data to Plot<br>
<div class="outline-text-5" id="text-orgc7ef633">
<div class="highlight">
<pre><span></span><span class="n">FEATURE</span> <span class="o">=</span> <span class="s2">"Goal Scored"</span>
<span class="n">pdp_goals</span> <span class="o">=</span> <span class="n">pdp</span><span class="o">.</span><span class="n">pdp_isolate</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">x_validate</span><span class="p">,</span>
                            <span class="n">model_features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="n">FEATURE</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="org05ea109"></a>Plot It<br>
<div class="outline-text-5" id="text-org05ea109">
<div class="highlight">
<pre><span></span><span class="n">pdp</span><span class="o">.</span><span class="n">pdp_plot</span><span class="p">(</span><span class="n">pdp_goals</span><span class="p">,</span> <span class="n">FEATURE</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="s2">"pdp_goals_scored.png"</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="o">/</span><span class="n">output</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"[[file:{output}]]"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="pdp_goals_scored.png" src="/posts/tutorials/partial-dependence-plots/pdp_goals_scored.png"></p>
</div>
<p>Looking at the plot we can interpret it as saying that scoring that first goal is helpful in getting someone on the team the Man of the Match, but after that, more goals doesn't help. The PDPBox documentation has no real information about interpreting the output (or anything they provide, really), but according to the Kaggle tutorial the y-axis is the change in the prediction from the baseline as x changes.</p>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org47522f7">
<h4 id="org47522f7">Distance Covered</h4>
<div class="outline-text-4" id="text-org47522f7">
<p>We can also look at how much the distance the players cover on the field matters.</p>
<div class="highlight">
<pre><span></span><span class="n">FEATURE</span> <span class="o">=</span> <span class="s2">"Distance Covered (Kms)"</span>
<span class="n">pdp_distance</span> <span class="o">=</span> <span class="n">pdp</span><span class="o">.</span><span class="n">pdp_isolate</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">x_validate</span><span class="p">,</span>
                               <span class="n">model_features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="n">FEATURE</span><span class="p">)</span>
<span class="n">pdp</span><span class="o">.</span><span class="n">pdp_plot</span><span class="p">(</span><span class="n">pdp_distance</span><span class="p">,</span> <span class="n">FEATURE</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="s2">"pdp_distance.png"</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="o">/</span><span class="n">output</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"[[file:{output}]]"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="pdp_distance.png" src="/posts/tutorials/partial-dependence-plots/pdp_distance.png"></p>
</div>
<p>It looks like there's one distance at which the probabilities increase and then going further doesn't matter.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">pdp_distance</span><span class="o">.</span><span class="n">feature_grids</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
<pre class="example">
102.0
</pre>
<p>So you want your team to cover at least 102 Kilometers, but covering more won't help you.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orga562fef">
<h3 id="orga562fef">A Random Forest Model</h3>
<div class="outline-text-3" id="text-orga562fef">
<p>The Decision Tree was useful because the simplicity made it easy to interpret, but an ensemble method like a Random Forest probably makes a better model so let's see what it reveals.</p>
<div class="highlight">
<pre><span></span><span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">FEATURE</span> <span class="o">=</span> <span class="s2">"Goal Scored"</span>
<span class="n">pdp_goals</span> <span class="o">=</span> <span class="n">pdp</span><span class="o">.</span><span class="n">pdp_isolate</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">forest</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">x_validate</span><span class="p">,</span>
                            <span class="n">model_features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="n">FEATURE</span><span class="p">)</span>
<span class="n">pdp</span><span class="o">.</span><span class="n">pdp_plot</span><span class="p">(</span><span class="n">pdp_goals</span><span class="p">,</span> <span class="n">FEATURE</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="s2">"pdp_forest_goals_scored.png"</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="o">/</span><span class="n">output</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"[[file:{output}]]"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="pdp_forest_goals_scored.png" src="/posts/tutorials/partial-dependence-plots/pdp_forest_goals_scored.png"></p>
</div>
<p>So, our random forest says that the greatest gain comes from the first goal, but there is in fact a greater probability as you increase the number of goals scored.</p>
<p>What about distance covered?</p>
<div class="highlight">
<pre><span></span><span class="n">FEATURE</span> <span class="o">=</span> <span class="s2">"Distance Covered (Kms)"</span>
<span class="n">pdp_distance</span> <span class="o">=</span> <span class="n">pdp</span><span class="o">.</span><span class="n">pdp_isolate</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">forest</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">x_validate</span><span class="p">,</span>
                               <span class="n">model_features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="n">FEATURE</span><span class="p">)</span>
<span class="n">pdp</span><span class="o">.</span><span class="n">pdp_plot</span><span class="p">(</span><span class="n">pdp_distance</span><span class="p">,</span> <span class="n">FEATURE</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="s2">"pdp_forest_distance.png"</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="o">/</span><span class="n">output</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"[[file:{output}]]"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="pdp_forest_distance.png" src="/posts/tutorials/partial-dependence-plots/pdp_forest_distance.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">pdp_distance</span><span class="o">.</span><span class="n">feature_grids</span><span class="p">[</span><span class="n">pdp_distance</span><span class="o">.</span><span class="n">pdp</span><span class="o">.</span><span class="n">argmax</span><span class="p">()])</span>
</pre></div>
<pre class="example">
102.0
</pre>
<p>Our forest says that, once again, covering 102 kilometers maximizes the probability, but in this case it appears that going beyond that actually decreases the probability that your team would have the man of the match.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org15aefcd">
<h3 id="org15aefcd">2D Partial Dependence Plots</h3>
<div class="outline-text-3" id="text-org15aefcd">
<p>Rather than plot a single feature against the probability of becoming the man of the match you can plot how two features interact to affect the probability.</p>
<div class="highlight">
<pre><span></span><span class="n">FEATURES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Goal Scored"</span><span class="p">,</span> <span class="s2">"Distance Covered (Kms)"</span><span class="p">]</span>
<span class="n">interaction</span> <span class="o">=</span> <span class="n">pdp</span><span class="o">.</span><span class="n">pdp_interact</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">x_validate</span><span class="p">,</span>
                               <span class="n">model_features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
                               <span class="n">features</span><span class="o">=</span><span class="n">FEATURES</span><span class="p">)</span>

<span class="n">pdp</span><span class="o">.</span><span class="n">pdp_interact_plot</span><span class="p">(</span><span class="n">pdp_interact_out</span><span class="o">=</span><span class="n">interaction</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">FEATURES</span><span class="p">,</span>
                      <span class="n">plot_type</span><span class="o">=</span><span class="s2">"contour"</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="s2">"goals_vs_distance.png"</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="o">/</span><span class="n">output</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"[[file:{output}]]"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="goals_vs_distance.png" src="/posts/tutorials/partial-dependence-plots/goals_vs_distance.png"></p>
</div>
<p><b>Note:</b> The version of PDPBox on pypi currently has a bug (as of February 8, 2020) that won't let you create the previous plot, install it from github instead.</p>
<p>The plot shows a more succinct version of the two plots we had seen before - the optimal point for these two features is 1 goal and 102 Kms covered.</p>
</div>
<div class="outline-4" id="outline-container-orgaf48c0f">
<h4 id="orgaf48c0f">Forest From the Trees</h4>
<div class="outline-text-4" id="text-orgaf48c0f">
<p>Let's re-run the same plot using the Random Forest instead of the Decision Tree.</p>
<div class="highlight">
<pre><span></span><span class="n">FEATURES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Goal Scored"</span><span class="p">,</span> <span class="s2">"Distance Covered (Kms)"</span><span class="p">]</span>
<span class="n">interaction</span> <span class="o">=</span> <span class="n">pdp</span><span class="o">.</span><span class="n">pdp_interact</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">forest</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">x_validate</span><span class="p">,</span>
                               <span class="n">model_features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
                               <span class="n">features</span><span class="o">=</span><span class="n">FEATURES</span><span class="p">)</span>

<span class="n">pdp</span><span class="o">.</span><span class="n">pdp_interact_plot</span><span class="p">(</span><span class="n">pdp_interact_out</span><span class="o">=</span><span class="n">interaction</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">FEATURES</span><span class="p">,</span>
                      <span class="n">plot_type</span><span class="o">=</span><span class="s2">"contour"</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="s2">"forest_goals_vs_distance.png"</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">OUTPUT_PATH</span><span class="o">/</span><span class="n">output</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"[[file:{output}]]"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="forest_goals_vs_distance.png" src="/posts/tutorials/partial-dependence-plots/forest_goals_vs_distance.png"></p>
</div>
<p>The random forest suggests a slightly different scenario - here you want 2 gooalds and between 100 and 110 Kms (or thereabouts) covered to get the best probability.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org10d9dd0">
<h2 id="org10d9dd0">End</h2>
<div class="outline-text-2" id="text-org10d9dd0">
<p>This showed how to use the PDPBox library to create Partial Dependence Plots to look at how input values for features affects the predicted outcomes. Unfortunately it looks like PDPBox might have been abandoned, but while it works it's a nice library.</p>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="/" rel="prev">Newer posts</a></li>
<li class="next"><a href="/index-5.html" rel="next">Older posts</a></li>
</ul>
<!--End of body content-->
<footer id="footer">Contents Â© 2020 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
