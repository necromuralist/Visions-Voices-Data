<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Visions, Voices, Data (Posts about visualization)</title><link>https://necromuralist.github.io/Visions-Voices-Data/</link><description></description><atom:link href="https://necromuralist.github.io/Visions-Voices-Data/categories/visualization.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2020 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Sun, 09 Feb 2020 03:44:11 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Partial Dependence Plots</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org72f4085"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgd507ca4"&gt;What is this about?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgc56e34e"&gt;How does it work?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orga29216c"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgdaa83ab"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgee1fc39"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgd8ab282"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgdc91614"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orga7d1197"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orga658183"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org5464cde"&gt;A Decision Tree Model&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org64a227c"&gt;Visualizing the Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgde6b241"&gt;Partial Dependency Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org820fb2a"&gt;Distance Covered&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org7b0c022"&gt;A Random Forest Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgcb5456e"&gt;2D Partial Dependence Plots&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org544244d"&gt;Forest From the Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org36d85b7"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org72f4085" class="outline-2"&gt;
&lt;h2 id="org72f4085"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org72f4085"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd507ca4" class="outline-3"&gt;
&lt;h3 id="orgd507ca4"&gt;What is this about?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd507ca4"&gt;
&lt;p&gt;
These are my notes/re-write of the &lt;a href="https://www.kaggle.com/dansbecker/partial-plots"&gt;Partial Dependence Plots&lt;/a&gt; tutorial on kaggle. Partial Dependence Plots are a complement to Permutation Importance in that Permutation Importance can tell you which features are contributing to the model but don't tell you how features change with different inputs. Given that they have the word "Plot" in their name you can guess that this is a visualization method and since humans have a hard time visualizing things with more than three dimensions, using them requires selecting a subset of features (or maybe just one feature) to visualize at a time, so the fact that Permutation Importance ranks features by their contribution might come in handy in deciding which features to use.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc56e34e" class="outline-3"&gt;
&lt;h3 id="orgc56e34e"&gt;How does it work?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc56e34e"&gt;
&lt;p&gt;
In a simplistic view we might say that it takes input data and then repeatedly alters the values in our feature of choice, freezing the other values so that we can see how varying the feature changes the prediction. You can then repeat this for multiple rows of data and take the average prediction for each value of our feature.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga29216c" class="outline-3"&gt;
&lt;h3 id="orga29216c"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga29216c"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdaa83ab" class="outline-4"&gt;
&lt;h4 id="orgdaa83ab"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgdaa83ab"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from pathlib import Path

import os
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgee1fc39" class="outline-4"&gt;
&lt;h4 id="orgee1fc39"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgee1fc39"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from matplotlib import pyplot
from pdpbox import pdp, get_dataset, info_plots
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

import graphviz
import numpy
import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd8ab282" class="outline-3"&gt;
&lt;h3 id="orgd8ab282"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd8ab282"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdc91614" class="outline-4"&gt;
&lt;h4 id="orgdc91614"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgdc91614"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SLUG = "partial-dependence-plots"
OUTPUT_PATH = Path("../../files/posts/tutorials/")/SLUG
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga7d1197" class="outline-4"&gt;
&lt;h4 id="orga7d1197"&gt;The Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga7d1197"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = Path("~/.env").expanduser()
load_dotenv(path, override=True)
data = pandas.read_csv(Path(os.getenv("FIFA-2018")).expanduser())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga658183" class="outline-2"&gt;
&lt;h2 id="orga658183"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga658183"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5464cde" class="outline-3"&gt;
&lt;h3 id="org5464cde"&gt;A Decision Tree Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5464cde"&gt;
&lt;p&gt;
The data is the same set from the Permutation Importance exercise (team data to predict whether one of them would become the &lt;i&gt;Budweiser Man of the Match&lt;/i&gt;). Out initial model will be a shallow decision tree so that we can compare its structure to the plots.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;y = data["Man of the Match"] == "Yes"
features = [column for column in data.columns if data[column].dtype == numpy.int64]
X = data[features]

x_train, x_validate, y_train, y_validate = train_test_split(X, y, random_state=1)

tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(x_train, y_train)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org64a227c" class="outline-4"&gt;
&lt;h4 id="org64a227c"&gt;Visualizing the Decision Tree&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org64a227c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=features)
graph = graphviz.Source(tree_graph)
output = "decision_tree.dot"
graph.render(OUTPUT_PATH/output, format="png")
print(f"[[file:{output}.png]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/decision_tree.dot.png" alt="decision_tree.dot.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgde6b241" class="outline-4"&gt;
&lt;h4 id="orgde6b241"&gt;Partial Dependency Plot&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgde6b241"&gt;
&lt;p&gt;
To create the plot we can use the &lt;a href="https://pdpbox.readthedocs.io/en/latest/"&gt;PDPBox library&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org6fef5c6"&gt;&lt;/a&gt;Create the Data to Plot&lt;br&gt;
&lt;div class="outline-text-5" id="text-org6fef5c6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURE = "Goal Scored"
pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=x_validate,
			    model_features=features, feature=FEATURE)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;

&lt;li&gt;&lt;a id="orgce6f5d8"&gt;&lt;/a&gt;Plot It&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgce6f5d8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pdp.pdp_plot(pdp_goals, FEATURE)
output = "pdp_goals_scored.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_goals_scored.png" alt="pdp_goals_scored.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;
Looking at the plot we can interpret it as saying that scoring that first goal is helpful in getting someone on the team the Man of the Match, but after that, more goals doesn't help. The PDPBox documentation has no real information about interpreting the output (or anything they provide, really), but according to the Kaggle tutorial the y-axis is the change in the prediction from the baseline as x changes.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-org820fb2a" class="outline-4"&gt;
&lt;h4 id="org820fb2a"&gt;Distance Covered&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org820fb2a"&gt;
&lt;p&gt;
We can also look at how much the distance the players cover on the field matters.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURE = "Distance Covered (Kms)"
pdp_distance = pdp.pdp_isolate(model=tree_model, dataset=x_validate,
			       model_features=features, feature=FEATURE)
pdp.pdp_plot(pdp_distance, FEATURE)
output = "pdp_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_distance.png" alt="pdp_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
It looks like there's one distance at which the probabilities increase and then going further doesn't matter.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(pdp_distance.feature_grids[3])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
102.0
&lt;/pre&gt;


&lt;p&gt;
So you want your team to cover at least 102 Kilometers, but covering more won't help you.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7b0c022" class="outline-3"&gt;
&lt;h3 id="org7b0c022"&gt;A Random Forest Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7b0c022"&gt;
&lt;p&gt;
The Decision Tree was useful because the simplicity made it easy to interpret, but an ensemble method like a Random Forest probably makes a better model so let's see what it reveals.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;forest = RandomForestClassifier(random_state=0).fit(x_train, y_train)

FEATURE = "Goal Scored"
pdp_goals = pdp.pdp_isolate(model=forest, dataset=x_validate,
			    model_features=features, feature=FEATURE)
pdp.pdp_plot(pdp_goals, FEATURE)
output = "pdp_forest_goals_scored.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_forest_goals_scored.png" alt="pdp_forest_goals_scored.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So, our random forest says that the greatest gain comes from the first goal, but there is in fact a greater probability as you increase the number of goals scored.
&lt;/p&gt;

&lt;p&gt;
What about distance covered?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURE = "Distance Covered (Kms)"
pdp_distance = pdp.pdp_isolate(model=forest, dataset=x_validate,
			       model_features=features, feature=FEATURE)
pdp.pdp_plot(pdp_distance, FEATURE)
output = "pdp_forest_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_forest_distance.png" alt="pdp_forest_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(pdp_distance.feature_grids[pdp_distance.pdp.argmax()])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
102.0
&lt;/pre&gt;


&lt;p&gt;
Our forest says that, once again, covering 102 kilometers maximizes the probability, but in this case it appears that going beyond that actually decreases the probability that your team would have the man of the match.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcb5456e" class="outline-3"&gt;
&lt;h3 id="orgcb5456e"&gt;2D Partial Dependence Plots&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcb5456e"&gt;
&lt;p&gt;
Rather than plot a single feature against the probability of becoming the man of the match you can plot how two features interact to affect the probability.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURES = ["Goal Scored", "Distance Covered (Kms)"]
interaction = pdp.pdp_interact(model=tree_model, dataset=x_validate,
			       model_features=features,
			       features=FEATURES)

pdp.pdp_interact_plot(pdp_interact_out=interaction, feature_names=FEATURES,
		      plot_type="contour")
output = "goals_vs_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/goals_vs_distance.png" alt="goals_vs_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;b&gt;Note:&lt;/b&gt; The version of PDPBox on pypi currently has a bug (as of February 8, 2020) that won't let you create the previous plot, install it from github instead.
&lt;/p&gt;

&lt;p&gt;
The plot shows a more succinct version of the two plots we had seen before - the optimal point for these two features is 1 goal and 102 Kms covered.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org544244d" class="outline-4"&gt;
&lt;h4 id="org544244d"&gt;Forest From the Trees&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org544244d"&gt;
&lt;p&gt;
Let's re-run the same plot using the Random Forest instead of the Decision Tree.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURES = ["Goal Scored", "Distance Covered (Kms)"]
interaction = pdp.pdp_interact(model=forest, dataset=x_validate,
			       model_features=features,
			       features=FEATURES)

pdp.pdp_interact_plot(pdp_interact_out=interaction, feature_names=FEATURES,
		      plot_type="contour")
output = "forest_goals_vs_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/forest_goals_vs_distance.png" alt="forest_goals_vs_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The random forest suggests a slightly different scenario - here you want 2 gooalds and between 100 and 110 Kms (or thereabouts) covered to get the best probability.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org36d85b7" class="outline-2"&gt;
&lt;h2 id="org36d85b7"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org36d85b7"&gt;
&lt;p&gt;
This showed how to use the PDPBox library to create Partial Dependence Plots to look at how input values for features affects the predicted outcomes. Unfortunately it looks like PDPBox might have been abandoned, but while it works it's a nice library.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>interpret</category><category>machine learning</category><category>tutorial</category><category>visualization</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/</guid><pubDate>Sat, 08 Feb 2020 20:48:50 GMT</pubDate></item><item><title>Trying out DABEST</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#org27a196f"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#org9d3e203"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#org07e536a"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#orgabbb33d"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#orga8af6cb"&gt;Petal Width&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#orgbcd0497"&gt;Hedge's G&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#org91c1a78"&gt;Cohen's D&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org27a196f" class="outline-2"&gt;
&lt;h2 id="org27a196f"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org27a196f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9d3e203" class="outline-3"&gt;
&lt;h3 id="org9d3e203"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9d3e203"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6c3bc52" class="outline-4"&gt;
&lt;h4 id="org6c3bc52"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6c3bc52"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from sklearn.datasets import load_iris
import dabest
import pandas
import seaborn
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org07e536a" class="outline-3"&gt;
&lt;h3 id="org07e536a"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org07e536a"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7515a90" class="outline-4"&gt;
&lt;h4 id="org7515a90"&gt;The Data Set&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7515a90"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;iris = load_iris()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(iris.DESCR)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
                
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher's paper. Note that it's the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher's paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp;amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

.. topic:: References

   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"
     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
     Mathematical Statistics" (John Wiley, NY, 1950).
   - Duda, R.O., &amp;amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp;amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments".  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.DataFrame(iris.data, columns=iris.feature_names)
target = pandas.Series(iris.target)
names = dict(zip(range(len(iris.target_names)), iris.target_names))
data["species"] = target.map(names)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgabbb33d" class="outline-2"&gt;
&lt;h2 id="orgabbb33d"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgabbb33d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga8af6cb" class="outline-3"&gt;
&lt;h3 id="orga8af6cb"&gt;Petal Width&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga8af6cb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;iris_dabest = dabest.load(data=data, x="species", y="petal width (cm)", idx=iris.target_names)
iris_dabest.mean_diff.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/petal_width.png" alt="petal_width.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbcd0497" class="outline-3"&gt;
&lt;h3 id="orgbcd0497"&gt;Hedge's G&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbcd0497"&gt;
&lt;p&gt;
&lt;a href="https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/hedgeg.htm"&gt;Hedges G&lt;/a&gt; is a measurement of effect size, similar to Cohen's d but with better properties when the samples are smaller or the sample sizes are different.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;iris_dabest.hedges_g.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/hedges_g.png" alt="hedges_g.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(iris_dabest.hedges_g)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
DABEST v0.2.7
=============
             
Good afternoon!
The current time is Mon Dec 16 15:56:24 2019.

The unpaired Hedges' g between setosa and versicolor is 6.76 [95%CI 5.71, 7.86].
The two-sided p-value of the Mann-Whitney test is 2.28e-18.

The unpaired Hedges' g between setosa and virginica is 8.49 [95%CI 7.08, 9.77].
The two-sided p-value of the Mann-Whitney test is 2.43e-18.

5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.
The p-value(s) reported are the likelihood(s) of observing the effect size(s),
if the null hypothesis of zero difference is true.

To get the results of all valid statistical tests, use `.hedges_g.statistical_tests`
&lt;/pre&gt;

&lt;p&gt;
According to &lt;a href="https://www.wikiwand.com/en/Effect_size"&gt;Wikipedia&lt;/a&gt;, an effect size of 2 is "huge" so since the differences between the setosa and versicolor and setosa and virginica are 6.76 and 8.49 respectively, we might conclude that there is a significant difference between the petal width of the setosa and the other two species.
&lt;/p&gt;

&lt;p&gt;
I don't think that's really what this is meant for, but I just wanted to see how it works.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org91c1a78" class="outline-3"&gt;
&lt;h3 id="org91c1a78"&gt;Cohen's D&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org91c1a78"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;iris_dabest.cohens_d.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/cohens_d.png" alt="cohens_d.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(iris_dabest.cohens_d)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
DABEST v0.2.7
=============
             
Good afternoon!
The current time is Mon Dec 16 16:46:25 2019.

The unpaired Cohen's d between setosa and versicolor is 6.82 [95%CI 5.76, 7.92].
The two-sided p-value of the Mann-Whitney test is 2.28e-18.

The unpaired Cohen's d between setosa and virginica is 8.56 [95%CI 7.13, 9.85].
The two-sided p-value of the Mann-Whitney test is 2.43e-18.

5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.
The p-value(s) reported are the likelihood(s) of observing the effect size(s),
if the null hypothesis of zero difference is true.

To get the results of all valid statistical tests, use `.cohens_d.statistical_tests`
&lt;/pre&gt;

&lt;p&gt;
In this case the Cohen's d and Hedges g look similar.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>exploration</category><category>statistics</category><category>visualization</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/</guid><pubDate>Mon, 16 Dec 2019 21:50:24 GMT</pubDate></item><item><title>HoloViews Introduction</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orgffe922c"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orgabb996c"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orge49c89e"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org684f94e"&gt;This Project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orga1b7724"&gt;The HoloViews Backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orgc33b03c"&gt;A Partial Bokeh Embedder&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orga4f5a93"&gt;The Data Set&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orga0e934b"&gt;Load It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org587e328"&gt;Make The DataFrame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org182253c"&gt;A Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org9a73ccf"&gt;Adding To the Layout&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgffe922c" class="outline-2"&gt;
&lt;h2 id="orgffe922c"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgffe922c"&gt;
&lt;p&gt;
I've already taken an initial look at HVPlot so I'm going to look at &lt;a href="http://holoviews.org/"&gt;HoloViews&lt;/a&gt; which acts as an intermediate layer between the main plotting libraries like bokeh and matplotlib and the upper layer given by HVPlot. I haven't used it before so I'm not really sure when you would use what. I guess &lt;code&gt;HVPlot&lt;/code&gt; gives you access to the &lt;code&gt;pandas&lt;/code&gt; plots in bokeh without a lot of work, which is nice, although I noticed that the plots tended to be missing things sometimes (like the Hover tool) so if you want to add more back in you probably have to understand &lt;code&gt;HoloViews&lt;/code&gt; which itself sometimes doesn't give you what you want (like the ability to render it in org-mode posts) so you still need bokeh too, sometimes. And of course I'm only using the static-page versions of everything, not the features that work with a bokeh or jupyter server. But I guess that's for later.
&lt;/p&gt;

&lt;p&gt;
I'm going to be working from the &lt;a href="http://holoviews.org/getting_started/Introduction.html"&gt;Introduction&lt;/a&gt; of their Getting Started guide.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgabb996c" class="outline-2"&gt;
&lt;h2 id="orgabb996c"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgabb996c"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge49c89e" class="outline-3"&gt;
&lt;h3 id="orge49c89e"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge49c89e"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org57a8f01" class="outline-4"&gt;
&lt;h4 id="org57a8f01"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org57a8f01"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdd59e7b" class="outline-4"&gt;
&lt;h4 id="orgdd59e7b"&gt;From PiPy&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgdd59e7b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;holoviews&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;opts&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fetch_california_housing&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;holoviews&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org684f94e" class="outline-3"&gt;
&lt;h3 id="org684f94e"&gt;This Project&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org684f94e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bartleby_the_penguin.tangles.embed_bokeh&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EmbedBokeh&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga1b7724" class="outline-3"&gt;
&lt;h3 id="orga1b7724"&gt;The HoloViews Backend&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga1b7724"&gt;
&lt;p&gt;
If you use &lt;code&gt;HVPlot&lt;/code&gt; you don't need to set the backend (because it defaults to 'bokeh', I think) but this is going to be about &lt;code&gt;HoloViews&lt;/code&gt; so I'm going to do it their way, rather than relying on all the &lt;code&gt;pandas&lt;/code&gt; methods.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extension&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"bokeh"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc33b03c" class="outline-3"&gt;
&lt;h3 id="orgc33b03c"&gt;A Partial Bokeh Embedder&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc33b03c"&gt;
&lt;p&gt;
Since the output folder is always the same I'm going to bind it to the EmbedBokeh definition.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"../../files/posts/libraries/holoviews-introduction/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EmbedBokeh&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;folder_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plot_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga4f5a93" class="outline-2"&gt;
&lt;h2 id="orga4f5a93"&gt;The Data Set&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga4f5a93"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga0e934b" class="outline-3"&gt;
&lt;h3 id="orga0e934b"&gt;Load It&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga0e934b"&gt;
&lt;p&gt;
Sklearn downloads it as a 'bunch' so we need to get it in that form first and then turn it into a data frame (I'm sure there's a way to skip this step but this is the way I already know how to do it).
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"~/data/datasets/california-housing"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_dir&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;bunch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_california_housing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bunch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DESCR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/data/datasets/california-housing
.. _california_housing_dataset:

California Housing dataset
--------------------------

**Data Set Characteristics:**

    :Number of Instances: 20640

    :Number of Attributes: 8 numeric, predictive attributes and the target

    :Attribute Information:
        - MedInc        median income in block
        - HouseAge      median house age in block
        - AveRooms      average number of rooms
        - AveBedrms     average number of bedrooms
        - Population    block population
        - AveOccup      average house occupancy
        - Latitude      house block latitude
        - Longitude     house block longitude

    :Missing Attribute Values: None

This dataset was obtained from the StatLib repository.
http://lib.stat.cmu.edu/datasets/

The target variable is the median house value for California districts.

This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

It can be downloaded/loaded using the
:func:`sklearn.datasets.fetch_california_housing` function.

.. topic:: References

    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
      Statistics and Probability Letters, 33 (1997) 291-297
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org587e328" class="outline-3"&gt;
&lt;h3 id="org587e328"&gt;Make The DataFrame&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org587e328"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bunch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bunch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"median_value"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bunch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   
1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   
2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   
3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   
4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   

   Longitude  median_value  
0    -122.23         4.526  
1    -122.22         3.585  
2    -122.24         3.521  
3    -122.25         3.413  
4    -122.25         3.422  
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org182253c" class="outline-2"&gt;
&lt;h2 id="org182253c"&gt;A Plot&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org182253c"&gt;
&lt;p&gt;
Our target is the median value of the house. Does that correlate with median income?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"MedInc"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Median Income"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
			    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"median_value"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Median Value"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
			    &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"California Housing"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
After setting up the basic plot we can do things to affect the appearance like setting the color or adding tools.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"red"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"hover"&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;script src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/income_vs_value.js" id="bfbe3866-7981-492f-b2f8-918396abac02"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9a73ccf" class="outline-2"&gt;
&lt;h2 id="org9a73ccf"&gt;Adding To the Layout&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9a73ccf"&gt;
&lt;p&gt;
What if we want to add a distrbution to the plot? HoloViews uses the &lt;code&gt;+&lt;/code&gt; operator to indicate that you want to append a plot to another one.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;layout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scatter&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Histogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;histogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HouseAge&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kdims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"HouseAge"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;layout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layout&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Histogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"hover"&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;script src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/income_vs_value_with_house_age.js" id="728203cb-7d9b-4ecd-aeb0-a30aec6a7655"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>exploration</category><category>holoviews</category><category>visualization</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/</guid><pubDate>Sat, 02 Feb 2019 22:15:01 GMT</pubDate></item></channel></rss>