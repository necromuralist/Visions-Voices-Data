<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Visions, Voices, Data (Posts about visualization)</title><link>https://necromuralist.github.io/Visions-Voices-Data/</link><description></description><atom:link href="https://necromuralist.github.io/Visions-Voices-Data/categories/visualization.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2020 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Tue, 11 Feb 2020 01:28:17 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>SHAP Values</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#orgdd0a13a"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#orgfd8b76e"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#orge2cb83e"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#org9db3e5d"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#org5d371be"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#org1da3ed2"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#org118f9db"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#orge7ada3e"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#orgff8da69"&gt;Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#orgc79a120"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#orgffab281"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#org5b8f736"&gt;A Single Row&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#org39b241d"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/#org0b0d423"&gt;See Also&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdd0a13a" class="outline-2"&gt;
&lt;h2 id="orgdd0a13a"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdd0a13a"&gt;
&lt;p&gt;
SHAP values interpret the impact of a certain value for a given feature when compared to the prediction you'd make if that feature instead took a baseline value. This helps us interpret predictions given specific values for our features. We'll do this using the &lt;a href="https://github.com/slundberg/shap"&gt;SHAP&lt;/a&gt; library, naturally.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfd8b76e" class="outline-3"&gt;
&lt;h3 id="orgfd8b76e"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfd8b76e"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge2cb83e" class="outline-4"&gt;
&lt;h4 id="orge2cb83e"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge2cb83e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from argparse import Namespace
from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9db3e5d" class="outline-4"&gt;
&lt;h4 id="org9db3e5d"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9db3e5d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from matplotlib import pyplot
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier

import numpy
import pandas
import seaborn
import shap
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5d371be" class="outline-4"&gt;
&lt;h4 id="org5d371be"&gt;Others&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5d371be"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import EnvironmentLoader, Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1da3ed2" class="outline-3"&gt;
&lt;h3 id="org1da3ed2"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1da3ed2"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org118f9db" class="outline-4"&gt;
&lt;h4 id="org118f9db"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org118f9db"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SLUG = "shap-values"
OUTPUT_PATH = Path("../../files/posts/tutorials/")/SLUG
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge7ada3e" class="outline-4"&gt;
&lt;h4 id="orge7ada3e"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge7ada3e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgff8da69" class="outline-4"&gt;
&lt;h4 id="orgff8da69"&gt;Environment&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgff8da69"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ENVIRONMENT = EnvironmentLoader()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc79a120" class="outline-4"&gt;
&lt;h4 id="orgc79a120"&gt;The Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc79a120"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.read_csv(Path(ENVIRONMENT["FIFA-2018"]).expanduser())
y = data["Man of the Match"] == "Yes"
FEATURES = [column for column in data.columns if data[column].dtype == numpy.int64]
X = data[FEATURES]
x_train, x_validate, y_train, y_validate = train_test_split(X, y, random_state=1)


model = RandomForestClassifier()

estimators = list(range(50, 200, 10))
max_depth = list(range(10, 100, 10)) + [None]

grid = dict(n_estimators=estimators,
	    max_depth=max_depth)
search = RandomizedSearchCV(estimator=model,
			    param_distributions=grid,
			    n_iter=40,
			    n_jobs=-1,
			    random_state=1)
with TIMER:
    search.fit(x_train, y_train)
first_model = search.best_estimator_
print(f"CV Training R^2: {search.best_score_:0.2f}")
print(f"Training R^2: {first_model.score(x_train, y_train): 0.2f}")
print(f"Validation R^2: {first_model.score(x_validate, y_validate):0.2f}")
print(search.best_params_)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2020-02-10 18:09:33,716 graeae.timers.timer start: Started: 2020-02-10 18:09:33.716565
The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.
2020-02-10 18:09:36,830 graeae.timers.timer end: Ended: 2020-02-10 18:09:36.830883
2020-02-10 18:09:36,832 graeae.timers.timer end: Elapsed: 0:00:03.114318
CV Training R^2: 0.70
Training R^2:  1.00
Validation R^2: 0.69
{'n_estimators': 100, 'max_depth': 30}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgffab281" class="outline-2"&gt;
&lt;h2 id="orgffab281"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgffab281"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5b8f736" class="outline-3"&gt;
&lt;h3 id="org5b8f736"&gt;A Single Row&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5b8f736"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ROW = 5
row_data = x_validate.iloc[ROW]
row_data_matrix = row_data.values.reshape(1, -1)
print(first_model.classes_)
print(first_model.predict_proba(row_data_matrix))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[False  True]
[[0.25 0.75]]
&lt;/pre&gt;


&lt;p&gt;
The &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba"&gt;&lt;code&gt;predict_proba&lt;/code&gt;&lt;/a&gt; method tells us the probability for the data for each class. So this team has a 70% chance that they do have a man of the match.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;explainer = shap.TreeExplainer(first_model)
shap_values = explainer.shap_values(row_data_matrix)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(explainer.shap_values.__doc__)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Estimate the SHAP values for a set of samples.

       Parameters
       ----------
       X : numpy.array, pandas.DataFrame or catboost.Pool (for catboost)
           A matrix of samples (# samples x # features) on which to explain the model's output.

       y : numpy.array
           An array of label values for each sample. Used when explaining loss functions.

       tree_limit : None (default) or int
           Limit the number of trees used by the model. By default None means no use the limit of the
           original model, and -1 means no limit.

       approximate : bool
           Run fast, but only roughly approximate the Tree SHAP values. This runs a method
           previously proposed by Saabas which only considers a single feature ordering. Take care
           since this does not have the consistency guarantees of Shapley values and places too
           much weight on lower splits in the tree.

       check_additivity : bool
           Run a validation check that the sum of the SHAP values equals the output of the model. This
           check takes only a small amount of time, and will catch potential unforeseen errors.
           Note that this check only runs right now when explaining the margin of the model.

       Returns
       -------
       For models with a single output this returns a matrix of SHAP values
       (# samples x # features). Each row sums to the difference between the model output for that
       sample and the expected value of the model output (which is stored in the expected_value
       attribute of the explainer when it is constant). For models with vector outputs this returns
       a list of such matrices, one for each output.

&lt;/pre&gt;

&lt;p&gt;
The array returned by the &lt;code&gt;shap_values&lt;/code&gt; method has two rows - one for each of our target classes. In this case we're asking if a team had a man of the match so we'll just look at the second array.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;shap.initjs()
figure = shap.force_plot(explainer.expected_value[1],
			 shap_values[1],
			 row_data_matrix,
			 feature_names=FEATURES,
			 matplotlib=True, show=False)
filename = "shap_one.png"

figure.savefig(OUTPUT_PATH/filename)
print(f"[[file:{filename}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/shap_one.png" alt="shap_one.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Our predicted probability that this team had a man of the match was 0.7, but the base-value for the set as a whole is 0.4979 (you can't see it in this plot for some reason), so our team is more likely to have one than most teams. The pink section of the plot shows the features that increased the probability and the part in blue shows the features that decreased the probability. The size of each feature's block is proportional to the amount the feature contributed, so the biggest block (&lt;i&gt;Goal Scored&lt;/i&gt;) contributed the most. The greatest negative feature was &lt;i&gt;Ball Possession %&lt;/i&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org39b241d" class="outline-2"&gt;
&lt;h2 id="org39b241d"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org39b241d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0b0d423" class="outline-3"&gt;
&lt;h3 id="org0b0d423"&gt;See Also&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0b0d423"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;The &lt;a href="https://shap.readthedocs.io/en/latest/"&gt;SHAP&lt;/a&gt; Documentation&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://github.com/slundberg/shap"&gt;SHAP github repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lundberg SM, Lee SI. A unified approach to interpreting model predictions. InAdvances in neural information processing systems 2017 (pp. 4765-4774). (&lt;a href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predicti"&gt;PDF Available Here&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>interpret</category><category>machine learning</category><category>tutorial</category><category>visualization</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/shap-values/</guid><pubDate>Mon, 10 Feb 2020 01:07:12 GMT</pubDate></item><item><title>Exercise In Partial Dependence Plots</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org116c1e6"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org6c9c970"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org1671a2c"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org5132b35"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#orgec8ed0d"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#orgdae510a"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#orgbcfa12b"&gt;The Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#orgbfcd106"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org01fb5ec"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org0feb87b"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org48a820f"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#orgd59439b"&gt;The First Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org5b9a1c3"&gt;Question 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#orgf53f3c3"&gt;Question 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org5ec7614"&gt;Question 3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#orgdc69d42"&gt;Question 4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/#org77524af"&gt;Question 5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org116c1e6" class="outline-2"&gt;
&lt;h2 id="org116c1e6"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org116c1e6"&gt;
&lt;p&gt;
This is my re-working of the Kaggle &lt;a href="https://www.kaggle.com/learn/machine-learning-explainability"&gt;Machine Learning Explainability&lt;/a&gt; exercise in Partial Dependece Plots. It uses data from the &lt;a href="https://www.kaggle.com/c/new-york-city-taxi-fare-prediction"&gt;Taxi Fare Prediction&lt;/a&gt; competition.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6c9c970" class="outline-3"&gt;
&lt;h3 id="org6c9c970"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6c9c970"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1671a2c" class="outline-4"&gt;
&lt;h4 id="org1671a2c"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1671a2c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5132b35" class="outline-4"&gt;
&lt;h4 id="org5132b35"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5132b35"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from matplotlib import pyplot
from matplotlib.pyplot import rcParams
from pdpbox import pdp
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, RandomizedSearchCV

import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgec8ed0d" class="outline-4"&gt;
&lt;h4 id="orgec8ed0d"&gt;Others&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgec8ed0d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import EnvironmentLoader, Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdae510a" class="outline-3"&gt;
&lt;h3 id="orgdae510a"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdae510a"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbcfa12b" class="outline-4"&gt;
&lt;h4 id="orgbcfa12b"&gt;The Environment&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgbcfa12b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ENVIRONMENT = EnvironmentLoader()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbfcd106" class="outline-4"&gt;
&lt;h4 id="orgbfcd106"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgbfcd106"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org01fb5ec" class="outline-4"&gt;
&lt;h4 id="org01fb5ec"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org01fb5ec"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SLUG = "exercise-in-partial-dependence-plots"
OUTPUT_PATH = Path("../../files/posts/tutorials/")/SLUG

rcParams["figure.figsize"] = (6, 4)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0feb87b" class="outline-4"&gt;
&lt;h4 id="org0feb87b"&gt;The Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0feb87b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ROWS = 5 * 10**4
data = pandas.read_csv(Path(ENVIRONMENT["NY-TAXI"]).expanduser()/"train.csv",
		       nrows=ROWS)
data = data.query('pickup_latitude &amp;gt; 40.7 and pickup_latitude &amp;lt; 40.8 and ' +
		  'dropoff_latitude &amp;gt; 40.7 and dropoff_latitude &amp;lt; 40.8 and ' +
		  'pickup_longitude &amp;gt; -74 and pickup_longitude &amp;lt; -73.9 and ' +
		  'dropoff_longitude &amp;gt; -74 and dropoff_longitude &amp;lt; -73.9 and ' +
		  'fare_amount &amp;gt; 0'
		  )
y = data.fare_amount
base_features = ['pickup_longitude',
		 'pickup_latitude',
		 'dropoff_longitude',
		 'dropoff_latitude']

X = data[base_features]

x_train, x_validate, y_train, y_validate = train_test_split(X, y, random_state=1)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org48a820f" class="outline-2"&gt;
&lt;h2 id="org48a820f"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org48a820f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd59439b" class="outline-3"&gt;
&lt;h3 id="orgd59439b"&gt;The First Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd59439b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;first_model = RandomForestRegressor(n_estimators=180,
				    max_depth=50, random_state=1).fit(x_train, y_train)
print(f"Training R^2: {first_model.score(x_train, y_train):0.2f}")
print(f"Validation R^2: {first_model.score(x_validate, y_validate):0.2f}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Training R^2: 0.92
Validation R^2: 0.43
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5b9a1c3" class="outline-3"&gt;
&lt;h3 id="org5b9a1c3"&gt;Question 1&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5b9a1c3"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURE = 'pickup_longitude'
pdp_dist = pdp.pdp_isolate(model=first_model,
			   dataset=x_validate,
			   model_features=base_features,
			   feature=FEATURE)
pdp.pdp_plot(pdp_dist, FEATURE)
output = f"{FEATURE}_pdp_plot.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[[file:pickup_longitude_pdp_plot.png]]
&lt;/pre&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png" alt="eb446225f180346680b332c924342792de7e5135.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/pickup_longitude_pdp_plot.png" alt="pickup_longitude_pdp_plot.png"&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png" alt="eb446225f180346680b332c924342792de7e5135.png"&gt;
&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;
Why does the partial dependence plot have this U-shape?
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;
At the extremes you have the locations that can potentially travel the furthest, creating the biggest fairs, but as you move to the center you reduce the amount you can possibly travel - although the change isn't symmetric so this isn't the only explanation, otherwise if it were then you would expect the two ends to have the highest values and the nadir to be at the halfway point (-73.95).
&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;
Does your explanation suggest what shape to expect in the partial dependence plots for the other features?
&lt;/p&gt;

&lt;p&gt;
Create all other partial plots.
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for FEATURE in base_features:
    pdp_dist = pdp.pdp_isolate(model=first_model,
			       dataset=x_validate,
			       model_features=base_features,
			       feature=FEATURE)
    pdp.pdp_plot(pdp_dist, FEATURE)
    output = f"{FEATURE}_pdp_plot.png"
    pyplot.savefig(OUTPUT_PATH/output)
    print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[[file:pickup_longitude_pdp_plot.png]]
[[file:pickup_latitude_pdp_plot.png]]
[[file:dropoff_longitude_pdp_plot.png]]
[[file:dropoff_latitude_pdp_plot.png]]
&lt;/pre&gt;

&lt;p&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png" alt="eb446225f180346680b332c924342792de7e5135.png"&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/f2c9b641a0b8f044729d15efd208f672e3ecb7c1.png" alt="f2c9b641a0b8f044729d15efd208f672e3ecb7c1.png"&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/6334156ec3931ff43161785fde6f1f27a460bccd.png" alt="6334156ec3931ff43161785fde6f1f27a460bccd.png"&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/4417e1dcc2d023ce83c886e006ce5de690bfe97b.png" alt="4417e1dcc2d023ce83c886e006ce5de690bfe97b.png"&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/pickup_latitude_pdp_plot.png" alt="pickup_latitude_pdp_plot.png"&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/dropoff_longitude_pdp_plot.png" alt="dropoff_longitude_pdp_plot.png"&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/dropoff_latitude_pdp_plot.png" alt="dropoff_latitude_pdp_plot.png"&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png" alt="eb446225f180346680b332c924342792de7e5135.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf53f3c3" class="outline-3"&gt;
&lt;h3 id="orgf53f3c3"&gt;Question 2&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf53f3c3"&gt;
&lt;p&gt;
Now you will run a 2D partial dependence plot.  As a reminder, here is the code from the tutorial.  
&lt;/p&gt;

&lt;p&gt;
Create a 2D plot for the features &lt;code&gt;pickup_longitude&lt;/code&gt; and &lt;code&gt;dropoff_longitude&lt;/code&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURES = "pickup_longitude dropoff_longitude".split()
interaction  =  pdp.pdp_interact(model=first_model,
				 dataset=x_validate,
				 model_features=base_features,
				 features=FEATURES)
pdp.pdp_interact_plot(pdp_interact_out=interaction,
		      feature_names=FEATURES, plot_type='contour')
output = "longitude_interaction.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[[file:longitude_interaction.png]]
&lt;/pre&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/744ca56de862b725f3b5132f1c80c9bf41837026.png" alt="744ca56de862b725f3b5132f1c80c9bf41837026.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/longitude_interaction.png" alt="longitude_interaction.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;
Our plot shows that the fares are highest at the top-left and bottom-right corners, as you might expect, since this would be the furthest distance from pickup to dropoff.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5ec7614" class="outline-3"&gt;
&lt;h3 id="org5ec7614"&gt;Question 3&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5ec7614"&gt;
&lt;blockquote&gt;
&lt;p&gt;
Consider a ride starting at longitude -73.92 and ending at longitude -74. Using the graph from the last question, estimate how much money the rider would have saved if they'd started the ride at longitude -73.98 instead?
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;
I don't exactly agree with the interpretation given by the kaggle notebook. Looking at the plot, -73.92 to -74 appears to cost 27, while a -73.92 to -74 would cost 9 - but the notebook says that -73.92 to -74 costs 24. So I would say there would be a saving of 18 while the given answer is 15. To reconcile the difference (kind of) we might say that -73.92 to -74 costs 12, not 9 - it's not really easy to tell by the plot, in which case I would also say the savings is 15.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdc69d42" class="outline-3"&gt;
&lt;h3 id="orgdc69d42"&gt;Question 4&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdc69d42"&gt;
&lt;blockquote&gt;
&lt;p&gt;
In the PDP's you've seen so far, location features have primarily served as a proxy to capture distance traveled. In the permutation importance lessons, you added the features `abs_lon_change` and `abs_lat_change` as a more direct measure of distance.
&lt;/p&gt;

&lt;p&gt;
Create these features again here.
&lt;/p&gt;

&lt;p&gt;
After you run it, identify the most important difference between this partial dependence plot and the one you got without absolute value features. The code to generate the PDP without absolute value features is at the top of this code cell.
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURE = 'pickup_longitude'
pdp_dist_original = pdp.pdp_isolate(model=first_model,
				    dataset=x_validate,
				    model_features=base_features,
				    feature=FEATURE)
pdp.pdp_plot(pdp_dist_original, FEATURE)
output = "pre_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/pre_distance.png" alt="pre_distance.png"&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/eb446225f180346680b332c924342792de7e5135.png" alt="eb446225f180346680b332c924342792de7e5135.png"&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data['abs_lon_change'] = abs(data.pickup_longitude - data.dropoff_longitude)
data['abs_lat_change'] = abs(data.pickup_latitude - data.dropoff_longitude)

features_2  = ['pickup_longitude',
	       'pickup_latitude',
	       'dropoff_longitude',
	       'dropoff_latitude',
	       'abs_lat_change',
	       'abs_lon_change']

X = data[features_2]
new_x_train, new_x_validate, new_y_train, new_y_validate = train_test_split(X, y, random_state=1)

estimators = list(range(50, 200, 10))
max_depth = list(range(10, 100, 10)) + [None]

grid = dict(n_estimators=estimators,
	    max_depth=max_depth)

model = RandomForestRegressor()
search = RandomizedSearchCV(estimator=model,
			    param_distributions=grid,
			    n_iter=40,
			    n_jobs=-1,
			    random_state=1)
with TIMER:
    search.fit(new_x_train, new_y_train)
second_model = search.best_estimator_
print(f"CV Training R^2: {search.best_score_:0.2f}")
print(f"Training R^2: {first_model.score(x_train, y_train): 0.2f}")
print(f"Validation R^2: {first_model.score(x_validate, y_validate):0.2f}")
print(search.best_params_)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2020-02-09 17:36:14,915 graeae.timers.timer start: Started: 2020-02-09 17:36:14.915123
2020-02-09 17:43:16,053 graeae.timers.timer end: Ended: 2020-02-09 17:43:16.053011
2020-02-09 17:43:16,053 graeae.timers.timer end: Elapsed: 0:07:01.137888
CV Training R^2: 0.46
Training R^2:  0.92
Validation R^2: 0.43
{'n_estimators': 190, 'max_depth': 30}
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURE = 'pickup_longitude'
pdp_dist = pdp.pdp_isolate(model=second_model,
			   dataset=new_x_validate,
			   model_features=features_2,
			   feature=FEATURE)

pdp.pdp_plot(pdp_dist, FEATURE)
output = "pickup_longitude_with_distance_added.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/pickup_longitude_with_distance_added.png" alt="pickup_longitude_with_distance_added.png"&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/.ob-jupyter/98c17657a972a412bc6d33619b6e23ee5818b884.png" alt="98c17657a972a412bc6d33619b6e23ee5818b884.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org77524af" class="outline-3"&gt;
&lt;h3 id="org77524af"&gt;Question 5&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org77524af"&gt;
&lt;blockquote&gt;
&lt;p&gt;
Consider a scenario where you have only 2 predictive features, which we will call `feat_A` and `feat_B`. Both features have minimum values of -1 and maximum values of 1.  The partial dependence plot for `feat_A` increases steeply over its whole range, whereas the partial dependence plot for feature B increases at a slower rate (less steeply) over its whole range.
Does this guarantee that `feat_A` will have a higher permutation importance than `feat_B`.  Why or why not?
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;
It doesn't - the partial dependence plot shows how the predictions change based on the inputs, but it isn't the same thing as the feature importance - it might be the case that a few inputs create a large difference but most points don't, in which case the feature importance won't be very large.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>interpret</category><category>machine learning</category><category>tutorial</category><category>visualization</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-partial-dependence-plots/</guid><pubDate>Sun, 09 Feb 2020 21:26:37 GMT</pubDate></item><item><title>Partial Dependence Plots</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgc9ff046"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgbf55cdd"&gt;What is this about?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org39ad250"&gt;How does it work?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org45cbc22"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org1243b13"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgb037c53"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org01fd838"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org248cd26"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orge42515e"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgd88870e"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgb2101df"&gt;A Decision Tree Model&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org5eb50fd"&gt;Visualizing the Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org18d972e"&gt;Partial Dependency Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org47522f7"&gt;Distance Covered&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orga562fef"&gt;A Random Forest Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org15aefcd"&gt;2D Partial Dependence Plots&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgaf48c0f"&gt;Forest From the Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org10d9dd0"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc9ff046" class="outline-2"&gt;
&lt;h2 id="orgc9ff046"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc9ff046"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbf55cdd" class="outline-3"&gt;
&lt;h3 id="orgbf55cdd"&gt;What is this about?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbf55cdd"&gt;
&lt;p&gt;
These are my notes/re-write of the &lt;a href="https://www.kaggle.com/dansbecker/partial-plots"&gt;Partial Dependence Plots&lt;/a&gt; tutorial on kaggle. Partial Dependence Plots are a complement to Permutation Importance in that Permutation Importance can tell you which features are contributing to the model but don't tell you how features change with different inputs. Given that they have the word "Plot" in their name you can guess that this is a visualization method and since humans have a hard time visualizing things with more than three dimensions, using them requires selecting a subset of features (or maybe just one feature) to visualize at a time, so the fact that Permutation Importance ranks features by their contribution might come in handy in deciding which features to use.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org39ad250" class="outline-3"&gt;
&lt;h3 id="org39ad250"&gt;How does it work?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org39ad250"&gt;
&lt;p&gt;
In a simplistic view we might say that it takes input data and then repeatedly alters the values in our feature of choice, freezing the other values so that we can see how varying the feature changes the prediction. You can then repeat this for multiple rows of data and take the average prediction for each value of our feature.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org45cbc22" class="outline-3"&gt;
&lt;h3 id="org45cbc22"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org45cbc22"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1243b13" class="outline-4"&gt;
&lt;h4 id="org1243b13"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1243b13"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb037c53" class="outline-4"&gt;
&lt;h4 id="orgb037c53"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb037c53"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dotenv&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dotenv&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pdpbox&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;get_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info_plots&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tree&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;graphviz&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org01fd838" class="outline-3"&gt;
&lt;h3 id="org01fd838"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org01fd838"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org248cd26" class="outline-4"&gt;
&lt;h4 id="org248cd26"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org248cd26"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;SLUG&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"partial-dependence-plots"&lt;/span&gt;
&lt;span class="n"&gt;OUTPUT_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"../../files/posts/tutorials/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;SLUG&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge42515e" class="outline-4"&gt;
&lt;h4 id="orge42515e"&gt;The Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge42515e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"~/.env"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;load_dotenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;override&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"FIFA-2018"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd88870e" class="outline-2"&gt;
&lt;h2 id="orgd88870e"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd88870e"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb2101df" class="outline-3"&gt;
&lt;h3 id="orgb2101df"&gt;A Decision Tree Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb2101df"&gt;
&lt;p&gt;
The data is the same set from the Permutation Importance exercise (team data to predict whether one of them would become the &lt;i&gt;Budweiser Man of the Match&lt;/i&gt;). Out initial model will be a shallow decision tree so that we can compare its structure to the plots.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Man of the Match"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"Yes"&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_validate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_validate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;tree_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_samples_split&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5eb50fd" class="outline-4"&gt;
&lt;h4 id="org5eb50fd"&gt;Visualizing the Decision Tree&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5eb50fd"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tree_graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tree&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;export_graphviz&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tree_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;graphviz&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Source&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tree_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"decision_tree.dot"&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;render&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_PATH&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"png"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"[[file:{output}.png]]"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/decision_tree.dot.png" alt="decision_tree.dot.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org18d972e" class="outline-4"&gt;
&lt;h4 id="org18d972e"&gt;Partial Dependency Plot&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org18d972e"&gt;
&lt;p&gt;
To create the plot we can use the &lt;a href="https://pdpbox.readthedocs.io/en/latest/"&gt;PDPBox library&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="orgc7ef633"&gt;&lt;/a&gt;Create the Data to Plot&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgc7ef633"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;FEATURE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Goal Scored"&lt;/span&gt;
&lt;span class="n"&gt;pdp_goals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_isolate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tree_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_validate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="n"&gt;model_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FEATURE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;

&lt;li&gt;&lt;a id="org05ea109"&gt;&lt;/a&gt;Plot It&lt;br&gt;
&lt;div class="outline-text-5" id="text-org05ea109"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdp_goals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;FEATURE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"pdp_goals_scored.png"&lt;/span&gt;
&lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_PATH&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"[[file:{output}]]"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_goals_scored.png" alt="pdp_goals_scored.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;
Looking at the plot we can interpret it as saying that scoring that first goal is helpful in getting someone on the team the Man of the Match, but after that, more goals doesn't help. The PDPBox documentation has no real information about interpreting the output (or anything they provide, really), but according to the Kaggle tutorial the y-axis is the change in the prediction from the baseline as x changes.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-org47522f7" class="outline-4"&gt;
&lt;h4 id="org47522f7"&gt;Distance Covered&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org47522f7"&gt;
&lt;p&gt;
We can also look at how much the distance the players cover on the field matters.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;FEATURE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Distance Covered (Kms)"&lt;/span&gt;
&lt;span class="n"&gt;pdp_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_isolate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tree_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_validate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			       &lt;span class="n"&gt;model_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FEATURE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdp_distance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;FEATURE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"pdp_distance.png"&lt;/span&gt;
&lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_PATH&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"[[file:{output}]]"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_distance.png" alt="pdp_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
It looks like there's one distance at which the probabilities increase and then going further doesn't matter.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdp_distance&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_grids&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
102.0
&lt;/pre&gt;


&lt;p&gt;
So you want your team to cover at least 102 Kilometers, but covering more won't help you.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga562fef" class="outline-3"&gt;
&lt;h3 id="orga562fef"&gt;A Random Forest Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga562fef"&gt;
&lt;p&gt;
The Decision Tree was useful because the simplicity made it easy to interpret, but an ensemble method like a Random Forest probably makes a better model so let's see what it reveals.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;forest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;FEATURE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Goal Scored"&lt;/span&gt;
&lt;span class="n"&gt;pdp_goals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_isolate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;forest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_validate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="n"&gt;model_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FEATURE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdp_goals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;FEATURE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"pdp_forest_goals_scored.png"&lt;/span&gt;
&lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_PATH&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"[[file:{output}]]"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_forest_goals_scored.png" alt="pdp_forest_goals_scored.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So, our random forest says that the greatest gain comes from the first goal, but there is in fact a greater probability as you increase the number of goals scored.
&lt;/p&gt;

&lt;p&gt;
What about distance covered?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;FEATURE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"Distance Covered (Kms)"&lt;/span&gt;
&lt;span class="n"&gt;pdp_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_isolate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;forest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_validate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			       &lt;span class="n"&gt;model_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FEATURE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdp_distance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;FEATURE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"pdp_forest_distance.png"&lt;/span&gt;
&lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_PATH&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"[[file:{output}]]"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_forest_distance.png" alt="pdp_forest_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdp_distance&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_grids&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;pdp_distance&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
102.0
&lt;/pre&gt;


&lt;p&gt;
Our forest says that, once again, covering 102 kilometers maximizes the probability, but in this case it appears that going beyond that actually decreases the probability that your team would have the man of the match.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org15aefcd" class="outline-3"&gt;
&lt;h3 id="org15aefcd"&gt;2D Partial Dependence Plots&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org15aefcd"&gt;
&lt;p&gt;
Rather than plot a single feature against the probability of becoming the man of the match you can plot how two features interact to affect the probability.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;FEATURES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Goal Scored"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Distance Covered (Kms)"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;interaction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_interact&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tree_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_validate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			       &lt;span class="n"&gt;model_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			       &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FEATURES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_interact_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdp_interact_out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;interaction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FEATURES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;plot_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"contour"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"goals_vs_distance.png"&lt;/span&gt;
&lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_PATH&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"[[file:{output}]]"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/goals_vs_distance.png" alt="goals_vs_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;b&gt;Note:&lt;/b&gt; The version of PDPBox on pypi currently has a bug (as of February 8, 2020) that won't let you create the previous plot, install it from github instead.
&lt;/p&gt;

&lt;p&gt;
The plot shows a more succinct version of the two plots we had seen before - the optimal point for these two features is 1 goal and 102 Kms covered.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgaf48c0f" class="outline-4"&gt;
&lt;h4 id="orgaf48c0f"&gt;Forest From the Trees&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgaf48c0f"&gt;
&lt;p&gt;
Let's re-run the same plot using the Random Forest instead of the Decision Tree.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;FEATURES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Goal Scored"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Distance Covered (Kms)"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;interaction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_interact&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;forest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x_validate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			       &lt;span class="n"&gt;model_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			       &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FEATURES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;pdp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pdp_interact_plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pdp_interact_out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;interaction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FEATURES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;plot_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"contour"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"forest_goals_vs_distance.png"&lt;/span&gt;
&lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_PATH&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"[[file:{output}]]"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/forest_goals_vs_distance.png" alt="forest_goals_vs_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The random forest suggests a slightly different scenario - here you want 2 gooalds and between 100 and 110 Kms (or thereabouts) covered to get the best probability.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org10d9dd0" class="outline-2"&gt;
&lt;h2 id="org10d9dd0"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org10d9dd0"&gt;
&lt;p&gt;
This showed how to use the PDPBox library to create Partial Dependence Plots to look at how input values for features affects the predicted outcomes. Unfortunately it looks like PDPBox might have been abandoned, but while it works it's a nice library.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>interpret</category><category>machine learning</category><category>tutorial</category><category>visualization</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/</guid><pubDate>Sat, 08 Feb 2020 20:48:50 GMT</pubDate></item><item><title>Trying out DABEST</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#org11bd7bb"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#orgdb433f1"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#org28995cc"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#org0ef2ac9"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#orgd119e94"&gt;Petal Width&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#org5d88711"&gt;Hedge's G&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/#orgeb657d5"&gt;Cohen's D&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org11bd7bb" class="outline-2"&gt;
&lt;h2 id="org11bd7bb"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org11bd7bb"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdb433f1" class="outline-3"&gt;
&lt;h3 id="orgdb433f1"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdb433f1"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6aed581" class="outline-4"&gt;
&lt;h4 id="org6aed581"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6aed581"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from sklearn.datasets import load_iris
import dabest
import pandas
import seaborn
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org28995cc" class="outline-3"&gt;
&lt;h3 id="org28995cc"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org28995cc"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org49fbab0" class="outline-4"&gt;
&lt;h4 id="org49fbab0"&gt;The Data Set&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org49fbab0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;iris = load_iris()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(iris.DESCR)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
                
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher's paper. Note that it's the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher's paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp;amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

.. topic:: References

   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"
     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
     Mathematical Statistics" (John Wiley, NY, 1950).
   - Duda, R.O., &amp;amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp;amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments".  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.DataFrame(iris.data, columns=iris.feature_names)
target = pandas.Series(iris.target)
names = dict(zip(range(len(iris.target_names)), iris.target_names))
data["species"] = target.map(names)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0ef2ac9" class="outline-2"&gt;
&lt;h2 id="org0ef2ac9"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0ef2ac9"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd119e94" class="outline-3"&gt;
&lt;h3 id="orgd119e94"&gt;Petal Width&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd119e94"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;iris_dabest = dabest.load(data=data, x="species", y="petal width (cm)", idx=iris.target_names)
iris_dabest.mean_diff.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/petal_width.png" alt="petal_width.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5d88711" class="outline-3"&gt;
&lt;h3 id="org5d88711"&gt;Hedge's G&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5d88711"&gt;
&lt;p&gt;
&lt;a href="https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/hedgeg.htm"&gt;Hedges G&lt;/a&gt; is a measurement of effect size, similar to Cohen's d but with better properties when the samples are smaller or the sample sizes are different.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;iris_dabest.hedges_g.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/hedges_g.png" alt="hedges_g.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(iris_dabest.hedges_g)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
DABEST v0.2.7
=============
             
Good afternoon!
The current time is Mon Dec 16 15:56:24 2019.

The unpaired Hedges' g between setosa and versicolor is 6.76 [95%CI 5.71, 7.86].
The two-sided p-value of the Mann-Whitney test is 2.28e-18.

The unpaired Hedges' g between setosa and virginica is 8.49 [95%CI 7.08, 9.77].
The two-sided p-value of the Mann-Whitney test is 2.43e-18.

5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.
The p-value(s) reported are the likelihood(s) of observing the effect size(s),
if the null hypothesis of zero difference is true.

To get the results of all valid statistical tests, use `.hedges_g.statistical_tests`
&lt;/pre&gt;

&lt;p&gt;
According to &lt;a href="https://www.wikiwand.com/en/Effect_size"&gt;Wikipedia&lt;/a&gt;, an effect size of 2 is "huge" so since the differences between the setosa and versicolor and setosa and virginica are 6.76 and 8.49 respectively, we might conclude that there is a significant difference between the petal width of the setosa and the other two species.
&lt;/p&gt;

&lt;p&gt;
I don't think that's really what this is meant for, but I just wanted to see how it works.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgeb657d5" class="outline-3"&gt;
&lt;h3 id="orgeb657d5"&gt;Cohen's D&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgeb657d5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;iris_dabest.cohens_d.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/cohens_d.png" alt="cohens_d.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(iris_dabest.cohens_d)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
DABEST v0.2.7
=============
             
Good afternoon!
The current time is Mon Dec 16 16:46:25 2019.

The unpaired Cohen's d between setosa and versicolor is 6.82 [95%CI 5.76, 7.92].
The two-sided p-value of the Mann-Whitney test is 2.28e-18.

The unpaired Cohen's d between setosa and virginica is 8.56 [95%CI 7.13, 9.85].
The two-sided p-value of the Mann-Whitney test is 2.43e-18.

5000 bootstrap samples were taken; the confidence interval is bias-corrected and accelerated.
The p-value(s) reported are the likelihood(s) of observing the effect size(s),
if the null hypothesis of zero difference is true.

To get the results of all valid statistical tests, use `.cohens_d.statistical_tests`
&lt;/pre&gt;

&lt;p&gt;
In this case the Cohen's d and Hedges g look similar.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>exploration</category><category>statistics</category><category>visualization</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/trying-out-dabest/</guid><pubDate>Mon, 16 Dec 2019 21:50:24 GMT</pubDate></item><item><title>HoloViews Introduction</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orga81d22f"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orgd24b738"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orga23c8f5"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orgf565130"&gt;This Project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org710b54c"&gt;The HoloViews Backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org904c77c"&gt;A Partial Bokeh Embedder&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org278fa33"&gt;The Data Set&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org6c56cac"&gt;Load It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org86b8bd2"&gt;Make The DataFrame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#orgc62a56e"&gt;A Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/#org9a4a6ad"&gt;Adding To the Layout&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga81d22f" class="outline-2"&gt;
&lt;h2 id="orga81d22f"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga81d22f"&gt;
&lt;p&gt;
I've already taken an initial look at HVPlot so I'm going to look at &lt;a href="http://holoviews.org/"&gt;HoloViews&lt;/a&gt; which acts as an intermediate layer between the main plotting libraries like bokeh and matplotlib and the upper layer given by HVPlot. I haven't used it before so I'm not really sure when you would use what. I guess &lt;code&gt;HVPlot&lt;/code&gt; gives you access to the &lt;code&gt;pandas&lt;/code&gt; plots in bokeh without a lot of work, which is nice, although I noticed that the plots tended to be missing things sometimes (like the Hover tool) so if you want to add more back in you probably have to understand &lt;code&gt;HoloViews&lt;/code&gt; which itself sometimes doesn't give you what you want (like the ability to render it in org-mode posts) so you still need bokeh too, sometimes. And of course I'm only using the static-page versions of everything, not the features that work with a bokeh or jupyter server. But I guess that's for later.
&lt;/p&gt;

&lt;p&gt;
I'm going to be working from the &lt;a href="http://holoviews.org/getting_started/Introduction.html"&gt;Introduction&lt;/a&gt; of their Getting Started guide.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd24b738" class="outline-2"&gt;
&lt;h2 id="orgd24b738"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd24b738"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga23c8f5" class="outline-3"&gt;
&lt;h3 id="orga23c8f5"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga23c8f5"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1df0acc" class="outline-4"&gt;
&lt;h4 id="org1df0acc"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1df0acc"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org57ad62b" class="outline-4"&gt;
&lt;h4 id="org57ad62b"&gt;From PiPy&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org57ad62b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;holoviews&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;opts&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;fetch_california_housing&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;holoviews&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf565130" class="outline-3"&gt;
&lt;h3 id="orgf565130"&gt;This Project&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf565130"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bartleby_the_penguin.tangles.embed_bokeh&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EmbedBokeh&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org710b54c" class="outline-3"&gt;
&lt;h3 id="org710b54c"&gt;The HoloViews Backend&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org710b54c"&gt;
&lt;p&gt;
If you use &lt;code&gt;HVPlot&lt;/code&gt; you don't need to set the backend (because it defaults to 'bokeh', I think) but this is going to be about &lt;code&gt;HoloViews&lt;/code&gt; so I'm going to do it their way, rather than relying on all the &lt;code&gt;pandas&lt;/code&gt; methods.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extension&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"bokeh"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org904c77c" class="outline-3"&gt;
&lt;h3 id="org904c77c"&gt;A Partial Bokeh Embedder&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org904c77c"&gt;
&lt;p&gt;
Since the output folder is always the same I'm going to bind it to the EmbedBokeh definition.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;plot_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"../../files/posts/libraries/holoviews-introduction/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EmbedBokeh&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;folder_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plot_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org278fa33" class="outline-2"&gt;
&lt;h2 id="org278fa33"&gt;The Data Set&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org278fa33"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6c56cac" class="outline-3"&gt;
&lt;h3 id="org6c56cac"&gt;Load It&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6c56cac"&gt;
&lt;p&gt;
Sklearn downloads it as a 'bunch' so we need to get it in that form first and then turn it into a data frame (I'm sure there's a way to skip this step but this is the way I already know how to do it).
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"~/data/datasets/california-housing"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_dir&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;bunch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fetch_california_housing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bunch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DESCR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/data/datasets/california-housing
.. _california_housing_dataset:

California Housing dataset
--------------------------

**Data Set Characteristics:**

    :Number of Instances: 20640

    :Number of Attributes: 8 numeric, predictive attributes and the target

    :Attribute Information:
        - MedInc        median income in block
        - HouseAge      median house age in block
        - AveRooms      average number of rooms
        - AveBedrms     average number of bedrooms
        - Population    block population
        - AveOccup      average house occupancy
        - Latitude      house block latitude
        - Longitude     house block longitude

    :Missing Attribute Values: None

This dataset was obtained from the StatLib repository.
http://lib.stat.cmu.edu/datasets/

The target variable is the median house value for California districts.

This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).

It can be downloaded/loaded using the
:func:`sklearn.datasets.fetch_california_housing` function.

.. topic:: References

    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
      Statistics and Probability Letters, 33 (1997) 291-297
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org86b8bd2" class="outline-3"&gt;
&lt;h3 id="org86b8bd2"&gt;Make The DataFrame&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org86b8bd2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bunch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bunch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"median_value"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bunch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   
1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   
2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   
3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   
4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   

   Longitude  median_value  
0    -122.23         4.526  
1    -122.22         3.585  
2    -122.24         3.521  
3    -122.25         3.413  
4    -122.25         3.422  
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc62a56e" class="outline-2"&gt;
&lt;h2 id="orgc62a56e"&gt;A Plot&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc62a56e"&gt;
&lt;p&gt;
Our target is the median value of the house. Does that correlate with median income?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"MedInc"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Median Income"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
			    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"median_value"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Median Value"&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
			    &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"California Housing"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
After setting up the basic plot we can do things to affect the appearance like setting the color or adding tools.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"red"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"hover"&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;script src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/income_vs_value.js" id="bfbe3866-7981-492f-b2f8-918396abac02"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9a4a6ad" class="outline-2"&gt;
&lt;h2 id="org9a4a6ad"&gt;Adding To the Layout&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9a4a6ad"&gt;
&lt;p&gt;
What if we want to add a distrbution to the plot? HoloViews uses the &lt;code&gt;+&lt;/code&gt; operator to indicate that you want to append a plot to another one.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;layout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;scatter&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Histogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;histogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HouseAge&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;kdims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"HouseAge"&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;layout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layout&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Histogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"hover"&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;script src="https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/income_vs_value_with_house_age.js" id="728203cb-7d9b-4ecd-aeb0-a30aec6a7655"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>exploration</category><category>holoviews</category><category>visualization</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/libraries/holoviews-introduction/</guid><pubDate>Sat, 02 Feb 2019 22:15:01 GMT</pubDate></item></channel></rss>