<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Visions, Voices, Data (Posts about permutation importance)</title><link>https://necromuralist.github.io/Visions-Voices-Data/</link><description></description><atom:link href="https://necromuralist.github.io/Visions-Voices-Data/categories/permutation-importance.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2020 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Sun, 09 Feb 2020 22:59:34 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Exercise in Permutation Importance</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org22d096f"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#orgf3f273d"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org18c6cba"&gt;From Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#orge2fb97f"&gt;From PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org887cabb"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org1343604"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org7345bb2"&gt;A Timer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#orgae3625d"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org504486f"&gt;The Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org82dc3d8"&gt;Table Printer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#orgf71f41e"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org717161e"&gt;The Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org15139e6"&gt;Set Up the Training and Test Sets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#orga533238"&gt;Build and Train the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org4b68a68"&gt;Questions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#orga4e9980"&gt;Question 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org19d74b2"&gt;A New Model&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#orgc13599c"&gt;Question 4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org0064e8a"&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org2965c18"&gt;The Permutation Importance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org570f4c4"&gt;Question 5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org86b0fdb"&gt;Question 6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/#org1cd4e38"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org22d096f" class="outline-2"&gt;
&lt;h2 id="org22d096f"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org22d096f"&gt;
&lt;p&gt;
This is my re-do of the &lt;a href="https://www.kaggle.com/learn/machine-learning-explainability"&gt;Machine Learning Explainability&lt;/a&gt; Permutation Importance exercise on kaggle. It uses the data from the &lt;a href="https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data"&gt;New York City Taxi Fare Prediction&lt;/a&gt; dataset on kaggle.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf3f273d" class="outline-3"&gt;
&lt;h3 id="orgf3f273d"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf3f273d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org18c6cba" class="outline-4"&gt;
&lt;h4 id="org18c6cba"&gt;From Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org18c6cba"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from argparse import Namespace
from functools import partial
from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge2fb97f" class="outline-4"&gt;
&lt;h4 id="orge2fb97f"&gt;From PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge2fb97f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from eli5.sklearn import PermutationImportance

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from tabulate import tabulate

import eli5
import hvplot.pandas
import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org887cabb" class="outline-4"&gt;
&lt;h4 id="org887cabb"&gt;Others&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org887cabb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import EmbedHoloviews, EnvironmentLoader, Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1343604" class="outline-3"&gt;
&lt;h3 id="org1343604"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1343604"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7345bb2" class="outline-4"&gt;
&lt;h4 id="org7345bb2"&gt;A Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7345bb2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgae3625d" class="outline-4"&gt;
&lt;h4 id="orgae3625d"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgae3625d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SLUG = "exercise-in-permutation-importance"
PATH = Path("../../files/posts/tutorials/")/SLUG
Plot = Namespace(
    width=1000,
    height=800,
    )
Embed = partial(EmbedHoloviews, folder_path=PATH)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org504486f" class="outline-4"&gt;
&lt;h4 id="org504486f"&gt;The Environment&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org504486f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ENVIRONMENT = EnvironmentLoader()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org82dc3d8" class="outline-4"&gt;
&lt;h4 id="org82dc3d8"&gt;Table Printer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org82dc3d8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TABLE = partial(tabulate, tablefmt="orgtbl", headers="keys", showindex=False)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf71f41e" class="outline-2"&gt;
&lt;h2 id="orgf71f41e"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf71f41e"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org717161e" class="outline-4"&gt;
&lt;h4 id="org717161e"&gt;The Dataset&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org717161e"&gt;
&lt;p&gt;
Since this is about permutation importance we're just going to load a subset (there are over five million rows in the dataset) and use previously discovered values to get rid of outliers.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ROWS = 5 * 10**4
PATH = Path(ENVIRONMENT["NY-TAXI"]).expanduser()
assert PATH.is_dir()
data = pandas.read_csv(PATH/"train.csv", nrows=ROWS)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(TABLE(data.iloc[0].reset_index().rename(columns={"index": "Column", 0: "Value"})))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Column&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;key&lt;/td&gt;
&lt;td class="org-right"&gt;2009-06-15 17:26:21.0000001&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;fare_amount&lt;/td&gt;
&lt;td class="org-right"&gt;4.5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;pickup_datetime&lt;/td&gt;
&lt;td class="org-right"&gt;2009-06-15 17:26:21 UTC&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;pickup_longitude&lt;/td&gt;
&lt;td class="org-right"&gt;-73.844311&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;pickup_latitude&lt;/td&gt;
&lt;td class="org-right"&gt;40.721319&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;dropoff_longitude&lt;/td&gt;
&lt;td class="org-right"&gt;-73.84161&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;dropoff_latitude&lt;/td&gt;
&lt;td class="org-right"&gt;40.712278000000005&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;passenger_count&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="orgdd997a7"&gt;&lt;/a&gt;Trim Outliers&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgdd997a7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(TABLE(data.describe(), showindex=True))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt; &lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;fare_amount&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;pickup_longitude&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;pickup_latitude&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;dropoff_longitude&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;dropoff_latitude&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;passenger_count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;count&lt;/td&gt;
&lt;td class="org-right"&gt;50000&lt;/td&gt;
&lt;td class="org-right"&gt;50000&lt;/td&gt;
&lt;td class="org-right"&gt;50000&lt;/td&gt;
&lt;td class="org-right"&gt;50000&lt;/td&gt;
&lt;td class="org-right"&gt;50000&lt;/td&gt;
&lt;td class="org-right"&gt;50000&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;mean&lt;/td&gt;
&lt;td class="org-right"&gt;11.3642&lt;/td&gt;
&lt;td class="org-right"&gt;-72.5098&lt;/td&gt;
&lt;td class="org-right"&gt;39.9338&lt;/td&gt;
&lt;td class="org-right"&gt;-72.5046&lt;/td&gt;
&lt;td class="org-right"&gt;39.9263&lt;/td&gt;
&lt;td class="org-right"&gt;1.66784&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;std&lt;/td&gt;
&lt;td class="org-right"&gt;9.68556&lt;/td&gt;
&lt;td class="org-right"&gt;10.3939&lt;/td&gt;
&lt;td class="org-right"&gt;6.22486&lt;/td&gt;
&lt;td class="org-right"&gt;10.4076&lt;/td&gt;
&lt;td class="org-right"&gt;6.01474&lt;/td&gt;
&lt;td class="org-right"&gt;1.28919&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;min&lt;/td&gt;
&lt;td class="org-right"&gt;-5&lt;/td&gt;
&lt;td class="org-right"&gt;-75.4238&lt;/td&gt;
&lt;td class="org-right"&gt;-74.0069&lt;/td&gt;
&lt;td class="org-right"&gt;-84.6542&lt;/td&gt;
&lt;td class="org-right"&gt;-74.0064&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;25%&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;td class="org-right"&gt;-73.9921&lt;/td&gt;
&lt;td class="org-right"&gt;40.7349&lt;/td&gt;
&lt;td class="org-right"&gt;-73.9912&lt;/td&gt;
&lt;td class="org-right"&gt;40.7344&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;50%&lt;/td&gt;
&lt;td class="org-right"&gt;8.5&lt;/td&gt;
&lt;td class="org-right"&gt;-73.9818&lt;/td&gt;
&lt;td class="org-right"&gt;40.7527&lt;/td&gt;
&lt;td class="org-right"&gt;-73.9801&lt;/td&gt;
&lt;td class="org-right"&gt;40.7534&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;75%&lt;/td&gt;
&lt;td class="org-right"&gt;12.5&lt;/td&gt;
&lt;td class="org-right"&gt;-73.9671&lt;/td&gt;
&lt;td class="org-right"&gt;40.7674&lt;/td&gt;
&lt;td class="org-right"&gt;-73.9636&lt;/td&gt;
&lt;td class="org-right"&gt;40.7682&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;max&lt;/td&gt;
&lt;td class="org-right"&gt;200&lt;/td&gt;
&lt;td class="org-right"&gt;40.7835&lt;/td&gt;
&lt;td class="org-right"&gt;401.083&lt;/td&gt;
&lt;td class="org-right"&gt;40.851&lt;/td&gt;
&lt;td class="org-right"&gt;43.4152&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;to_plot = data[[column for column in data.columns if "latitude" in column or "longitude" in column]]
plot = to_plot.hvplot.box().opts(title="Column Box-Plots", width=Plot.width, height=Plot.height)
Embed(plot=plot, file_name="column_box_plots")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/column_box_plots.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
So you can see that there are negative fares, which seems wrong, and some outliers.
&lt;/p&gt;

&lt;p&gt;
This uses the pandas &lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html"&gt;query&lt;/a&gt; method which let's you write slightly more readable code for boolean slicing.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(f"{len(data):,}")
data = data.query("pickup_latitude &amp;gt; 40.7 and pickup_latitude &amp;lt; 40.8 and " +
		  "dropoff_latitude &amp;gt; 40.7 and dropoff_latitude &amp;lt; 40.8 and " +
		  "pickup_longitude &amp;gt; -74 and pickup_longitude &amp;lt; -73.9 and " +
		  "dropoff_longitude &amp;gt; -74 and dropoff_longitude &amp;lt; -73.9 and " +
		  "fare_amount &amp;gt; 0"
		  )
print(f"{len(data):,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
50,000
31,289
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-org15139e6" class="outline-4"&gt;
&lt;h4 id="org15139e6"&gt;Set Up the Training and Test Sets&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org15139e6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;y = data.fare_amount
base_features = ['pickup_longitude',
		 'pickup_latitude',
		 'dropoff_longitude',
		 'dropoff_latitude',
		 'passenger_count']

X = data[base_features]
x_train, x_validate, y_train, y_validate = train_test_split(X, y, random_state=1)

print(f"{len(x_train):,}")
print(f"{len(x_validate):,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
23,466
7,823
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga533238" class="outline-3"&gt;
&lt;h3 id="orga533238"&gt;Build and Train the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga533238"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;estimators = list(range(50, 200, 10))
max_depth = list(range(10, 100, 10)) + [None]

grid = dict(n_estimators=estimators,
	    max_depth=max_depth)

model = RandomForestRegressor()
search = RandomizedSearchCV(estimator=model,
			    param_distributions=grid,
			    n_iter=40,
			    n_jobs=-1,
			    random_state=1)
with TIMER:
    search.fit(x_train, y_train)
first_model = search.best_estimator_
print(f"CV Training R^2: {search.best_score_:0.2f}")
print(f"Training R^2: {first_model.score(x_train, y_train): 0.2f}")
print(f"Validation R^2: {first_model.score(x_validate, y_validate):0.2f}")
print(search.best_params_)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2020-02-09 14:28:34,418 graeae.timers.timer start: Started: 2020-02-09 14:28:34.418069
/home/athena/.virtualenvs/Visions-Voices-Data/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
2020-02-09 14:34:20,498 graeae.timers.timer end: Ended: 2020-02-09 14:34:20.498920
2020-02-09 14:34:20,500 graeae.timers.timer end: Elapsed: 0:05:46.080851
CV Training R^2: 0.45
Training R^2:  0.92
Validation R^2: 0.43
{'n_estimators': 180, 'max_depth': 50}
&lt;/pre&gt;


&lt;p&gt;
So it isn't really a great model, but we'll ignore that for now.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4b68a68" class="outline-3"&gt;
&lt;h3 id="org4b68a68"&gt;Questions&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4b68a68"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga4e9980" class="outline-4"&gt;
&lt;h4 id="orga4e9980"&gt;Question 1&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga4e9980"&gt;
&lt;blockquote&gt;
&lt;p&gt;
The first model uses the following features:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;pickup_longitude&lt;/li&gt;
&lt;li&gt;pickup_latitude&lt;/li&gt;
&lt;li&gt;dropoff_longitude&lt;/li&gt;
&lt;li&gt;dropoff_latitude&lt;/li&gt;
&lt;li&gt;passenger_count&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Before running any code… which variables seem potentially useful for predicting taxi fares? Do you think permutation importance will necessarily identify these features as important?
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;
I think that pickup and dropoff latitude might be important, since this would reflect where in the city the person was and wanted to go. Passenger count might make a difference as well, but I don't know if there's a greater charge for more people. Longitude might also be useful, but my guess would be that the North-South location is more indicative of the type of place you are in (uptown or downtown) and thus how far you have to travel (I have a vague notion that New York City is longer vertically than horizontally, but I don't know if this is true). This would be even more important if the fares change by location, but I don't know if that's the case.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;permutor = PermutationImportance(first_model, random_state=1).fit(x_validate, y_validate)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ipython_html = eli5.show_weights(permutor, feature_names=x_validate.columns.tolist())
table = pandas.read_html(ipython_html.data)[0]
print(TABLE(table))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Weight&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Feature&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;0.8413  ± 0.0171&lt;/td&gt;
&lt;td class="org-left"&gt;dropoff_latitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.8135  ± 0.0223&lt;/td&gt;
&lt;td class="org-left"&gt;pickup_latitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.5723  ± 0.0370&lt;/td&gt;
&lt;td class="org-left"&gt;pickup_longitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.5324  ± 0.0257&lt;/td&gt;
&lt;td class="org-left"&gt;dropoff_longitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;-0.0014  ± 0.0015&lt;/td&gt;
&lt;td class="org-left"&gt;passenger_count&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
So it looks like latitude and longitude are important, with latitude a little more important than longitude and passenger count isn't important.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org19d74b2" class="outline-3"&gt;
&lt;h3 id="org19d74b2"&gt;A New Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org19d74b2"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc13599c" class="outline-4"&gt;
&lt;h4 id="orgc13599c"&gt;Question 4&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc13599c"&gt;
&lt;blockquote&gt;
&lt;p&gt;
Without detailed knowledge of New York City, it's difficult to rule out most hypotheses about why latitude features matter more than longitude.
&lt;/p&gt;

&lt;p&gt;
A good next step is to disentangle the effect of being in certain parts of the city from the effect of total distance traveled.  
&lt;/p&gt;

&lt;p&gt;
The code below creates new features for longitudinal and latitudinal distance. It then builds a model that adds these new features to those you already had.
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0064e8a" class="outline-4"&gt;
&lt;h4 id="org0064e8a"&gt;Feature Engineering&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0064e8a"&gt;
&lt;p&gt;
We're going to estimate the distance traveled by using the differences in latitude and longitude from the pickup to the dropoff. This should give us a taxicab-distance estimate.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data['absolute_change_longitude'] = abs(data.dropoff_longitude - data.pickup_longitude)
data['absolute_change_latitude'] = abs(data.dropoff_latitude - data.pickup_latitude)

features_2  = ['pickup_longitude',
	       'pickup_latitude',
	       'dropoff_longitude',
	       'dropoff_latitude',
	       'absolute_change_latitude',
	       'absolute_change_longitude']

X = data[features_2]
new_x_train, new_x_validate, new_y_train, new_y_validate = train_test_split(X, y, random_state=1)

estimators = list(range(100, 250, 10))
max_depth = list(range(10, 50, 10)) + [None]
model = RandomForestRegressor()
search = RandomizedSearchCV(estimator=model,
			    param_distributions=grid,
			    n_jobs=-1,
			    random_state=1)
with TIMER:
    search.fit(new_x_train, new_y_train)
#second_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(new_x_train, new_y_train)
second_model = search.best_estimator_
print(f"Mean Cross-Validation Training R^2: {search.best_score_:0.2f}")
print(f"Training R^2: {second_model.score(new_x_train, new_y_train): 0.2f}")
print("Validation R^2: "
      f"{second_model.score(new_x_validate, new_y_validate):0.2f}")
print(search.best_params_)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2020-02-09 14:53:43,624 graeae.timers.timer start: Started: 2020-02-09 14:53:43.624798
2020-02-09 14:55:36,920 graeae.timers.timer end: Ended: 2020-02-09 14:55:36.919979
2020-02-09 14:55:36,920 graeae.timers.timer end: Elapsed: 0:01:53.295181
Mean Cross-Validation Training R^2: 0.49
Training R^2:  0.70
Validation R^2: 0.47
{'n_estimators': 190, 'max_depth': 10}
&lt;/pre&gt;


&lt;p&gt;
Still a pretty bad model, but that's not the point, I guess.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2965c18" class="outline-4"&gt;
&lt;h4 id="org2965c18"&gt;The Permutation Importance&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2965c18"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;permutor = PermutationImportance(second_model, random_state=1).fit(new_x_validate, new_y_validate)
ipython_html = eli5.show_weights(permutor, feature_names=new_x_validate.columns.tolist())
table = pandas.read_html(ipython_html.data)[0]
print(TABLE(table))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Weight&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Feature&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;0.5960  ± 0.0239&lt;/td&gt;
&lt;td class="org-left"&gt;absolute_change_latitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.4362  ± 0.0438&lt;/td&gt;
&lt;td class="org-left"&gt;absolute_change_longitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.0310  ± 0.0175&lt;/td&gt;
&lt;td class="org-left"&gt;pickup_latitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.0226  ± 0.0029&lt;/td&gt;
&lt;td class="org-left"&gt;dropoff_latitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.0211  ± 0.0085&lt;/td&gt;
&lt;td class="org-left"&gt;dropoff_longitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.0164  ± 0.0033&lt;/td&gt;
&lt;td class="org-left"&gt;pickup_longitude&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
The distance traveled seems to be the most important feature for the fare, even more than the actual locations, probably because taxis charge by distance.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org570f4c4" class="outline-4"&gt;
&lt;h4 id="org570f4c4"&gt;Question 5&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org570f4c4"&gt;
&lt;p&gt;
This question is about the scale of the parameters. Here's a sample.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(TABLE(new_x_train.sample(random_state=1).iloc[0].reset_index()))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;index&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;31975&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;pickup_longitude&lt;/td&gt;
&lt;td class="org-right"&gt;-73.9706&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;pickup_latitude&lt;/td&gt;
&lt;td class="org-right"&gt;40.7613&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;dropoff_longitude&lt;/td&gt;
&lt;td class="org-right"&gt;-73.9806&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;dropoff_latitude&lt;/td&gt;
&lt;td class="org-right"&gt;40.7483&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;absolute_change_latitude&lt;/td&gt;
&lt;td class="org-right"&gt;0.01302&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;absolute_change_longitude&lt;/td&gt;
&lt;td class="org-right"&gt;0.010067&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
And here's some statistics about each.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(new_x_validate.describe())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
       pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \
count       7823.000000      7823.000000        7823.000000       7823.000000   
mean         -73.976957        40.756877         -73.975293         40.757591   
std            0.014663         0.018064           0.015877          0.018669   
min          -73.999977        40.700400         -73.999992         40.700293   
25%          -73.988180        40.745044         -73.987078         40.746345   
50%          -73.979933        40.757881         -73.978427         40.758602   
75%          -73.968008        40.769486         -73.966296         40.770561   
max          -73.900123        40.799865         -73.901790         40.799984   

       absolute_change_latitude  absolute_change_longitude  
count               7823.000000                7823.000000  
mean                   0.015091                   0.013029  
std                    0.012508                   0.011554  
min                    0.000000                   0.000000  
25%                    0.006089                   0.004968  
50%                    0.011745                   0.010110  
75%                    0.020781                   0.017798  
max                    0.084413                   0.087337  
&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;
A colleague observes that the values for &lt;code&gt;absolute_change_longitude&lt;/code&gt; and &lt;code&gt;absolute_change_latitude&lt;/code&gt; are pretty small (all values are between -0.1 and 0.1), whereas other variables have larger values.  Do you think this could explain why those coordinates had larger permutation importance values in this case?  
&lt;/p&gt;

&lt;p&gt;
Consider an alternative where you created and used a feature that was 100X as large for these features, and used that larger feature for training and importance calculations. Would this change the outputted permutation importance values?
&lt;/p&gt;

&lt;p&gt;
Why or why not?
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for column in ("pickup_longitude pickup_latitude dropoff_longitude "
	       "dropoff_latitude absolute_change_latitude "
	       "absolute_change_longitude").split():
    print(f"{column}: {new_x_validate[column].max() - new_x_validate[column].min():0.3f}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
pickup_longitude: 0.100
pickup_latitude: 0.099
dropoff_longitude: 0.098
dropoff_latitude: 0.100
absolute_change_latitude: 0.084
absolute_change_longitude: 0.087
&lt;/pre&gt;


&lt;p&gt;
Intuitively I would think that the difference in the scales would make a difference.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data["bigger_pickup_longitude"] = data.pickup_longitude * 100
data["bigger_absolute_change_longitude"] = data.absolute_change_longitude * 100
features_3  = ['pickup_longitude',
	       'pickup_latitude',
	       'dropoff_longitude',
	       'dropoff_latitude',
	       'absolute_change_latitude',
	       'absolute_change_longitude',
	       'bigger_pickup_longitude',
	       'bigger_absolute_change_longitude'
	       ]

X = data[features_3]
big_x_train, big_x_validate, big_y_train, big_y_validate = train_test_split(X, y, random_state=1)
model = RandomForestRegressor()
search = RandomizedSearchCV(estimator=model,
			    param_distributions=grid,
			    n_jobs=-1,
			    random_state=1)
with TIMER:
    search.fit(big_x_train, big_y_train)
big_model = search.best_estimator_
print(f"Mean Cross-Validation Training R^2: {search.best_score_:0.2f}")
print(f"Training R^2: {big_model.score(big_x_train, big_y_train): 0.2f}")
print("Validation R^2: "
      f"{big_model.score(big_x_validate, big_y_validate):0.2f}")
print(search.best_params_)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2020-02-09 15:06:45,693 graeae.timers.timer start: Started: 2020-02-09 15:06:45.693742
/home/athena/.virtualenvs/Visions-Voices-Data/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  "timeout or by a memory leak.", UserWarning
2020-02-09 15:09:15,559 graeae.timers.timer end: Ended: 2020-02-09 15:09:15.559561
2020-02-09 15:09:15,560 graeae.timers.timer end: Elapsed: 0:02:29.865819
Mean Cross-Validation Training R^2: 0.49
Training R^2:  0.70
Validation R^2: 0.47
{'n_estimators': 190, 'max_depth': 10}
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;permutor = PermutationImportance(big_model, random_state=1).fit(big_x_validate, big_y_validate)
ipython_html = eli5.show_weights(permutor, feature_names=big_x_validate.columns.tolist())
table = pandas.read_html(ipython_html.data)[0]
print(TABLE(table))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Weight&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Feature&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;0.6034  ± 0.0436&lt;/td&gt;
&lt;td class="org-left"&gt;absolute_change_latitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.1794  ± 0.0126&lt;/td&gt;
&lt;td class="org-left"&gt;bigger_absolute_change_longitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.1366  ± 0.0062&lt;/td&gt;
&lt;td class="org-left"&gt;absolute_change_longitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.0326  ± 0.0217&lt;/td&gt;
&lt;td class="org-left"&gt;pickup_latitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.0242  ± 0.0040&lt;/td&gt;
&lt;td class="org-left"&gt;dropoff_latitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.0194  ± 0.0083&lt;/td&gt;
&lt;td class="org-left"&gt;dropoff_longitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.0188  ± 0.0085&lt;/td&gt;
&lt;td class="org-left"&gt;pickup_longitude&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;0.0116  ± 0.0018&lt;/td&gt;
&lt;td class="org-left"&gt;bigger_pickup_longitude&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Making the pickup longitude  didn't change its ranking relative to the other features so I wouldn't say that the scale had an effect.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org86b0fdb" class="outline-4"&gt;
&lt;h4 id="org86b0fdb"&gt;Question 6&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org86b0fdb"&gt;
&lt;blockquote&gt;
&lt;p&gt;
You've seen that the feature importance for latitudinal distance is greater than the importance of longitudinal distance. From this, can we conclude whether travelling a fixed latitudinal distance tends to be more expensive than traveling the same longitudinal distance?
&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;
No, the feature importance indicates that it is useful in predicting fares, but it doesn't automatically mean that the fares will increase with the change in latitude. It might be the case that the change in longitude affects the cost of a change in latitude as well, so a fixed latitude distance might change depending on the longitude or latitude + longitude combination.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1cd4e38" class="outline-2"&gt;
&lt;h2 id="org1cd4e38"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1cd4e38"&gt;
&lt;p&gt;
The suggested next tutorial is about &lt;a href="https://www.kaggle.com/dansbecker/partial-plots"&gt;Partial Dependence Plots&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>feature selection</category><category>permutation importance</category><category>tutorial</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/exercise-in-permutation-importance/</guid><pubDate>Thu, 06 Feb 2020 18:45:53 GMT</pubDate></item></channel></rss>