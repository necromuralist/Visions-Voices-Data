<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Visions, Voices, Data (Posts about trees)</title><link>https://necromuralist.github.io/Visions-Voices-Data/</link><description></description><atom:link href="https://necromuralist.github.io/Visions-Voices-Data/categories/trees.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2021 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Thu, 25 Feb 2021 01:00:53 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Decision Trees</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/machine-learning/decision-trees/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/machine-learning/decision-trees/#org14947fe"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/machine-learning/decision-trees/#orgc1c31db"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/machine-learning/decision-trees/#org973908e"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/machine-learning/decision-trees/#org5dbe8c4"&gt;Splitting A Node&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/machine-learning/decision-trees/#org1b4e42b"&gt;Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/machine-learning/decision-trees/#orgb71801c"&gt;Binary Splitting of Qualitative Attributes Using the Gini Index&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org14947fe" class="outline-2"&gt;
&lt;h2 id="org14947fe"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org14947fe"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc1c31db" class="outline-3"&gt;
&lt;h3 id="orgc1c31db"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc1c31db"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfb474e7" class="outline-4"&gt;
&lt;h4 id="orgfb474e7"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfb474e7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
from typing import Any
from math import log2
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfe7cd9c" class="outline-4"&gt;
&lt;h4 id="orgfe7cd9c"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfe7cd9c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from expects import (
    be_within,
    expect,
)
from tabulate import tabulate
import attr
import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org973908e" class="outline-3"&gt;
&lt;h3 id="org973908e"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org973908e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TABLE = partial(tabulate, headers="keys", tablefmt="orgtbl")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5dbe8c4" class="outline-2"&gt;
&lt;h2 id="org5dbe8c4"&gt;Splitting A Node&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5dbe8c4"&gt;
&lt;p&gt;
We choose which is the next node to split by checking the amount of information we would gain for each candidate node and picking the one that gives us the highest gain. We do this by measuring the &lt;b&gt;&lt;b&gt;impurity&lt;/b&gt;&lt;/b&gt; of the nodes, which is a measurement of how dissimilar the class labels are for a node.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1b4e42b" class="outline-3"&gt;
&lt;h3 id="org1b4e42b"&gt;Entropy&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1b4e42b"&gt;
&lt;p&gt;
One measure of "impurity" is &lt;a href="https://www.wikiwand.com/en/Entropy_(information_theory)"&gt;entropy&lt;/a&gt;, a measurement of the information associated with our nodes.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2036aa2" class="outline-4"&gt;
&lt;h4 id="org2036aa2"&gt;Node Entropy&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2036aa2"&gt;
&lt;p&gt;
Here's where we'll calculate the entropy for a node.
&lt;/p&gt;

&lt;p&gt;
\[
Entropy = - \sum_{i=0}^{c-1} p_i(t)log_2 p_i(t)
\]
&lt;/p&gt;

&lt;p&gt;
Where \(p_1(t)\) is the fraction of training data (&lt;i&gt;t&lt;/i&gt;) that has the classification &lt;i&gt;i&lt;/i&gt;. We can translate that to a python function.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; def node_entropy(data: pandas.Series, debug: bool=False) -&amp;gt; float:
     """Calculate the entropy for a child node

     Args:
      data: target data filtered to match the child-node's class
      debug: emit values

     Returns:
      entropy for this node
     """
     if debug:
	 print("calculating node-entropy")
     total = len(data)
     accumulated = 0
     for classification in data.unique():
	 p = len(data[data == classification])/total
	 if debug:
	     print(f"\tclass: {classification}, p: {p} entropy: {p * log2(p)}")
	 accumulated += p * log2(p)
     if debug:
	 print(f"Node Entropy: {accumulated}")
     return -accumulated
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5c670e4" class="outline-4"&gt;
&lt;h4 id="org5c670e4"&gt;Entropy of a Node's Children&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5c670e4"&gt;
&lt;p&gt;
We'll use the entropy formula to get the entropy for an indivdual node but to get the total contribution from the possible splits we'll take a weighted sum of the node entropies.
&lt;/p&gt;

&lt;p&gt;
\[
I(children) = \sum_{j=1}^k \frac{N(v_j)}{N} I(v_j)
\]
&lt;/p&gt;

&lt;p&gt;
Where \(\frac{N(v_j)}{N}\) is the fraction of the child data in node &lt;i&gt;j&lt;/i&gt; and \(I(v_j)\) is the entropy (Impurity) of node &lt;i&gt;j&lt;/i&gt;.
&lt;/p&gt;

&lt;p&gt;
Once again, in python.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def children_impurity(data: pandas.DataFrame, column: str, target: str,
		      impurity: object=node_entropy,
	    debug: bool=False) -&amp;gt; float:
    """Calculate the entropy for the parent of child nodes

    Args:
     data: the container for the values to check
     column: the column to get the entropy
     target: the target column
     impurity: the function to calculate the impurity of the child
     debug: whether to print some intermediate values

    Returns:
     entropy for the data
    """
    if debug:
	print("Calculating entropy for child nodes")
    total = len(data)
    accumulator = 0
    for classification in data[column].unique():
	child = data[data[column] == classification][target]
	if debug:
	    print(f"\tI_({classification}) = ({len(child)}/{total}) "
		  f"x {impurity(child)}")
	accumulator += (len(child)/total) * impurity(child)
    if debug:
	print(f"Child node entropy: {accumulator}")
    return accumulator
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org777a848" class="outline-4"&gt;
&lt;h4 id="org777a848"&gt;Information Gain&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org777a848"&gt;
&lt;p&gt;
The measurement of how much is gained is the difference between a parent node and its children.
\[
\Delta = I(parent) - I(children)
\]
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; def information_gain(data: pandas.Series, column: str, target: str,
		      debug: bool=False) -&amp;gt; float:
     """See how much entropy is removed using this node

     Args:
      data: source to check
      column: name of the column representing the parent node
      target: name of the column we are trying to predict
      debug: emit messages
     """
     return node_entropy(data[target], debug=debug) - children_impurity(
	 data, column=column, target=target, debug=debug)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6b28023" class="outline-4"&gt;
&lt;h4 id="org6b28023"&gt;Home Loans&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6b28023"&gt;
&lt;p&gt;
To make this concrete we can look at a small dataset of people applying for a loan. We want to know if they are likely to default and we need to decide if we want our first split to be on whether they own a home or are married (we'll ignore income for this check because it's meant to illustrate splitting qualitative data).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; @attr.s(auto_attribs=True, slots=True, frozen=True)
 class LoanColumns:
     owner: str = "Home Owner"
     married: str = "Marital Status"
     income: str = "Annual Income"
     defaulted: str = "Defaulted"

 LOANS = LoanColumns()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; loans = pandas.DataFrame({
     LOANS.owner: [True, False, False, True, False, False, True, False, False, False],
     LOANS.married: ["Single", "Married", "Single", "Married", "Divorced", "Single", "Divorced", "Single", "Married", "Single"],
     LOANS.income: [125000, 100000, 70000, 120000, 95000, 60000, 220000, 85000, 75000, 90000],
     LOANS.defaulted: [False, False, False, False, True, False, False, True, False, True],
 })
 print(TABLE(loans))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;Â &lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Home Owner&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Marital Status&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Annual Income&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Defaulted&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;Single&lt;/td&gt;
&lt;td class="org-right"&gt;125000&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;Married&lt;/td&gt;
&lt;td class="org-right"&gt;100000&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;Single&lt;/td&gt;
&lt;td class="org-right"&gt;70000&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;Married&lt;/td&gt;
&lt;td class="org-right"&gt;120000&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;Divorced&lt;/td&gt;
&lt;td class="org-right"&gt;95000&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;5&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;Single&lt;/td&gt;
&lt;td class="org-right"&gt;60000&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;td class="org-left"&gt;Divorced&lt;/td&gt;
&lt;td class="org-right"&gt;220000&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;7&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;Single&lt;/td&gt;
&lt;td class="org-right"&gt;85000&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;8&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;Married&lt;/td&gt;
&lt;td class="org-right"&gt;75000&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;9&lt;/td&gt;
&lt;td class="org-left"&gt;False&lt;/td&gt;
&lt;td class="org-left"&gt;Single&lt;/td&gt;
&lt;td class="org-right"&gt;90000&lt;/td&gt;
&lt;td class="org-left"&gt;True&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
The first step is to calculate the entropy for the entire set using whether they defaulted or not as our classification.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; impurity_parent = node_entropy(loans[LOANS.defaulted])
 print(f"I_parent: {impurity_parent:0.3f}")

 expect(impurity_parent).to(be_within(0.8812, 0.8813))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
I_parent: 0.881
&lt;/pre&gt;


&lt;p&gt;
The next step is to figure out which of our two chosen attributes gives us the most information gain- whether the person was a Home Owner or their Marital Status. We could just look at which has a lower entropy, but the problem is stated so that we want to find the greatest difference between the class' entropy and the parent entropy instead.
&lt;/p&gt;
&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org032f489"&gt;&lt;/a&gt;Home Owners&lt;br&gt;
&lt;div class="outline-text-5" id="text-org032f489"&gt;
&lt;p&gt;
We have two child nodes - one for homeowners and one for non-homeowners.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; print(loans[loans[LOANS.owner]][LOANS.defaulted].value_counts())
 print()
 print(loans[~loans[LOANS.owner]][LOANS.defaulted].value_counts())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
False    3
Name: Defaulted, dtype: int64

False    4
True     3
Name: Defaulted, dtype: int64
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; impurity_home_owner = children_entropy(loans,
					column=LOANS.owner,
					target=LOANS.defaulted, debug=True)
 print(f"{impurity_home_owner: 0.3f}")
 expect(impurity_home_owner).to(be_within(0.689, 0.691))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Calculating entropy for child nodes
	I_(True) = (3/10) x -0.0
	I_(False) = (7/10) x 0.9852281360342515
Child node entropy: 0.6896596952239761
 0.690
&lt;/pre&gt;


&lt;p&gt;
Odd that python allows negative zero-valuesâ¦ Now we can see what the information gain will be.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; gain_home_owner = information_gain(loans, LOANS.owner, LOANS.defaulted)
 print(f"Delta Home-Owner: {gain_home_owner: 0.3}")
 expect(gain_home_owner).to(be_within(0.190, 0.19165))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Delta Home-Owner:  0.192
&lt;/pre&gt;


&lt;p&gt;
I seem to have precision differences with the bookâ¦
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orgbac89be"&gt;&lt;/a&gt;Married Applicants&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgbac89be"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; gain_married = information_gain(loans, LOANS.married, LOANS.defaulted, debug=True)
 print(f"Delta Married: {gain_married: 0.3f}")
 expect(gain_married).to(be_within(0.194, 0.196))
 choice = max(((gain_home_owner, LOANS.owner),
	       (gain_married, LOANS.married)))
 print(f"Next Node: {choice}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example" id="org62a2c0e"&gt;
calculating node-entropy
        class: False, p: 0.7 entropy: -0.3602012209808308
        class: True, p: 0.3 entropy: -0.5210896782498619
Node Entropy: -0.8812908992306927
Calculating entropy for child nodes
        I_(Single) = (5/10) x 0.9709505944546686
        I_(Married) = (3/10) x -0.0
        I_(Divorced) = (2/10) x 1.0
Child node entropy: 0.6854752972273344
Delta Married:  0.196
Next Node: (0.19581560200335835, 'Marital Status')
&lt;/pre&gt;

&lt;p&gt;
Since we gain more information from checking whether someone was married or not, that would be the next node we would choose to split.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb71801c" class="outline-3"&gt;
&lt;h3 id="orgb71801c"&gt;Binary Splitting of Qualitative Attributes Using the Gini Index&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb71801c"&gt;
&lt;p&gt;
\[
\textit{Gini Index} = 1 - \sum_{i=0}^{c - 1}
\]
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def gini(data: pandas.Series) -&amp;gt; float:
    """Calculate the gini index for the data"""
    total = len(data)
    accumulator = 0
    for classification in data.unique():
	accumulator += (len(data[data==classification])/total)**2
    return 1 - accumulator
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org384a58d" class="outline-4"&gt;
&lt;h4 id="org384a58d"&gt;Parent Impurity&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org384a58d"&gt;
&lt;p&gt;
First we get the gini index for the overall data.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parent_gini = gini(loans[LOANS.defaulted])
print(f"I_parent = {parent_gini: 0.3f}")
expect(parent_gini).to(be_within(0.420, 0.421))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
I_parent =  0.420
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3c0fef7" class="outline-4"&gt;
&lt;h4 id="org3c0fef7"&gt;Home Owner Impurity&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3c0fef7"&gt;
&lt;p&gt;
Now the homeowner attribute.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;homeowner_gini = children_impurity(loans, LOANS.owner, LOANS.defaulted, gini, debug=True)
expect(homeowner_gini).to(be_within(0.342, 0.344))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Calculating entropy for child nodes
	I_(True) = (3/10) x 0.0
	I_(False) = (7/10) x 0.48979591836734704
Child node entropy: 0.3428571428571429
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org926b7a7" class="outline-4"&gt;
&lt;h4 id="org926b7a7"&gt;Married Impurity&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org926b7a7"&gt;
&lt;p&gt;
This is different from the entropy case because we want to do binary splits but the marital status attribute has three values (&lt;i&gt;Single&lt;/i&gt;, &lt;i&gt;Married&lt;/i&gt;, and &lt;i&gt;Divorced&lt;/i&gt;) so we need to use a different function that does each attribute against the other (or we could add columns which turn the marital status into a binary attribute, but this seems simpler).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def binary_gini(data: pandas.Series, column: str, target: str,
		classification: object, debug: bool=False) -&amp;gt; float:
    """Calculate the gini value for the data using one against many

    Args:
     data: source with qualitative values
     column: column with the classifications to test
     target: column with the classifications to predict
     classification: the classification to compare
     debug: whether to emit messages
    """
    total = len(data)
    classified = data[data[column] == classification]
    others = data[data[column] != classification]
    if debug:
	print(f"N({classification}/N) I({classification}) = {len(classified)/total * gini(classified[target]):0.3f}")
	print(f"N(!{classification}/N) I!({classification}) = {len(others)/total * gini(others[target]):0.3f}")
	print(f"I({classification}) = {((len(classified)/total) * gini(classified[target]) + (len(others)/total) * gini(others[target])):0.3f}")
    return ((len(classified)/total) * gini(classified[target])
	    + (len(others)/total) * gini(others[target]))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;@attr.s(auto_attribs=True, slots=True, frozen=True)
class MaritalStatus:
    single: str="Single"
    married: str="Married"
    divorced: str="Divorced"

status = MaritalStatus()
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;i_single = binary_gini(loans, LOANS.married, LOANS.defaulted, status.single,
		       debug=True)
print()
i_married = binary_gini(loans, LOANS.married, LOANS.defaulted, status.married,
			debug=True)
print()
i_divorced = binary_gini(loans, LOANS.married, LOANS.defaulted,
			 status.divorced, debug=True)
expect(i_single).to(be_within(0.39, 0.41))
expect(i_divorced).to(be_within(0.39, 0.41))
expect(i_married).to(be_within(0.342, 0.344))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example" id="org63fa806"&gt;
N(Single/N) I(Single) = 0.240
N(!Single/N) I!(Single) = 0.160
I(Single) = 0.400

N(Married/N) I(Married) = 0.000
N(!Married/N) I!(Married) = 0.343
I(Married) = 0.343

N(Divorced/N) I(Divorced) = 0.100
N(!Divorced/N) I!(Divorced) = 0.300
I(Divorced) = 0.400
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;best = 0
best_split = None
for candidate, label in ((homeowner_gini, "Homeowner"),
			 (i_single, "Single"),
			 (i_married, "Married"),
			 (i_divorced, "Divorced")):
    delta = parent_gini - candidate
    if delta &amp;gt; best:
	best = delta
	best_split = label
    print(f"Delta {label} = {delta:0.3f}")
print(f"Best Split: {best_split}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Delta Homeowner = 0.077
Delta Single = 0.020
Delta Married = 0.077
Delta Divorced = 0.020
Best Split: Homeowner
&lt;/pre&gt;


&lt;p&gt;
Either using Home Ownership or Whether someone is married would be the best candidates for the next split.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>trees</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/machine-learning/decision-trees/</guid><pubDate>Sun, 26 Jan 2020 00:31:40 GMT</pubDate></item></channel></rss>