<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Visions, Voices, Data (Posts about interpret)</title><link>https://necromuralist.github.io/Visions-Voices-Data/</link><description></description><atom:link href="https://necromuralist.github.io/Visions-Voices-Data/categories/interpret.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2020 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Sun, 09 Feb 2020 03:44:11 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Partial Dependence Plots</title><link>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org72f4085"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgd507ca4"&gt;What is this about?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgc56e34e"&gt;How does it work?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orga29216c"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgdaa83ab"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgee1fc39"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgd8ab282"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgdc91614"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orga7d1197"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orga658183"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org5464cde"&gt;A Decision Tree Model&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org64a227c"&gt;Visualizing the Decision Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgde6b241"&gt;Partial Dependency Plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org820fb2a"&gt;Distance Covered&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org7b0c022"&gt;A Random Forest Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#orgcb5456e"&gt;2D Partial Dependence Plots&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org544244d"&gt;Forest From the Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/#org36d85b7"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org72f4085" class="outline-2"&gt;
&lt;h2 id="org72f4085"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org72f4085"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd507ca4" class="outline-3"&gt;
&lt;h3 id="orgd507ca4"&gt;What is this about?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd507ca4"&gt;
&lt;p&gt;
These are my notes/re-write of the &lt;a href="https://www.kaggle.com/dansbecker/partial-plots"&gt;Partial Dependence Plots&lt;/a&gt; tutorial on kaggle. Partial Dependence Plots are a complement to Permutation Importance in that Permutation Importance can tell you which features are contributing to the model but don't tell you how features change with different inputs. Given that they have the word "Plot" in their name you can guess that this is a visualization method and since humans have a hard time visualizing things with more than three dimensions, using them requires selecting a subset of features (or maybe just one feature) to visualize at a time, so the fact that Permutation Importance ranks features by their contribution might come in handy in deciding which features to use.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc56e34e" class="outline-3"&gt;
&lt;h3 id="orgc56e34e"&gt;How does it work?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc56e34e"&gt;
&lt;p&gt;
In a simplistic view we might say that it takes input data and then repeatedly alters the values in our feature of choice, freezing the other values so that we can see how varying the feature changes the prediction. You can then repeat this for multiple rows of data and take the average prediction for each value of our feature.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga29216c" class="outline-3"&gt;
&lt;h3 id="orga29216c"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga29216c"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdaa83ab" class="outline-4"&gt;
&lt;h4 id="orgdaa83ab"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgdaa83ab"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from pathlib import Path

import os
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgee1fc39" class="outline-4"&gt;
&lt;h4 id="orgee1fc39"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgee1fc39"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from matplotlib import pyplot
from pdpbox import pdp, get_dataset, info_plots
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

import graphviz
import numpy
import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd8ab282" class="outline-3"&gt;
&lt;h3 id="orgd8ab282"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd8ab282"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdc91614" class="outline-4"&gt;
&lt;h4 id="orgdc91614"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgdc91614"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SLUG = "partial-dependence-plots"
OUTPUT_PATH = Path("../../files/posts/tutorials/")/SLUG
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga7d1197" class="outline-4"&gt;
&lt;h4 id="orga7d1197"&gt;The Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga7d1197"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = Path("~/.env").expanduser()
load_dotenv(path, override=True)
data = pandas.read_csv(Path(os.getenv("FIFA-2018")).expanduser())
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga658183" class="outline-2"&gt;
&lt;h2 id="orga658183"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga658183"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5464cde" class="outline-3"&gt;
&lt;h3 id="org5464cde"&gt;A Decision Tree Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5464cde"&gt;
&lt;p&gt;
The data is the same set from the Permutation Importance exercise (team data to predict whether one of them would become the &lt;i&gt;Budweiser Man of the Match&lt;/i&gt;). Out initial model will be a shallow decision tree so that we can compare its structure to the plots.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;y = data["Man of the Match"] == "Yes"
features = [column for column in data.columns if data[column].dtype == numpy.int64]
X = data[features]

x_train, x_validate, y_train, y_validate = train_test_split(X, y, random_state=1)

tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(x_train, y_train)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org64a227c" class="outline-4"&gt;
&lt;h4 id="org64a227c"&gt;Visualizing the Decision Tree&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org64a227c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=features)
graph = graphviz.Source(tree_graph)
output = "decision_tree.dot"
graph.render(OUTPUT_PATH/output, format="png")
print(f"[[file:{output}.png]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/decision_tree.dot.png" alt="decision_tree.dot.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgde6b241" class="outline-4"&gt;
&lt;h4 id="orgde6b241"&gt;Partial Dependency Plot&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgde6b241"&gt;
&lt;p&gt;
To create the plot we can use the &lt;a href="https://pdpbox.readthedocs.io/en/latest/"&gt;PDPBox library&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org6fef5c6"&gt;&lt;/a&gt;Create the Data to Plot&lt;br&gt;
&lt;div class="outline-text-5" id="text-org6fef5c6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURE = "Goal Scored"
pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=x_validate,
			    model_features=features, feature=FEATURE)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;

&lt;li&gt;&lt;a id="orgce6f5d8"&gt;&lt;/a&gt;Plot It&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgce6f5d8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pdp.pdp_plot(pdp_goals, FEATURE)
output = "pdp_goals_scored.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_goals_scored.png" alt="pdp_goals_scored.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;
Looking at the plot we can interpret it as saying that scoring that first goal is helpful in getting someone on the team the Man of the Match, but after that, more goals doesn't help. The PDPBox documentation has no real information about interpreting the output (or anything they provide, really), but according to the Kaggle tutorial the y-axis is the change in the prediction from the baseline as x changes.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-org820fb2a" class="outline-4"&gt;
&lt;h4 id="org820fb2a"&gt;Distance Covered&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org820fb2a"&gt;
&lt;p&gt;
We can also look at how much the distance the players cover on the field matters.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURE = "Distance Covered (Kms)"
pdp_distance = pdp.pdp_isolate(model=tree_model, dataset=x_validate,
			       model_features=features, feature=FEATURE)
pdp.pdp_plot(pdp_distance, FEATURE)
output = "pdp_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_distance.png" alt="pdp_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
It looks like there's one distance at which the probabilities increase and then going further doesn't matter.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(pdp_distance.feature_grids[3])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
102.0
&lt;/pre&gt;


&lt;p&gt;
So you want your team to cover at least 102 Kilometers, but covering more won't help you.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7b0c022" class="outline-3"&gt;
&lt;h3 id="org7b0c022"&gt;A Random Forest Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7b0c022"&gt;
&lt;p&gt;
The Decision Tree was useful because the simplicity made it easy to interpret, but an ensemble method like a Random Forest probably makes a better model so let's see what it reveals.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;forest = RandomForestClassifier(random_state=0).fit(x_train, y_train)

FEATURE = "Goal Scored"
pdp_goals = pdp.pdp_isolate(model=forest, dataset=x_validate,
			    model_features=features, feature=FEATURE)
pdp.pdp_plot(pdp_goals, FEATURE)
output = "pdp_forest_goals_scored.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_forest_goals_scored.png" alt="pdp_forest_goals_scored.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So, our random forest says that the greatest gain comes from the first goal, but there is in fact a greater probability as you increase the number of goals scored.
&lt;/p&gt;

&lt;p&gt;
What about distance covered?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURE = "Distance Covered (Kms)"
pdp_distance = pdp.pdp_isolate(model=forest, dataset=x_validate,
			       model_features=features, feature=FEATURE)
pdp.pdp_plot(pdp_distance, FEATURE)
output = "pdp_forest_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/pdp_forest_distance.png" alt="pdp_forest_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(pdp_distance.feature_grids[pdp_distance.pdp.argmax()])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
102.0
&lt;/pre&gt;


&lt;p&gt;
Our forest says that, once again, covering 102 kilometers maximizes the probability, but in this case it appears that going beyond that actually decreases the probability that your team would have the man of the match.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcb5456e" class="outline-3"&gt;
&lt;h3 id="orgcb5456e"&gt;2D Partial Dependence Plots&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcb5456e"&gt;
&lt;p&gt;
Rather than plot a single feature against the probability of becoming the man of the match you can plot how two features interact to affect the probability.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURES = ["Goal Scored", "Distance Covered (Kms)"]
interaction = pdp.pdp_interact(model=tree_model, dataset=x_validate,
			       model_features=features,
			       features=FEATURES)

pdp.pdp_interact_plot(pdp_interact_out=interaction, feature_names=FEATURES,
		      plot_type="contour")
output = "goals_vs_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/goals_vs_distance.png" alt="goals_vs_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;b&gt;Note:&lt;/b&gt; The version of PDPBox on pypi currently has a bug (as of February 8, 2020) that won't let you create the previous plot, install it from github instead.
&lt;/p&gt;

&lt;p&gt;
The plot shows a more succinct version of the two plots we had seen before - the optimal point for these two features is 1 goal and 102 Kms covered.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org544244d" class="outline-4"&gt;
&lt;h4 id="org544244d"&gt;Forest From the Trees&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org544244d"&gt;
&lt;p&gt;
Let's re-run the same plot using the Random Forest instead of the Decision Tree.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FEATURES = ["Goal Scored", "Distance Covered (Kms)"]
interaction = pdp.pdp_interact(model=forest, dataset=x_validate,
			       model_features=features,
			       features=FEATURES)

pdp.pdp_interact_plot(pdp_interact_out=interaction, feature_names=FEATURES,
		      plot_type="contour")
output = "forest_goals_vs_distance.png"
pyplot.savefig(OUTPUT_PATH/output)
print(f"[[file:{output}]]")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/forest_goals_vs_distance.png" alt="forest_goals_vs_distance.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The random forest suggests a slightly different scenario - here you want 2 gooalds and between 100 and 110 Kms (or thereabouts) covered to get the best probability.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org36d85b7" class="outline-2"&gt;
&lt;h2 id="org36d85b7"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org36d85b7"&gt;
&lt;p&gt;
This showed how to use the PDPBox library to create Partial Dependence Plots to look at how input values for features affects the predicted outcomes. Unfortunately it looks like PDPBox might have been abandoned, but while it works it's a nice library.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>interpret</category><category>machine learning</category><category>tutorial</category><category>visualization</category><guid>https://necromuralist.github.io/Visions-Voices-Data/posts/tutorials/partial-dependence-plots/</guid><pubDate>Sat, 08 Feb 2020 20:48:50 GMT</pubDate></item></channel></rss>